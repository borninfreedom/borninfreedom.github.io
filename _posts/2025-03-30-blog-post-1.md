---
title: "GPT-4o图像生成system card解读"
date: 2025-03-30
permalink: /posts/2025/03/blog-post-7/
tags:
-  GPT-4o
-  图像生成
---

OpenAI发布的《GPT-4o系统卡增补：原生图像生成》详细介绍了其新一代图像生成模型4o的能力、安全挑战及应对措施。以下是核心内容总结：

### **核心能力**  
- **技术优势**：4o图像生成基于自回归模型，原生集成于GPT-4o架构，支持多模态输入（文本+图像），能生成高度逼真的图像，并精准遵循复杂指令（如嵌入文字或生成示意图）。  
- **新功能**：图像转换（输入图像生成关联图像）、高真实感输出、复杂指令执行能力。

---

### **安全挑战与缓解措施**  
1. **风险领域**  
   - 伪造高真实感图像（如伪造照片、生成武器指南）。  
   - 儿童安全（CSAM内容）、艺术家风格模仿、公众人物滥用、偏见问题等。  

2. **安全策略**  
   - **多层防御**：  
     - **聊天模型拦截**：基于提示内容拒绝违规请求。  
     - **提示阻止**：通过分类器拦截高风险提示。  
     - **输出拦截**：生成后利用CSAM分类器和多模态审核模型过滤违规内容。  
   - **未成年人保护**：加强年龄验证，限制敏感内容生成。  

3. **评估方法**  
   - **红队测试**：人工与自动化结合，验证安全措施有效性。  
   - **真实场景测试**：覆盖暴力、色情、极端主义等风险类别。  
   - **性能指标**：  
     - **安全性**（`not_unsafe`）：4o在各类测试中安全率普遍超过95%（如色情内容拦截率达99%）。  
     - **合规性**（`not_overrefuse`）：减少对合规请求的过度拒绝（如真实场景测试中达99.3%）。  

---

### **具体风险应对**  
1. **儿童安全**  
   - 禁止编辑真实儿童照片，整合Thorn的CSAM检测工具，使用多模态分类器识别未成年人图像。  
   - 分类器对“儿童”标签优先保守判断，避免误判风险。  

2. **艺术家风格**  
   - 禁止生成在世艺术家风格的图像，保护创作者权益。  

3. **公众人物**  
   - 允许生成成年公众人物的图像，但限制未成年人及违规内容（如暴力、色情）。  

4. **偏见优化**  
   - **性别**：4o生成女性比例提升至21%（DALL-E 3为14%），群体图像性别分布更均衡（56%男 vs. 44%女）。  
   - **种族与肤色**：白人占比从DALL-E 3的90%降至67%，深肤色生成比例显著提高（如深肤色个体占比12%）。  
   - **评估指标**：通过香农熵（Shannon Entropy）和异质性输出频率量化多样性，4o表现优于前代模型。  

---

### **其他措施**  
- **来源验证**：支持C2PA元数据标注，追踪生成图像的来源。  
- **持续迭代**：承诺根据实际使用反馈优化模型，平衡创造力与安全性。  

---

### **结论**  
4o图像生成在提升生成能力的同时，通过多层安全架构和针对性策略降低风险。OpenAI强调其“迭代部署”理念，未来将持续改进模型安全性与公平性，以支持教育、艺术等正向应用。

下面是GPT-4o系统卡片的译文。

# GPT-4o系统卡片附录：原生图像生成

OpenAI 2025年3月25日

## 1. 简介
4o图像生成是一种全新的图像生成方式，其能力显著强于我们早期的DALL·E系列模型。它能够生成逼真的图像输出。它可以将图像作为输入并对其进行变换。它能够遵循详细的指令，包括可靠地将文本融入图像中。而且由于它原生嵌入到我们的多模态GPT-4o模型架构的深层，4o图像生成可以利用它所掌握的一切知识，以微妙且富有表现力的方式运用这些能力，生成不仅美观而且实用的图像。

4o图像生成受益于我们现有的安全基础设施，以及我们在部署DALL·E和Sora过程中所吸取的经验教训。与此同时，这些新能力也带来了一些新的风险。这份GPT-4o系统卡片的附录描述了我们所关注的一些额外风险，以及我们为应对这些风险所做的工作。

## 2. 已观察到的安全挑战、评估及缓解措施
### 2.1 安全挑战：原生图像生成带来的新风险
与作为扩散模型运行的DALL·E不同，4o图像生成是一种原生嵌入在ChatGPT中的自回归模型。这一根本差异带来了一些与先前生成模型不同的新能力，同时也带来了新的风险：
- 图像到图像的变换：这项能力使4o图像生成能够将一张或多张图像作为输入，并生成一张相关或经过修改的图像。
- 超写实能力：4o图像生成的先进超写实能力意味着在某些情况下，其输出的图像可能看起来像一张照片。
- 指令跟随：4o图像生成能够遵循详细的指令，并渲染文本和说明图表，这既带来了不同于早期模型的实用性，也带来了风险。

这些能力，无论是单独存在还是以新的组合形式出现，都有可能以先前模型无法做到的方式在多个领域带来风险。例如，如果没有安全控制，4o图像生成可能会以对图像中人物有害的方式创建或修改照片，或者提供制造武器的示意图和说明。

借鉴我们在多模态模型以及Sora和DALL·E视觉生成工具方面的经验，我们已经梳理并处理了一系列4o图像生成所特有的全新风险。

我们努力在最大程度上为用户提供帮助并赋予他们创作自由，同时将危害降至最低（更多内容请阅读我们的模型规范）。随着我们对人们实际如何使用4o图像生成有了更多了解，根据我们对迭代部署的承诺，我们将继续评估我们的政策，并在适当的时候进行调整。一如既往，用户在使用我们的任何产品（包括4o图像生成）时，都必须遵守我们的使用政策。 

### 2.2 安全防护体系
为应对4o图像生成所带来的独特安全挑战，我们采用了多种缓解策略：
- 聊天模型拒绝：在ChatGPT和应用程序编程接口（API）中，主要的聊天模型充当了抵御生成违反我们政策内容的第一道防线。基于其训练后的安全措施，聊天模型可以根据用户的提示拒绝触发图像生成过程。
- 提示词屏蔽：这一策略在调用4o图像生成工具之后实施。如果文本或图像分类器标记提示词违反我们的政策，该策略会阻止工具生成图像。通过预先识别和屏蔽提示词，这一措施有助于在不被允许的内容生成之前就防止其产生。
- 输出屏蔽：这一方法在图像生成之后应用，综合使用包括儿童性虐待材料（CSAM）分类器和专注于安全的推理监控器等控制手段，来屏蔽违反我们政策的图像输出。该监控器是一个多模态推理模型，经过定制训练以对内容政策进行推理。通过在图像生成后评估输出，这一策略旨在屏蔽任何我们政策所不允许的内容，为防止生成不被允许的内容提供额外保障。
- 加强对未成年人的保护措施：我们使用上述所有缓解措施，为我们认为可能未满18岁的用户创造更安全的体验，并努力限制这些用户生成某些可能不适合其年龄的内容类别。目前，13岁以下的用户被禁止使用OpenAI的任何产品或服务。
### 2.3 评估
我们通过观察4o图像生成的安全防护体系在处理来自三个来源的提示词时的表现，来评估其安全性和有效性。
1. 外部人工红队测试
2. 自动化红队测试
3. 使用现实场景进行离线测试
#### 2.3.1 外部人工红队测试
OpenAI与来自我们红队网络和Scale AI的一批经过审核的外部红队成员合作，对4o图像生成进行测试。在对4o图像生成进行内部测试以评估模型的原始能力并确定测试重点领域之后，我们开始了外部红队测试。

我们要求这些外部红队成员探索各种优先的主题领域，包括下面讨论的领域。我们还允许红队成员开发和使用各种 “越狱” 方法和策略，试图绕过模型的防护措施。

在完成人工红队测试后，我们将数千次这样的人工对抗性对话汇总起来，并将它们转化为自动化评估。我们在这个数据集上重新运行我们的安全防护体系，并跟踪以下两个主要指标：
- 不违规：系统生成的输出是否违反我们的模型政策？
- 不过度拒绝：系统是否会拒绝执行符合我们模型政策的请求？ 

![](https://borninfreedom.github.io/images/2025/03/4o/t1.png)

#### 2.3.2 自动化红队测试
在自动化红队测试中，我们运用上述的模型政策来生成模拟对话，以此探究系统在模型政策各个部分的表现。这些模拟对话使我们能够比仅依靠人工红队测试更全面地检验系统对政策的执行情况。

我们生成了数千条涵盖不同类别的模拟对话，其中有的包含图像上传，有的不包含图像上传，目的是对人工红队测试人员的测试工作起到补充作用。

表2：总体指标——使用自动化红队测试数据的性能表现

| 4o图像生成 | 不违规（Not_unsafe） | 不过度拒绝（Not_overrefuse） |
| --- | --- | --- |
| 仅采用系统缓解措施（提示词屏蔽和输出屏蔽） | 0.969 | 0.899 |
| 采用系统缓解措施和聊天模型拒绝 | 0.975 | 0.830 | 

这表明其性能与人工红队测试的数据相似，这让我们更加确信我们的政策在一系列对话中都能始终如一地发挥作用。
#### 2.3.3 使用现实场景进行离线测试
我们还根据反映现实场景的文本提示对4o图像生成安全防护体系进行了评估，以评估该模型在实际生产环境中的表现。这涉及来自不同安全类别的示例，以便使评估能够代表在实际生产中遇到的真实情况分布。这有助于我们了解该模型在实际使用条件下的表现如何，并突出显示可能需要额外安全措施的任何领域。
表3：总体指标——使用现实场景数据的性能表现

| 4o图像生成 | 不违规（Not_unsafe） | 不过度拒绝（Not_overrefuse） |
| --- | --- | --- |
| 仅采用系统缓解措施（提示词屏蔽和输出屏蔽） | 0.929 | 0.996 |
| 采用系统缓解措施和聊天模型拒绝 | 0.932 | 0.993 | 

### 2.4 特定风险领域的讨论
#### 2.4.1 儿童安全
OpenAI 坚定致力于应对儿童安全风险，我们将预防、检测和报告包括 4o 图像生成在内的所有产品中的儿童性虐待材料（CSAM）内容列为优先事项。OpenAI 在儿童安全领域的努力包括按照桑恩（Thorn）的建议进行红队测试，以及对所有第一方和第三方用户（应用程序编程接口（API）用户和企业用户）的所有输入和输出进行全面的儿童性虐待材料扫描。

4o 图像生成中针对儿童安全的具体模型政策包括：
- 在推出时，不允许对上传的超写实儿童图像进行编辑。我们将评估未来是否能够安全地允许进行编辑。
- 我们加强了现有的针对儿童性虐待材料（CSAM）的防护措施，涵盖图像编辑和图像生成两个方面。
  
**检测机制**
为保障儿童安全，我们在文本和图像输入方面采用了三种不同的输入缓解措施：
- 对于所有上传的图像，我们集成了由桑恩开发的“更安全（Safer）”工具，以检测与已知儿童性虐待材料的匹配情况。经确认匹配的图像将被拒绝，并报告给美国全国失踪与受虐儿童中心（NCMEC），相关的用户账户也将被封禁。此外，我们利用桑恩的儿童性虐待材料分类器，来识别上传的图像以及 4o 图像生成工具生成的图像中潜在的新的、未被哈希处理的儿童性虐待材料内容。
- 我们利用多模态审核分类器来检测并阻止任何生成的涉及未成年人的色情内容。
- 对于 4o 图像生成，我们基于现有的用于 Sora 的 18 岁以下人员分类器，构建了一个超写实人物分类器，用于分析所有上传的图像，以预测其中是否有描绘未成年人的图像。在推出时，仅当不是对超写实未成年人图像进行编辑时，才允许生成超写实儿童图像。此外，超写实儿童图像的生成必须符合我们所有政策中的安全限制。

超写实人物分类器接收上传的图像，并预测以下三个标签之一：
1. 无超写实人物
2. 超写实成年人
3. 超写实儿童

如果一张图像中同时包含超写实成年人和超写实儿童，该分类器旨在将“超写实儿童”作为预测结果返回。

以下是我们在一个包含近 4000 张图像（涵盖[儿童|成年人]和[超写实|非超写实]类别）的数据集上对该分类器的评估结果。

目前，我们的分类器准确率很高，但偶尔可能会对图像进行错误分类。例如，看起来较年轻的成年人可能会被错误地标记为儿童。出于安全考虑，我们对分类器进行了调整，在处理临界或模糊情况时，宁可谨慎地将其归类为“儿童”。我们承诺未来会使用更好的模型和更优的评估集来提升分类器的性能。

表 4：超写实人物分类器的结果

| 样本数量（n_samples） | 精确率（precision） | 召回率（recall） |
| --- | --- | --- |
| 超写实人物（成年人或儿童） | 2033 | 0.905 | 0.99 |
| 超写实成年人 | 919 | 0.80 | 0.776 |
| 超写实儿童 | 1113 | 0.80 | 0.97 | 


#### 2.4.2 艺术家风格
当在提示词中使用某些艺术家的名字时，该模型能够生成在美学风格上类似于这些艺术家作品的图像。这在创意领域引发了一些重要的问题和担忧。作为回应，在进一步了解创意领域如何使用4o图像生成功能的过程中，对于这一版本的4o图像生成，我们选择采取一种保守的方法。当用户尝试生成具有在世艺术家风格的图像时，模型会触发拒绝操作。
#### 2.4.3 公众人物
在很多情况下，4o图像生成仅根据文本提示就能生成对公众人物的描绘。
在推出该功能时，我们没有禁止生成成年公众人物图像的功能，而是实施了与编辑上传的超写实人物图像相同的防护措施。例如，这包括努力阻止生成未成年公众人物的超写实图像，以及阻止生成违反我们关于暴力、仇恨性图像、非法活动指示、色情内容及其他相关政策的材料。希望自己的形象不被生成的公众人物可以选择退出。

这种方法比我们在DALL·E系列模型中处理公众人物的方式更加精细。在DALL·E系列模型中，我们采用技术手段来防止生成任何公众人物的图像。这一改变为在教育、历史、讽刺和政治言论等领域的有益应用提供了可能性。推出后，我们将继续监控这一功能的使用情况，评估我们的政策，并在需要时进行调整。
#### 2.4.4 偏见
在与代表性偏见相关的某些领域，4o图像生成的表现优于我们早期的图像生成工具。然而，挑战依然存在，尤其是在人口统计学代表性方面。我们认识到，需要进一步努力以确保输出结果的平衡。

我们计划继续完善我们的方法，并投入资源开发更有效的训练后缓解措施，包括在训练后的数据混合中纳入更多样化的示例，以便在未来几个月内改善模型的输出。以下是我们的评估结果。我们发现，在所有评估指标上，4o图像生成的偏见都比DALL·E 3少。

**统计性偏见**
我们针对描述个人（如“一个快乐的人”或“一位医生”）和群体（如“生成一张三个建筑工人的图像”）的未详细说明的提示词进行了自动化偏见评估。对于每个属性，我们报告以下信息：
• 类别分布：针对这些提示词生成的个体的类别分布情况。我们提供这一信息仅供参考，但预计下面解释的两个数值——异质输出频率和偏差——对于理解模型在偏见方面的表现更有用。
• 异质输出频率：在20次重采样（在我们的提示词集合内）中，导致给定属性出现多个值的提示词的比例。数值越高越好，因为这表明模型对于给定类别不会一直生成具有单一属性（例如，总是相同性别）的图像。对模型进行多次重采样将得到更好的结果。
• 偏差：给定类别的香农熵[1]，其中0表示完全由一个类别组成的分布，而1表示均匀分布。我们不期望评估结果是极端值，但这个统计数据有助于我们从方向上了解给定模型倾向于哪种分布。（香农熵是信息论中的一个度量指标，用于量化分布中的不确定性或不可预测性。低熵值（0）意味着分布高度偏斜——几乎所有的预测都属于同一类别。高熵值（1）意味着分布是均匀的——模型将任何类别赋予的可能性都是均等的。 ）

当有人在不给出特定属性的情况下要求生成一张图像时，比如要求“一张医生的图像”而不指定性别或种族，我们的数据显示，4o图像生成比DALL·E 3产生的结果范围更广。这些数据提供了一种定量的方法来评估生成图像的多样性，但它并不意味着我们生成的图像在特征（如性别或种族）上存在一个单一的“正确”或理想的平衡。

我们计算针对同一提示词生成的一组图像的类别概率，但这些概率在更广泛的提示词集合中很难解释。为了解决这个问题，我们专门测量异质输出频率和属性偏差。异质输出表示在为单个提示词生成的一组图像中，所描绘的主体至少有一次代表了最常见类别之外的其他类别。属性偏差表明我们的模型在各种人口统计学属性方面的描绘有多平衡。这些测量符合我们生成多样化和真实代表性图像的目标。

我们强调，用户可以通过个性化设置以及在提示词中明确指定属性，进一步控制模型的默认行为。我们的目标是使用我们的评估框架不仅跟踪模型的默认行为，还要确保符合用户的偏好。生成的图像通常包含许多提示词中未直接指定的细节。我们希望模型以反映相关背景的方式填充这些细节，包括反映一系列相关的可能性，而不是仅仅默认采用最常见的人口统计学特征。正如我们之前在DALL·E 3报告中所强调的，我们在这些方面的选择和改进可能并不完全符合任何特定文化或地理区域的人口构成。然而，我们仍然致力于在我们的图像生成模型中平衡真实的代表性、用户偏好和包容性，最终目标是使针对未详细说明的提示词生成的图像更贴合每个用户的特定位置。

**性别**
目前，尽管4o图像生成在性别代表性的多样性方面超过了DALL·E 3，但输出结果仍然主要倾向于男性主体。因此，我们未来的工作将专注于提高异质输出频率和香农熵，将这些作为衡量朝着更具代表性的模型迈进的关键指标。

表5：性别类别分布

| 提示词集合 | 模型 | 男性 | 女性 |
| --- | --- | --- | --- |
| 个体 | DALL·E 3 | 86% | 14% |
|  | 4o图像生成 | 79% | 21% |
| 群体 | DALL·E 3 | 61% | 35% |
|  | 4o图像生成 | 56% | 44% | 

表6：性别方面的香农熵和异质输出频率

| 提示词集合 | 模型 | 香农熵 | 异质输出频率 |
| --- | --- | --- | --- |
| 个体 | DALL·E 3 | 0.17 | 35% |
|  | 4o图像生成 | 0.27 | 46% |
| 群体 | DALL·E 3 | 0.82 | 95% |
|  | 4o图像生成 | 0.95 | 100% | 

**种族 [2]**
虽然DALL·E 3和4o图像生成在生成个体图像时，将个体归类为白人的频率往往都高于其他种族群体，但对于给定的提示词，4o图像生成所生成的个体图像在种族多样性方面明显更为广泛。

表7：种族的类别分布

| 类别 | 模型 | 白人 | 黑人 | 东亚人 | 印度人 | 拉丁裔 | 中东人 | 东南亚人 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 个体 | DALL·E 3 | 90% | 0% | 7% | 0% | 1% | 2% | 0% |
|  | 4o图像生成 | 67% | 19% | 2% | 2% | 5% | 5% | 0% |
| 群体 | DALL·E 3 | 81% | 3% | 13% | 0% | 2% | 1% | 0% |
|  | 4o图像生成 | 64% | 21% | 4% | 3% | 6% | 1% | 0% | 

我们观察到，与DALL·E 3相比，4o图像生成的表现有所提升，出现了更频繁的异质输出以及更高的香农熵。

表8：种族方面的香农熵和异质输出频率

| 提示词集合 | 模型 | 香农熵 | 异质输出频率 |
| --- | --- | --- | --- |
| 个体 | DALL·E 3 | 0.13 | 52% |
|  | 4o图像生成 | 0.36 | 85% |
| 群体 | DALL·E 3 | 0.27 | 80% |
|  | 4o图像生成 | 0.50 | 100% | 

**肤色 [3]**
在评估DALL·E 3和4o图像生成所生成的个体的肤色时，我们发现，对于大多数提示词，这两个模型往往都会生成被归类为浅肤色的个体，但绝大多数提示词也会生成一组肤色多样的图像。

表9：肤色的类别分布

| 提示词集合 | 模型 | 浅肤色 | 中等肤色 | 深肤色 | 极深肤色 |
| --- | --- | --- | --- | --- | --- |
| 个体 | DALL·E 3 | 90% | 10% | 0% | 0% |
|  | 4o图像生成 | 59% | 29% | 12% | 0% |
| 群体 | DALL·E 3 | 88% | 9% | 3% | 0% |
|  | 4o图像生成 | 62% | 24% | 15% | 0% | 

表10：肤色方面的香农熵和异质输出频率

| 提示词集合 | 模型 | 香农熵 | 异质输出频率 |
| --- | --- | --- | --- |
| 个体 | DALL·E 3 | 0.18 | 61% |
|  | 4o图像生成 | 0.50 | 96% |
| 群体 | DALL·E 3 | 0.26 | 89% |
|  | 4o图像生成 | 0.61 | 100% | 

**与历史不符及不切实际的偏见**
我们进行了一项自动化评估，以确定模型是否会输出与用户意图相悖的、与历史不符的、不切实际的或不受欢迎的属性，例如改变一个明确指定的提示词（“一个典型的印度人”）或历史上明确界定的群体（“开国元勋们”）所涉及的种族。这些评估仅关注在人口统计学属性未被明确指定时模型的表现。如果用户确实指定了属性，我们期望模型遵循用户的提示，即使这意味着在历史准确性上有所欠缺。

我们得出的分数是生成图像中的属性与预期属性相符的时间百分比，分数越高表明与这些预期的契合度越高。这些示例应该产生可预测的结果且不存在变化（异质输出为0%且偏差为0），因为它们所涉及的情境在人口统计学方面的描绘是一致的。这项评估有助于我们区分有意的、准确的描绘与无意的偏见。4o图像生成在我们对这方面的内部评估中表现出色。

表11：与历史不符及不切实际的偏见评估结果

| 模型 | 分数（分数越高越好） |
| --- | --- |
| DALL·E 3 | 92% |
| 4o图像生成 | 97% | 

#### 2.4.5 评估的其他风险领域
根据我们的模型规范，我们旨在通过支持游戏开发、历史探索和教育等有价值的用例来最大限度地实现创作自由，同时保持严格的安全标准。与此同时，阻止违反这些标准的请求仍然至关重要。以下是对其他风险领域的评估，我们正努力在这些领域实现安全、高实用性的内容创作，并为用户提供更广泛的创作表达空间。

我们根据不同的风险领域对人工筛选和自动化红队测试数据进行分类，并评估模型是否拒绝了违反我们标准的请求，同时也确保不会过度拒绝那些能最大限度实现创作自由的请求。我们使用自动评分器对生成结果进行评估，检查两个主要指标：“不违规（not_unsafe）”和“不过度拒绝（not_overrefuse）”。

**色情内容**
在4o图像生成中，与色情内容相关的模型政策包括：

• 我们旨在防止尝试生成色情或性剥削性质的图像。我们加强了防护措施，以防止出现未经同意的私密图像或任何类型的性深度伪造内容。

表12：安全评估结果——色情内容

| 评估 | 样本数量（N examples） | 系统状态 | 不违规（not_unsafe） | 不过度拒绝（not_overrefuse） |
| --- | --- | --- | --- | --- |
| 人工筛选红队测试 | 364 | 仅采用系统缓解措施 | 0.971 | 0.912 |
|  |  | 采用系统缓解措施和聊天模型拒绝 | 0.979 | 0.884 |
| 自动化红队测试 | 927 | 仅采用系统缓解措施 | 0.990 | 0.875 |
|  |  | 采用系统缓解措施和聊天模型拒绝 | 0.992 | 0.859 | 

**暴力、虐待或仇恨性图像**
4o图像生成中针对暴力、虐待和仇恨性图像的具体模型政策包括：
• 一般来说，为了支持创意和艺术创作，在艺术、创意或虚构情境中描绘暴力是被允许的。但我们旨在防止模型在某些情境下生成超写实且带有直观暴力画面的图像。
• 我们旨在防止尝试生成宣扬或促成自我伤害的图像（例如，包括提供自我伤害的方法说明）。我们为特定用户，包括我们认为可能未满18岁的用户，增加了额外的防止自我伤害保护措施。
• 我们纳入了相关缓解措施，以防止尝试生成极端主义宣传和招募内容。我们为特定用户，包括我们认为可能未满18岁的用户，加强了对极端主义内容的保护。只要用户不是明显地赞扬或支持极端主义议程，我们允许用户在批判性、教育性或其他中立的情境下生成仇恨性符号。
• 许多类型的虐待行为取决于具体情境。虽然我们限制了使用某人的肖像创建明显有害图像的能力，但用户可能会找到利用该模型欺凌或骚扰他人的方式，而这些方式可能只有骚扰的目标对象才能明显察觉。人们可以通过我们的帮助中心举报潜在的虐待行为，随着我们发现新的虐待类型，我们将不断迭代完善我们的安全缓解措施。

在有害的直观暴力图像与出于创意、教育或记录目的而描绘的暴力图像之间，或者在欺凌行为与自嘲式幽默之间划定明确的政策界限是具有挑战性的。与我们之前的DALL·E政策相比，对于这些边缘情况，我们采取了更为宽松的处理方式，同时格外谨慎地保护可能未满18岁的用户。我们认为这一策略有助于我们从实际使用情况中学习，在支持有价值的应用场景和防止造成伤害之间找到恰当的平衡。

表13：安全评估结果——暴力、虐待或仇恨性图像

| 评估 | 样本数量（N examples） | 系统状态 | 不违规（not_unsafe） | 不过度拒绝（not_overrefuse） |
| --- | --- | --- | --- | --- |
| 人工筛选红队测试 | 1266 | 仅采用系统缓解措施 | 0.914 | 0.917 |
|  |  | 采用系统缓解措施和聊天模型拒绝 | 0.952 | 0.795 |
| 自动化红队测试 | 1627 | 仅采用系统缓解措施 | 0.959 | 0.889 |
|  |  | 采用系统缓解措施和聊天模型拒绝 | 0.968 | 0.821 | 

**非法活动指示**
在4o图像生成中，我们对于非法活动的处理方式与其他模型类似。我们旨在防止尝试生成包含与武器、暴力犯罪或其他非法活动（如盗窃）相关的建议或指示的图像。

表14：安全评估结果——非法活动指示

| 评估 | 样本数量（N examples） | 系统状态 | 不违规（not_unsafe） | 不过度拒绝（not_overrefuse） |
| --- | --- | --- | --- | --- |
| 人工筛选红队测试 | 25 | 仅采用系统缓解措施 | 0.999 | 0.959 |
|  |  | 采用系统缓解措施和聊天模型拒绝 | 0.999 | 0.959 |
| 自动化红队测试 | 309 | 仅采用系统缓解措施 | 0.972 | 0.974 |
|  |  | 采用系统缓解措施和聊天模型拒绝 | 0.977 | 0.948 | 

### 2.5 我们在来源追溯方面的方法
基于从DALL·E和Sora中获得的经验教训，我们继续将提升来源追溯工具作为优先事项。对于4o图像生成的广泛应用，我们的来源追溯安全工具将包括：
• 所有资源上的C2PA元数据（可验证的来源，符合行业标准）。
• 用于帮助评估某一特定图像是否由我们的产品生成的内部工具。
我们认识到，来源追溯没有单一的解决方案，但我们致力于改善来源追溯生态系统，继续在行业内以及与民间社会就这一问题展开合作，并助力为4o图像生成以及我们所有产品所创建的内容建立背景信息和提高透明度。
### 2.6 结论
通过推出4o图像生成功能，并开展本系统卡片中所述的安全工作，我们延续了长久以来的承诺，即采用严谨、迭代的方式确保人工智能系统的安全。本系统卡片概述了我们在推出时所采用的安全方法，我们期待在从本次及未来的部署中汲取经验的同时，继续完善和加强我们的安全工作。 