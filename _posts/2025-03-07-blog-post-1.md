---
title: "知识蒸馏综述Knowledge Distillation: A Survey解读"
date: 2025-03-07
permalink: /posts/2025/03/blog-post-1/
tags:
-  蒸馏
---

论文链接：[Knowledge Distillation: A Survey](https://arxiv.org/pdf/2006.05525)


**摘要**：近年来，深度神经网络在工业界和学术界都取得了成功，尤其是在计算机视觉任务方面。深度学习的巨大成功主要归功于它能够扩展以对大规模数据进行编码，并且能够处理数十亿的模型参数。然而，在资源有限的设备（如手机和嵌入式设备）上部署这些复杂的深度模型是一项挑战，这不仅是因为其计算复杂度高，还因为其对存储空间的要求大。为此，已经开发出了各种各样的模型压缩和加速技术。作为模型压缩和加速的一种代表性类型，知识蒸馏有效地从一个大型教师模型中学习到一个小型学生模型。它受到了学术界越来越多的关注。本文从知识类别、训练方案、师生架构、蒸馏算法、性能比较和应用等方面对知识蒸馏进行了全面的综述。此外，还简要回顾了知识蒸馏中面临的挑战，并对未来的研究进行了讨论和展望。
**关键词：深度神经网络；模型压缩；知识蒸馏；知识迁移；师生架构。**

# 1 引言
在过去的几年里，深度学习已经成为人工智能领域诸多成功的基础，包括在计算机视觉（克里热夫斯基等人，2012 年）、强化学习（西尔弗等人，2016 年；阿肖克等人，2018 年；赖等人，2020 年）以及自然语言处理（德夫林等人，2019 年）等各种应用领域。借助包括残差连接（何等人，2016 年、2020 年 b）和批量归一化（伊夫和塞格迪，2015 年）等许多最新技术，在功能强大的 GPU 或 TPU 集群上训练具有数千层的超深度模型变得很容易。例如，在一个拥有数百万张图像的流行图像识别基准数据集上训练一个残差网络（ResNet）模型只需不到十分钟（邓等人，2009 年；孙等人，2019 年）；训练一个用于语言理解的强大的 BERT 模型也不超过一个半小时（德夫林等人，2019 年；尤等人，2019 年）。大规模的深度模型已经取得了巨大的成功，然而，其巨大的计算复杂度和大量的存储需求使得在实时应用中部署它们成为一项巨大的挑战，尤其是在资源有限的设备上，如视频监控设备和自动驾驶汽车。

为了开发高效的深度模型，近期的研究工作通常集中在以下两个方面：1）深度模型的高效构建模块，包括像 MobileNets（霍华德等人，2017 年；桑德勒等人，2018 年）和 ShuffleNets（张等人，2018 年 a；马等人，2018 年）中使用的深度可分离卷积；2）模型压缩和加速技术，具体可分为以下几类（程等人，2018 年）。
• 参数修剪和共享：这些方法专注于从深度神经网络中去除无关紧要的参数，且对模型性能没有显著影响。这一类又进一步分为模型量化（吴等人，2016 年）、模型二值化（库尔巴里奥等人，2015 年）、结构矩阵（辛德瓦尼等人，2015 年）和参数共享（韩等人，2015 年；王等人，2019 年 f）。
• 低秩分解：这些方法通过使用矩阵和张量分解来识别深度神经网络中的冗余参数（于等人，2017 年；丹顿等人，2014 年）。
• 迁移的紧凑卷积滤波器：这些方法通过迁移或压缩卷积滤波器来去除无关紧要的参数（翟等人，2016 年）。
• 知识蒸馏（KD）：这些方法将知识从一个较大的深度神经网络中提取到一个较小的网络中（辛顿等人，2015 年）。
对模型压缩和加速的全面综述超出了本文的范围。本文的重点是知识蒸馏，近年来它受到了研究界越来越多的关注。大型深度神经网络已经取得了显著的成功，性能良好，尤其是在处理大规模数据的现实场景中，因为过参数化在考虑新数据时提高了模型的泛化性能（张等人，2018 年；布鲁茨库斯和格洛布森，2019 年；艾伦-朱等人，2019 年；阿罗拉等人，2018 年；屠等人，2020 年）。然而，由于移动设备和嵌入式系统的计算能力和内存有限，在这些设备上部署深度模型是一个巨大的挑战。为了解决这个问题，布西卢阿等人（2006 年）首先提出了模型压缩的概念，即将信息从一个大型模型或一组模型转移到训练一个小型模型上，且不会导致准确性的显著下降。在半监督学习中，还引入了使用未标记数据在完全监督的教师模型和学生模型之间进行知识迁移（厄纳等人，2011 年）。后来，从大型模型中学习小型模型的方法被正式推广为知识蒸馏（辛顿等人，2015 年）。在知识蒸馏中，一个小型学生模型通常由一个大型教师模型进行监督（布西卢阿等人，2006 年；巴和卡鲁阿纳，2014 年；辛顿等人，2015 年；厄本等人，2017 年）。其主要思想是学生模型模仿教师模型，以便获得具有竞争力甚至更优的性能。关键问题是如何将知识从大型教师模型转移到小型学生模型。基本上，一个知识蒸馏系统由三个关键组件组成：知识、蒸馏算法和师生架构。知识蒸馏的一般师生框架如图 1 所示。 
![](https://borninfreedom.github.io/images/2025/03/kd/1.png)

尽管知识蒸馏在实践中取得了巨大成功，但在理论或实证层面上对其进行理解的研究并不多（厄纳等人，2011 年；程等人，2020 年；冯（Phuong）和兰珀特，2019 年 a；赵（Cho）和哈里哈兰，2019 年）。具体而言，厄纳等人（2011 年）证明了使用未标记数据将知识从教师模型转移到学生模型是大概率近似正确（PAC）可学习的。为了理解知识蒸馏的工作机制，冯和兰珀特在深度线性分类器的场景下，为学习蒸馏后的学生网络的泛化边界和快速收敛性提供了理论依据（冯和兰珀特，2019 年 a）。这一依据回答了学生学习的内容以及学习速度的问题，并揭示了决定蒸馏成功的因素。成功的蒸馏依赖于数据几何结构、蒸馏目标的优化偏差以及学生分类器的强单调性。程等人量化了从深度神经网络中间层提取视觉概念的过程，以此来解释知识蒸馏（程等人，2020 年）。吉（Ji）和朱从风险边界、数据效率以及不完美的教师模型等方面，在理论上对广泛的神经网络中的知识蒸馏进行了解释（吉和朱，2020 年）。赵和哈里哈兰通过实证详细分析了知识蒸馏的有效性（赵和哈里哈兰，2019 年）。实证结果表明，由于模型容量差距，更大的模型不一定是更好的教师（米尔扎德等人，2020 年）。实验还表明，蒸馏会对学生的学习产生不利影响。赵和哈里哈兰（2019 年）并未涵盖对不同形式的知识蒸馏中关于知识、蒸馏以及师生之间相互影响的实证评估。知识蒸馏也被用于标签平滑、评估教师模型的准确性以及获取最优输出层几何结构的先验知识（唐等人，2020 年）。

用于模型压缩的知识蒸馏类似于人类的学习方式。受此启发，近期的知识蒸馏方法已扩展到师生学习（辛顿等人，2015 年）、相互学习（张等人，2018 年 b）、辅助教学（米尔扎德等人，2020 年）、终身学习（翟等人，2019 年）和自学习（袁等人，2020 年）。知识蒸馏的大多数扩展都集中在压缩深度神经网络上。由此产生的轻量级学生网络可以轻松部署在诸如视觉识别、语音识别和自然语言处理（NLP）等应用中。此外，知识蒸馏中从一个模型到另一个模型的知识转移可以扩展到其他任务，如对抗攻击（帕佩诺等人，2016 年）、数据增强（李等人，2019 年 a；戈登和杜，2019 年）、数据隐私与安全（王等人，2019 年 a）。受用于模型压缩的知识蒸馏的启发，知识转移的理念进一步应用于压缩训练数据，即数据集蒸馏，它将知识从一个大数据集转移到一个小数据集，以减少深度模型的训练负担（王等人，2018 年 c；博赫达尔等人，2020 年）。


![](https://borninfreedom.github.io/images/2025/03/kd/2.png)
图 2 知识蒸馏的结构示意图以及相邻章节之间的关系。本综述的主体内容主要包括知识蒸馏的基本原理、知识类型、蒸馏方案、师生架构、蒸馏算法、性能比较、应用、讨论、面临的挑战以及未来的发展方向。请注意，在本图中“章节”缩写为“Sec.”。 


在本文中，我们对知识蒸馏进行了全面的综述。本次综述的主要目标是：1）概述知识蒸馏，包括几种典型的知识类型、蒸馏方法和架构；2）回顾知识蒸馏的最新进展，包括算法以及在不同现实场景中的应用；3）从知识转移的不同角度，包括不同类型的知识、训练方案、蒸馏算法和结构以及应用等方面，探讨知识蒸馏面临的一些障碍，并提供相关见解。最近，也有一篇关于知识蒸馏的类似综述（王和尹，2020 年），它从视觉领域的师生学习的不同角度介绍了全面的进展情况及其面临的挑战。与（王和尹，2020 年）不同的是，我们的综述主要从广泛的知识类型、蒸馏方案、蒸馏算法、性能比较以及不同应用领域等角度关注知识蒸馏。本文的组织结构如图 2 所示。

第 2 节和第 3 节分别总结了不同种类的知识和蒸馏方法。第 4 节阐述了知识蒸馏中关于师生结构的现有研究。第 5 节全面总结了最新的知识蒸馏方法。第 6 节报告了知识蒸馏的性能比较情况。第 7 节展示了知识蒸馏的众多应用。第 8 节讨论了知识蒸馏中具有挑战性的问题和未来的发展方向，并给出了结论。 


# 2. 知识

![](https://borninfreedom.github.io/images/2025/03/kd/3.png)

在知识蒸馏中，知识类型、蒸馏策略以及师生架构在学生模型的学习过程中起着至关重要的作用。在本节中，我们将重点介绍知识蒸馏中不同类别的知识。传统的知识蒸馏方法将大型深度模型的对数几率（logits）作为教师知识（辛顿等人，2015 年；金等人，2018 年；巴和卡鲁阿纳，2014 年；米尔扎德等人，2020 年）。中间层的激活值、神经元或特征也可以用作指导学生模型学习的知识（罗梅罗等人，2015 年；黄和王，2017 年；安等人，2019 年；赫奥等人，2019 年 c；扎戈鲁伊科和科莫达基斯，2017 年）。不同的激活值、神经元或样本对之间的关系包含了教师模型学习到的丰富信息（伊姆等人，2017 年；李和宋，2019 年；刘等人，2019 年 g；通和森，2019 年；于等人，2019 年）。此外，教师模型的参数（或层间连接）也包含着另一种知识（刘等人，2019 年 c）。我们将从以下几类来讨论不同形式的知识：基于响应的知识、基于特征的知识以及基于关系的知识。教师模型中不同类别知识的直观示例见图 3。
## 2.1 基于响应的知识
基于响应的知识通常是指教师模型最后输出层的神经响应。其主要思想是直接模仿教师模型的最终预测结果。基于响应的知识蒸馏方法简单有效，可用于模型压缩，并且已广泛应用于不同的任务和应用场景中。给定一个对数几率向量\(z\)，作为深度模型最后全连接层的输出，基于响应的知识蒸馏损失可以表示为：

![](https://borninfreedom.github.io/images/2025/03/kd/m1.png)

其中\(L_R(.)\)表示对数几率的散度损失，\(z_t\)和\(z_s\)分别是教师模型和学生模型的对数几率。一个典型的基于响应的知识蒸馏（KD）模型如图 4 所示。
![](https://borninfreedom.github.io/images/2025/03/kd/4.png)


基于响应的知识可用于不同类型的模型预测。例如，在目标检测任务中，响应可能包含对数几率以及边界框的偏移量（陈等人，2017 年）。在语义地标定位任务中，如人体姿态估计，教师模型的响应可能包括每个地标的热图（张等人，2019 年 a）。最近，基于响应的知识已被进一步探索，将真实标签信息作为条件目标（孟等人，2019 年）。

在图像分类中，最流行的基于响应的知识是所谓的软目标（辛顿等人，2015 年；巴和卡鲁阿纳，2014 年）。具体来说，软目标是指输入属于各个类别的概率，可以通过 softmax 函数进行估计，公式如下：
![](https://borninfreedom.github.io/images/2025/03/kd/m2.png)

其中\(z_i\)是第\(i\)类的对数几率，引入温度因子\(T\)来控制每个软目标的重要性。正如（辛顿等人，2015 年）所述，软目标包含了教师模型中信息丰富的隐性知识。相应地，软对数几率的蒸馏损失可以改写为：

![](https://borninfreedom.github.io/images/2025/03/kd/m3.png)

一般来说，\(L_R(p(z_t,T),p(z_s,T))\)通常采用 KL 散度损失。显然，优化公式（1）或（3）可以使学生模型的对数几率\(z_s\)与教师模型的对数几率\(z_t\)相匹配。为了便于理解基于响应的知识蒸馏，图 5 给出了一个传统知识蒸馏的基准模型，它是蒸馏损失和学生损失的结合。需要注意的是，学生损失通常定义为真实标签与学生模型软对数几率之间的交叉熵损失\(L_{CE}(y,p(z_s,T = 1))\)。
![](https://borninfreedom.github.io/images/2025/03/kd/5.png)


基于响应的知识的概念简单直接且易于理解，特别是在“隐性知识”的背景下。从另一个角度来看，软目标的有效性类似于标签平滑（金和金，2017 年）或正则化器（米勒等人，2019 年；丁等人，2019 年）。然而，基于响应的知识通常依赖于最后一层的输出，例如软目标，因此无法解决教师模型的中间层监督问题，而这对于使用超深度神经网络进行表示学习来说是非常重要的（罗梅罗等人，2015 年）。由于软对数几率实际上是类别概率分布，基于响应的知识蒸馏也仅限于监督学习。 


## 2.2 基于特征的知识
深度神经网络擅长学习具有越来越高抽象程度的多个层次的特征表示，这被称为表示学习（本吉奥等人，2013 年）。因此，最后一层的输出以及中间层的输出，即特征图，都可以用作监督学生模型训练的知识。具体而言，来自中间层的基于特征的知识是对基于响应的知识的良好扩展，尤其适用于训练更窄更深的网络。

![](https://borninfreedom.github.io/images/2025/03/kd/6.png)

中间表示最早在 Fitnets 模型（罗梅罗等人，2015 年）中被引入，目的是为改进学生模型的训练提供提示。其主要思想是直接匹配教师模型和学生模型的特征激活值。受此启发，人们提出了各种其他方法来间接匹配特征（扎戈鲁伊科和科莫达基斯，2017 年；金等人，2018 年；赫奥等人，2019 年 c；帕斯班等人，2021 年；陈等人，2021 年；王等人，2020 年 b）。具体来说，扎戈鲁伊科和科莫达基斯（2017 年）从原始特征图中导出了一个“注意力图”来表示知识。黄和王（2017 年）利用神经元选择性转移对注意力图进行了推广。帕萨利斯和特法斯（2018 年）通过匹配特征空间中的概率分布来转移知识。为了更轻松地转移教师知识，金等人（2018 年）引入了所谓的“因子”，将其作为一种更容易理解的中间表示形式。为了缩小教师模型和学生模型之间的性能差距，金等人（2019 年）提出了路径约束提示学习，通过教师模型提示层的输出来监督学生模型。最近，赫奥等人（2019 年 c）提议使用隐藏神经元的激活边界来进行知识转移。有趣的是，教师模型中间层的参数共享与基于响应的知识一起也被用作教师知识（周等人，2018 年）。为了匹配教师模型和学生模型之间的语义，陈等人（2021 年）提出了跨层知识蒸馏，通过注意力分配为每个学生层自适应地分配合适的教师层。

![](https://borninfreedom.github.io/images/2025/03/kd/t1.png)

一般来说，基于特征的知识转移的蒸馏损失可以表示为：
![](https://borninfreedom.github.io/images/2025/03/kd/tmp1.png)

## 2.3 基于关系的知识
基于响应的知识和基于特征的知识都利用了教师模型中特定层的输出。基于关系的知识则进一步探究不同层之间或数据样本之间的关系。

为了探究不同特征图之间的关系，伊姆（Yim）等人（2017 年）提出了一种求解过程流（FSP），它由两层之间的格拉姆矩阵（Gram matrix）定义。FSP 矩阵总结了特征图对之间的关系，它是通过计算两层特征之间的内积得到的。利用特征图之间的相关性作为蒸馏知识，有人提出通过奇异值分解进行知识蒸馏，以提取特征图中的关键信息（李等人，2018 年）。为了利用多个教师的知识，张和彭（2018 年）分别将每个教师模型的对数几率和特征作为节点，构建了两个图。具体来说，在知识转移之前，通过对数几率图和表示图对不同教师的重要性和关系进行建模（张和彭，2018 年）。李和宋（2019 年）提出了基于多头图的知识蒸馏方法。图知识是通过多头注意力网络得到的任意两个特征图之间的数据内关系。为了探究成对提示信息，学生模型还模仿教师模型成对提示层之间的互信息流动（帕萨利斯等人，2020 年 b）。一般来说，基于特征图关系的基于关系的知识蒸馏损失可以表示为：

![](https://borninfreedom.github.io/images/2025/03/kd/m5.png)

![](https://borninfreedom.github.io/images/2025/03/kd/tmp2.png)


传统的知识转移方法通常涉及单个知识蒸馏，即教师的单个软目标直接被蒸馏到学生模型中。实际上，蒸馏得到的知识不仅包含特征信息，还包含数据样本之间的相互关系（尤等人，2017 年；朴等人，2019 年）。具体来说，刘等人（2019 年 g）通过实例关系图提出了一种稳健且有效的知识蒸馏方法。实例关系图中转移的知识包含实例特征、实例关系以及跨层的特征空间变换。朴等人（2019 年）提出了一种关系知识蒸馏方法，它从实例关系中转移知识。基于流形学习的思想，通过特征嵌入来学习学生网络，这种方式保留了教师网络中间层中样本的特征相似性（陈等人，2021 年）。利用数据的特征表示，将数据样本之间的关系建模为概率分布（帕萨利斯和特法斯，2018 年；帕萨利斯等人，2020 年 a）。通过知识转移来匹配教师模型和学生模型的概率分布。通（Tung）和森（Mori）（2019 年）提出了一种保持相似性的知识蒸馏方法。特别是，源于教师网络中输入对的相似激活的保持相似性的知识被转移到学生网络中，同时保持了成对的相似性。彭等人（2019 年 a）提出了一种基于相关性一致的知识蒸馏方法，其中蒸馏得到的知识既包含实例级信息，也包含实例之间的相关性。利用相关性一致进行蒸馏，学生网络可以学习到实例之间的相关性。

![](https://borninfreedom.github.io/images/2025/03/kd/7.png)

如上所述，基于实例关系的基于关系的知识蒸馏损失可以表示为：
![](https://borninfreedom.github.io/images/2025/03/kd/m6.png)
![](https://borninfreedom.github.io/images/2025/03/kd/tmp3.png)

![](https://borninfreedom.github.io/images/2025/03/kd/t2.png)


蒸馏得到的知识可以从不同角度进行分类，例如数据的结构化知识（刘等人，2019 年 g；陈等人，2021 年；彭等人，2019 年 a；通和森，2019 年；田等人，2020 年），关于输入特征的先验信息（洛佩兹 - 帕斯等人，2016 年；瓦普尼克和伊兹梅洛夫，2015 年）。不同类别的基于关系的知识总结见表 2。具体来说，\(L_{EM}(.)\)、\(L_{H}(.)\)、\(L_{AW}(.)\)和\(\|\cdot\|_F\)分别是Earth Mover distance、胡贝尔损失、角度损失和弗罗贝尼乌斯范数。尽管最近提出了一些类型的基于关系的知识，但如何将特征图或数据样本中的关系信息建模为知识，仍然值得进一步研究。 


# 3 蒸馏方案
在本节中，我们将讨论教师模型和学生模型的蒸馏方案（即训练方案）。根据教师模型是否与学生模型同时更新，知识蒸馏的学习方案可直接分为三大类：离线蒸馏、在线蒸馏和自蒸馏，如图 8 所示。

![](https://borninfreedom.github.io/images/2025/03/kd/8.png)

## 3.1 离线蒸馏
以往的大多数知识蒸馏方法都是离线进行的。在传统的知识蒸馏（辛顿等人，2015 年）中，知识是从预训练的教师模型转移到学生模型中的。因此，整个训练过程分为两个阶段，即：1）在蒸馏之前，首先在一组训练样本上训练大型教师模型；2）在蒸馏过程中，使用教师模型以对数几率或中间特征的形式提取知识，然后用这些知识来指导学生模型的训练。

离线蒸馏的第一个阶段通常不被视为知识蒸馏的一部分，也就是说，假设教师模型是预先定义好的。人们很少关注教师模型的结构及其与学生模型的关系。因此，离线方法主要侧重于改进知识转移的不同部分，包括知识的设计（辛顿等人，2015 年；罗梅罗等人，2015 年）以及用于匹配特征或分布的损失函数（黄和王，2017 年；帕萨利斯和特法斯，2018 年；扎戈鲁伊科和科莫达基斯，2017 年；米尔扎德等人，2020 年；李等人，2020 年 d；赫奥等人，2019 年 b；阿西夫等人，2020 年）。离线方法的主要优点是简单且易于实现。例如，教师模型可能包含一组使用不同软件包训练的模型，这些模型可能位于不同的机器上。知识可以被提取并存储在缓存中。

离线蒸馏方法通常采用单向知识转移和两阶段训练过程。然而，具有巨大训练时间的复杂高容量教师模型是无法避免的，而在教师模型的指导下，离线蒸馏中对学生模型的训练通常是高效的。此外，大型教师模型和小型学生模型之间的容量差距始终存在，并且学生模型在很大程度上依赖于教师模型。
## 3.2 在线蒸馏
尽管离线蒸馏方法简单有效，但离线蒸馏中的一些问题已经引起了研究界越来越多的关注（米尔扎德等人，2020 年）。为了克服离线蒸馏的局限性，人们提出了在线蒸馏方法，以进一步提高学生模型的性能，特别是在没有大容量高性能教师模型可用的情况下（张等人，2018 年 b；陈等人，2020 年 a）。在在线蒸馏中，教师模型和学生模型同时更新，并且整个知识蒸馏框架是端到端可训练的。

已经提出了各种在线知识蒸馏方法，尤其是在过去的几年里（张等人，2018 年 b；陈等人，2020 年 a；谢等人，2019 年；阿尼尔等人，2018 年；金等人，2019 年 b；周等人，2018 年；瓦拉瓦尔卡等人，2020 年；吴和龚，2021 年；张等人，2021 年 a）。具体来说，在深度相互学习（张等人，2018 年 b）中，多个神经网络以协作的方式工作。在训练过程中，任何一个网络都可以作为学生模型，而其他模型可以作为教师模型。为了提高泛化能力，通过使用软对数几率的集成来扩展深度相互学习（郭等人，2020 年）。陈等人（2020 年 a）进一步将辅助对等模型和一个组长引入深度相互学习中，以形成一组多样化的对等模型。为了降低计算成本，朱和龚（2018 年）提出了一种多分支架构，其中每个分支表示一个学生模型，并且不同的分支共享相同的主干网络。金等人（2019 年 b）没有使用对数几率的集成，而是引入了一个特征融合模块来构建教师分类器。谢等人（2019 年）用低成本的卷积操作替换卷积层来形成学生模型。阿尼尔等人（2018 年）采用在线蒸馏来训练大规模分布式神经网络，并提出了一种在线蒸馏的变体，称为协同蒸馏。协同蒸馏并行训练多个具有相同架构的模型，并且任何一个模型都是通过从其他模型转移知识来进行训练的。最近，一种在线对抗知识蒸馏方法被提出，通过鉴别器利用来自类别概率和特征图的知识来同时训练多个网络（钟等人，2020 年）。最近，通过使用生成对抗网络（GAN）生成不同的示例设计出了对抗协同蒸馏（张等人，2021 年 a）。

在线蒸馏是一种具有高效并行计算能力的单阶段端到端训练方案。然而，现有的在线方法（例如，相互学习）通常无法解决在线设置中高容量教师模型的问题，这使得在在线设置中进一步探索教师模型和学生模型之间的关系成为一个有趣的研究课题。 

## 3.3 自蒸馏
在自蒸馏中，教师模型和学生模型使用相同的网络（张等人，2019 年 b；侯等人，2019 年；张和萨本库，2020 年；杨等人，2019 年 b；李等人，2019 年 a；冯和兰珀特，2019 年 b；兰等人，2018 年；徐和刘，2019 年；莫巴希等人，2020 年）。这可以被视为在线蒸馏的一种特殊情况。具体来说，张等人（2019 年 b）提出了一种新的自蒸馏方法，在这种方法中，网络较深层的知识被蒸馏到较浅层。与（张等人，2019 年 b）中的自蒸馏类似，侯等人（2019 年）提出了一种用于车道检测的自注意力蒸馏方法。该网络将自身各层的注意力图作为较低层的蒸馏目标。快照蒸馏（杨等人，2019 年 b）是自蒸馏的一种特殊变体，在这种方法中，网络早期训练阶段（教师）的知识被转移到后期训练阶段（学生），以支持同一网络内的监督训练过程。为了通过提前退出进一步减少推理时间，冯和兰珀特（2019 年 b）提出了基于蒸馏的训练方案，在训练过程中，提前退出层试图模仿后期退出层的输出。最近，莫巴希等人（2020 年）从理论上对自蒸馏进行了分析，张和萨本库（2020 年）通过实验证明了其性能的提升。

此外，最近还提出了一些有趣的自蒸馏方法（袁等人，2020 年；云等人，2020 年；哈恩和崔，2019 年）。具体而言，袁等人基于对标签平滑正则化的分析提出了无教师知识蒸馏方法（袁等人，2020 年）。哈恩和崔提出了一种新颖的自知识蒸馏方法，其中自知识由预测概率组成，而非传统的软概率（哈恩和崔，2019 年）。这些预测概率由训练模型的特征表示定义，它们反映了特征嵌入空间中数据的相似性。云等人提出了基于类别（class-wise）的自知识蒸馏，以在同一源数据中使用相同模型匹配类内样本和增强样本之间训练模型的输出分布（云等人，2020 年）。此外，李等人（2019 年 a）提出的自蒸馏被用于数据增强，并且增强的自知识被蒸馏到模型自身中。自蒸馏还被用于逐个优化具有相同架构的深度模型（教师或学生网络）（富拉内洛等人，2018 年；巴盖里内扎德等人，2018 年）。每个网络都通过师生优化的方式，将前一个网络的知识进行蒸馏。

此外，从人类师生学习的角度也可以直观地理解离线蒸馏、在线蒸馏和自蒸馏。离线蒸馏意味着知识渊博的教师向学生传授知识；在线蒸馏意味着教师和学生相互一起学习；自蒸馏意味着学生自主学习知识。而且，就像人类学习一样，这三种蒸馏方式由于各自的优势可以相互结合、相互补充。例如，通过多种知识转移框架，自蒸馏和在线蒸馏可以得到合理整合（孙等人，2021 年）。


# 4 师生架构

![](https://borninfreedom.github.io/images/2025/03/kd/9.png)

在知识蒸馏中，师生架构是形成知识转移的通用载体。换句话说，从教师到学生的知识获取和蒸馏的质量也取决于如何设计教师网络和学生网络。


从人类学习的习惯来看，我们希望学生能够找到合适的教师。因此，为了在知识蒸馏中很好地完成知识的捕捉和蒸馏，如何选择或设计合适的教师和学生结构是一个非常重要但又很困难的问题。最近，在蒸馏过程中，教师和学生的模型设置几乎是预先固定的，具有不变的规模和结构，这样很容易导致模型容量差距。然而，如何具体设计教师和学生的架构，以及为什么它们的架构由这些模型设置所决定，这方面的研究几乎是缺失的。在本节中，我们将讨论教师模型和学生模型结构之间的关系，如图 9 所示。

在（辛顿等人，2015 年）中，知识蒸馏最初被设计用于压缩深度神经网络的集合。深度神经网络的复杂性主要来自两个维度：深度和宽度。通常需要将知识从更深更宽的神经网络转移到更浅更窄的神经网络（罗梅罗等人，2015 年）。学生网络通常被选择为以下几种情况：1）教师网络的简化版本，层数更少且每层的通道数也更少（王等人，2018 年 a；朱和龚，2018 年；李等人，2020 年 d）；2）教师网络的量化版本，其中网络结构得以保留（波利诺等人，2018 年；米什拉和马尔，2018 年；魏等人，2018 年；申等人，2019 年）；3）具有高效基本操作的小型网络（霍华德等人，2017 年；张等人，2018 年 a；黄等人，2017 年）；4）具有优化全局网络结构的小型网络（刘等人，2019 年 i；谢等人，2020 年；顾和特雷斯普，2020 年）；5）与教师相同的网络（张等人，2018 年 b；富拉内洛等人，2018 年；塔尔瓦伊宁和瓦尔波拉，2017 年）。

大型深度神经网络和小型学生神经网络之间的模型容量差距会降低知识转移的效果（米尔扎德等人，2020 年；高等人，2021 年）。为了有效地将知识转移到学生网络中，已经提出了各种方法来有控制地降低模型的复杂度（张等人，2018 年 b；诺瓦克和科尔索，2018 年；克劳利等人，2018 年；刘等人，2019 年 a、i；王等人，2018 年 a；顾和特雷斯普，2020 年）。具体来说，米尔扎德等人（2020 年）引入了一个教师助手来缓解教师模型和学生模型之间的训练差距。通过残差学习进一步缩小了这个差距，即使用助手结构来学习残差误差（高等人，2021 年）。另一方面，最近的一些方法也专注于最小化学生模型和教师模型在结构上的差异。例如，波利诺等人（2018 年）将网络量化与知识蒸馏相结合，即学生模型是教师模型的小型量化版本。诺瓦克和科尔索（2018 年）提出了一种结构压缩方法，该方法涉及将多层学习到的知识转移到单层。王等人（2018 年 a）在保留感受野的同时，逐步从教师网络向学生网络进行分块知识转移。在在线设置中，教师网络通常是学生网络的集合，其中学生模型相互之间具有相似的结构（或相同的结构）（张等人，2018 年 b；朱和龚，2018 年；富拉内洛等人，2018 年；陈等人，2020 年 a）。

最近，深度可分离卷积已被广泛用于为移动或嵌入式设备设计高效的神经网络（乔列特，2017 年；霍华德等人，2017 年；桑德勒等人，2018 年；张等人，2018 年 a；马等人，2018 年）。受神经架构搜索（或 NAS）成功的启发，通过基于高效元操作或模块搜索全局结构，小型神经网络的性能得到了进一步提升（吴等人，2019 年；谭等人，2019 年；谭和乐，2019 年；拉多萨沃维奇等人，2020 年）。此外，动态搜索知识转移机制的想法也出现在知识蒸馏中，例如，使用强化学习以数据驱动的方式自动去除冗余层（阿肖克等人，2018 年），以及在给定教师网络的情况下搜索最优的学生网络（刘等人，2019 年 i；谢等人，2020 年；顾和特雷斯普，2020 年）。

以前的大多数研究工作要么专注于设计教师和学生模型的结构，要么专注于它们之间的知识转移方案。为了使小型学生模型能够很好地匹配大型教师模型，以提高知识蒸馏的性能，自适应的师生学习架构是必要的。最近，知识蒸馏中的神经架构搜索概念，即在教师模型的指导下联合搜索学生结构和知识转移，将成为未来研究的一个有趣课题。 

# 5 蒸馏算法
一种简单但非常有效的知识转移思路是，直接匹配教师模型和学生模型之间基于响应的知识、基于特征的知识（罗梅罗等人，2015 年；辛顿等人，2015 年），或者特征空间中的表示分布（帕萨利斯和特法斯，2018 年）。为了在更复杂的场景中改进知识转移的过程，已经提出了许多不同的算法。在本节中，我们将回顾知识蒸馏领域内，近期提出的典型知识转移蒸馏方法类型。
## 5.1 对抗蒸馏
在知识蒸馏中，教师模型很难完美地从真实数据分布中学习。同时，学生模型的容量较小，因此无法准确地模仿教师模型（米尔扎德等人，2020 年）。是否有其他训练学生模型的方法，使其能够模仿教师模型呢？最近，对抗学习因其在生成网络（即生成对抗网络，GANs）中的巨大成功而备受关注（古德费洛等人，2014 年）。具体来说，生成对抗网络中的判别器估计一个样本来自训练数据分布的概率，而生成器则试图用生成的数据样本欺骗判别器。受此启发，许多对抗知识蒸馏方法被提出，以使教师和学生网络能够更好地理解真实数据分布（王等人，2018 年 e；徐等人，2018 年 a；米凯埃利和斯托基，2019 年；徐等人，2018 年 b；刘等人，2018 年；王等人，2018 年 f；陈等人，2019 年 a；沈等人，2019 年 d；舒等人，2019 年；刘等人，2020 年 a；贝拉吉亚尼斯等人，2018 年）。

![](https://borninfreedom.github.io/images/2025/03/kd/10.png)
图10 主要对抗蒸馏方法的不同类别。（a）生成对抗网络（GAN）中的生成器生成训练数据以提升知识蒸馏（KD）的性能；教师模型可被用作判别器。（b）生成对抗网络中的判别器确保学生模型（也充当生成器）模仿教师模型。（c）教师模型和学生模型组成一个生成器；在线知识蒸馏通过判别器得到增强。 


如图 10 所示，基于对抗学习的蒸馏方法，尤其是使用生成对抗网络的方法，可主要分为以下三类。第一类方法中，训练一个对抗生成器来生成合成数据，这些合成数据要么直接用作训练数据集（陈等人，2019 年 a；叶等人，2020 年），要么用于扩充训练数据集（刘等人，2018 年），如图 10（a）所示。此外，米凯埃利和斯托基（2019 年）利用对抗生成器生成难例样本，用于知识转移。一般来说，这类基于生成对抗网络的知识蒸馏所使用的蒸馏损失可以表示为：
![](https://borninfreedom.github.io/images/2025/03/kd/tmp4.png)


为了使学生模型更好地匹配教师模型，第二类方法引入了一个判别器，通过使用对数几率（徐等人，2018 年 a、b）或特征（王等人，2018 年 f）来区分学生模型和教师模型的样本，如图 10（b）所示。具体来说，贝拉吉亚尼斯等人（2018 年）使用未标记的数据样本进行知识转移。沈等人（2019 年 d）使用了多个判别器。此外，舒等人（2019 年）使用了一种有效的中间监督，即压缩后的知识，来缓解教师模型和学生模型之间的容量差距。王等人（2018 年 f）提出的一个代表性模型就属于这一类，其可以表示为：
![](https://borninfreedom.github.io/images/2025/03/kd/m8.png)


其中\(G\)是学生网络，\(L_{GAN}(.)\)表示生成对抗网络中使用的典型损失函数，用于使学生模型和教师模型的输出尽可能相似。

第三类方法中，对抗知识蒸馏以在线的方式进行，即教师模型和学生模型在每次迭代中联合优化（王等人，2018 年 e；钟等人，2020 年），如图 10（c）所示。此外，通过知识蒸馏来压缩生成对抗网络，一个学习到的小型生成对抗网络学生模型通过知识转移来模仿一个较大的生成对抗网络教师模型（阿吉纳尔多等人，2019 年；李等人，2020 年 c）。

综上所述，从上述对抗蒸馏方法中可以总结出三个要点：生成对抗网络是一种有效的工具，可通过教师知识转移增强学生的学习能力；生成对抗网络与知识蒸馏相结合，可以生成有价值的数据，从而提高知识蒸馏的性能，并克服无法使用和获取数据的局限性；知识蒸馏可用于压缩生成对抗网络。

## 5.2 多教师蒸馏
不同的教师架构可以为学生网络提供各自有用的知识。在训练学生网络的过程中，多个教师网络可以单独或整体地用于蒸馏。在典型的师生框架中，教师通常是一个大型模型或一组大型模型的集合。为了从多个教师那里转移知识，最简单的方法是使用所有教师的平均响应作为监督信号（辛顿等人，2015 年）。最近已经提出了几种多教师知识蒸馏方法（索和巴拉苏布拉马尼亚姆，2016 年；尤等人，2017 年；陈等人，2019 年 b；富拉内洛等人，2018 年；杨等人，2019 年 a；张等人，2018 年 b；李等人，2019 年 c；朴和夸克，2020 年；帕佩诺等人，2017 年；福田等人，2017 年；鲁德等人，2017 年；吴等人，2019 年 a；杨等人，2020 年 d；翁库尔比萨尔等人，2019 年；赵等人，2020 年 a；袁等人，2021 年）。多教师蒸馏的通用框架如图 11 所示。
![](https://borninfreedom.github.io/images/2025/03/kd/11.png)


事实证明，多个教师网络对于训练学生模型通常是有效的，它们使用对数几率和特征表示作为知识。除了所有教师的平均对数几率外，尤等人（2017 年）还进一步纳入了中间层的特征，以促进不同训练样本之间的差异。为了同时利用对数几率和中间层特征，陈等人（2019 年 b）使用了两个教师网络，其中一个教师将基于响应的知识转移给学生，另一个教师将基于特征的知识转移给学生。福田等人（2017 年）在每次迭代中从教师网络池中随机选择一个教师。为了从多个教师那里转移基于特征的知识，在学生网络中添加了额外的教师分支，以模仿教师的中间层特征（朴和夸克，2020 年；阿西夫等人，2020 年）。重生网络以逐步的方式处理多个教师，即第\(t\)步的学生作为第\(t + 1\)步学生的教师（富拉内洛等人，2018 年），类似的想法也可以在（杨等人，2019 年 a）中找到。为了有效地进行知识转移并挖掘多个教师的潜力，已经提出了几种替代方法，例如通过向给定教师添加不同类型的噪声（索和巴拉苏布拉马尼亚姆，2016 年），或者使用随机模块和跳跃连接（李等人，2019 年 c）来模拟多个教师。在（沈等人，2019 年 a；罗等人，2019 年；沈等人，2019 年 b；罗等人，2020 年）中，设计了使用多个具有特征集成的教师模型的知识融合方法。通过知识融合，许多公开可用的预训练深度模型可以作为教师被重复使用。更有趣的是，由于多教师蒸馏的特殊性质，其扩展应用可用于通过知识适应实现域适应（鲁德等人，2017 年），以及保护数据的隐私和安全（翁库尔比萨尔等人，2019 年；帕佩诺等人，2017 年）。

表3 使用不同类型知识和蒸馏方案的多教师蒸馏方法总结。基于响应的知识、基于特征的知识和基于关系的知识分别缩写为“ResK”、“FeaK”和“RelK”。 
![](https://borninfreedom.github.io/images/2025/03/kd/t3.png)


使用不同类型知识和蒸馏方案的典型多教师蒸馏方法总结见表 3。一般来说，多教师知识蒸馏可以提供丰富的知识，并根据不同教师的多样化知识定制一个通用的学生模型。然而，如何有效地整合来自多个教师的不同类型知识，还需要进一步研究。 

## 5.3 跨模态蒸馏
在训练或测试过程中，某些模态的数据或标签可能无法获取（古普塔等人，2016 年；加西亚等人，2018 年；赵等人，2018 年；罗赫达等人，2018 年；赵等人，2020 年 b）。因此，在不同模态之间转移知识就显得尤为重要。

以下回顾了几种使用跨模态知识转移的典型场景。

古普塔等人（2016 年）给定一个在一种模态（如 RGB 图像）上预训练的教师模型，该模型拥有大量标注良好的数据样本，他们将教师模型的知识转移到具有新的未标注输入模态（如深度图像和光流）的学生模型中。具体来说，所提出的方法依赖于涉及两种模态（即 RGB 图像和深度图像）的未标注配对样本。然后，教师从 RGB 图像中获取的特征被用于对学生进行有监督的训练（古普塔等人，2016 年）。配对样本背后的理念是通过成对样本配准来转移注释或标签信息，并且已广泛应用于跨模态应用中（阿尔巴尼等人，2018 年；赵等人，2018 年；托克尔和加尔，2019 年）。为了透过墙壁或对有遮挡的图像进行人体姿态估计，赵等人（2018 年）使用了同步的无线电信号和相机图像。通过跨模态知识转移来进行基于无线电的人体姿态估计。托克尔和加尔（2019 年）从两种模态（RGB 视频和骨架序列）中获取了配对样本。这些样本对被用于将在 RGB 视频上学到的知识转移到基于骨架的人体动作识别模型中。
![](https://borninfreedom.github.io/images/2025/03/kd/12.png)


为了仅使用 RGB 图像提高动作识别性能，加西亚等人（2018 年）对另一种模态（即深度图像）进行了跨模态蒸馏，以便为 RGB 图像模态生成一个虚拟流。田等人（2020 年）引入了一种对比损失，以在不同模态之间转移成对关系。为了提高目标检测性能，罗赫达等人（2018 年）提出使用生成对抗网络在缺失和可用的模态之间进行跨模态蒸馏。跨模态蒸馏的通用框架如图 12 所示。

此外，多等人（2019 年）提出了一种基于知识蒸馏的视觉问答方法，其中将来自以图像 - 问题 - 答案为输入的三线性交互教师模型的知识蒸馏到以图像 - 问题为输入的双线性交互学生模型的学习中。帕萨利斯和特法斯（2018 年）提出的概率知识蒸馏也被用于将知识从文本模态转移到视觉模态。霍夫曼等人（2016 年）提出了一种基于跨模态蒸馏的模态虚拟架构，以提高检测性能。此外，这些跨模态蒸馏方法还可以在多个领域之间转移知识（昆杜等人，2019 年；陈等人，2019 年 c；苏和马吉，2017 年）。

![](https://borninfreedom.github.io/images/2025/03/kd/t4.png)

不同模态、知识类型和蒸馏方案的跨模态蒸馏总结见表 4。具体而言，可以看出在跨模态场景下的视觉识别任务中，知识蒸馏表现良好。然而，当存在模态差距（例如，不同模态之间缺乏配对样本）时，跨模态知识转移是一项具有挑战性的研究。 

## 5.4 基于图的蒸馏
![](https://borninfreedom.github.io/images/2025/03/kd/13.png)

大多数知识蒸馏算法专注于将单个实例的知识从教师模型转移到学生模型，而最近提出的一些方法则利用图来探索数据内部的关系（陈等人，2021 年；张和彭，2018 年；李和宋，2019 年；朴等人，2019 年；姚等人，2020 年；马和梅，2019 年；侯等人，2020 年）。这些基于图的蒸馏方法的主要思想有：1）将图作为教师知识的载体；或者 2）使用图来控制教师知识的消息传递。基于图的蒸馏通用框架如图 13 所示。如 2.3 节所述，基于图的知识属于基于关系的知识范畴。在本节中，我们将介绍基于图的知识的典型定义以及基于图的消息传递蒸馏算法。

具体来说，在（张和彭，2018 年）中，每个顶点代表一个自监督教师。然后使用对数几率和中间特征构建两个图，即对数几率图和表示图，以将知识从多个自监督教师转移到学生模型。在（陈等人，2021 年）中，图被用于维护高维空间中样本之间的关系。然后使用提出的局部性保持损失函数进行知识转移。李和宋（2019 年）使用多头图分析了数据内部的关系，其中顶点是来自卷积神经网络不同层的特征。朴等人（2019 年）直接转移了数据样本之间的相互关系，即匹配教师图和学生图之间的边。通和森（2019 年）使用相似性矩阵来表示教师模型和学生模型中输入对激活的相互关系。学生模型的相似性矩阵与教师模型的相似性矩阵相匹配。此外，彭等人（2019 年 a）不仅匹配了基于响应的知识和基于特征的知识，还使用了基于图的知识。在（刘等人，2019 年 g）中，实例特征和实例关系分别被建模为图的顶点和边。

与使用基于图的知识不同，有几种方法使用图来控制知识转移。具体来说，罗等人（2018 年）考虑了模态差异，以纳入来自源域的先验信息。引入了一个有向图，称为蒸馏图，以探索不同模态之间的关系。每个顶点代表一种模态，边表示一种模态与另一种模态之间的连接强度。米纳米等人（2019 年）提出了一种基于双向图的多样化协作学习方法，以探索多样化的知识转移模式。姚等人（2020 年）引入了图神经网络来处理基于图的知识的知识转移。此外，通过知识蒸馏，图卷积教师网络的拓扑语义作为拓扑感知知识被转移到图卷积学生网络中（杨等人，2020 年 b）。

基于图的蒸馏可以转移数据中有价值的结构知识。然而，如何恰当地构建图来对数据的结构知识进行建模仍然是一项具有挑战性的研究。

## 5.5 基于注意力的蒸馏
由于注意力能够很好地反映卷积神经网络的神经元激活情况，一些注意力机制被应用于知识蒸馏中，以提高学生网络的性能（扎戈鲁伊科和科莫达基斯，2017 年；黄和王，2017 年；斯里尼瓦斯和弗勒雷，2018 年；克劳利等人，2018 年；宋等人，2018 年）。在这些基于注意力的知识蒸馏方法中（克劳利等人，2018 年；黄和王，2017 年；斯里尼瓦斯和弗勒雷，2018 年；扎戈鲁伊科和科莫达基斯，2017 年），定义了不同的注意力转移机制，用于将知识从教师网络蒸馏到学生网络。注意力转移的核心是为神经网络各层的特征嵌入定义注意力图。也就是说，关于特征嵌入的知识是通过注意力图函数来转移的。与注意力图不同，宋等人（2018 年）提出了一种不同的注意力知识蒸馏方法。该方法使用注意力机制来分配不同的置信度规则（宋等人，2018 年）。

## 5.6 无数据蒸馏
为了克服由于隐私、合法性、安全性和保密性等问题导致的数据不可用的问题，已经提出了一些无数据知识蒸馏方法（陈等人，2019 年 a；洛佩斯等人，2017 年；纳亚克等人，2019 年；米凯埃利和斯托基，2019 年；哈鲁什等人，2020 年；叶等人，2020 年；纳亚克等人，2021 年；乔拉等人，2021 年）。正如“无数据”所暗示的，这种方法没有训练数据。相反，数据是新生成的或合成生成的。具体来说，在（陈等人，2019 年 a；叶等人，2020 年；米凯埃利和斯托基，2019 年；柳等人，2019 年；胡等人，2020 年）中，转移数据是由生成对抗网络生成的。在提出的无数据知识蒸馏方法（洛佩斯等人，2017 年）中，用于训练学生网络的转移数据是通过使用教师网络的层激活或层频谱激活来重构的。尹等人（2020 年）提出了 DeepInversion，它使用知识蒸馏来生成合成图像，以进行无数据知识转移。纳亚克等人（2019 年）提出了零样本知识蒸馏，该方法不使用现有的数据。转移数据是通过使用教师网络的参数对 softmax 空间进行建模而产生的。实际上，（米凯埃利和斯托基，2019 年；纳亚克等人，2019 年）中的目标数据是通过使用教师网络的特征表示信息生成的。与零样本学习类似，通过将知识从教师模型蒸馏到学生神经网络中，设计了一种少样本学习的知识蒸馏方法（木村等人，2018 年；沈等人，2021 年）。教师使用有限的标记数据。此外，还有一种新型的蒸馏方法称为数据蒸馏，它与无数据蒸馏类似（拉多萨沃维奇等人，2018 年；刘等人，2019 年 d；张等人，2020 年 d）。在数据蒸馏中，从教师模型生成的未标记数据的新训练注释被用于训练学生模型。
![](https://borninfreedom.github.io/images/2025/03/kd/14.png)


总之，无数据蒸馏中的合成数据通常是由预训练教师模型的特征表示生成的，如图 14 所示。尽管无数据蒸馏在数据不可用的情况下显示出了巨大的潜力，但它仍然是一项非常具有挑战性的任务，即如何生成高质量的多样化训练数据以提高模型的泛化能力。 

## 5.7 量化蒸馏
![](https://borninfreedom.github.io/images/2025/03/kd/15.png)


网络量化通过将高精度网络（如 32 位浮点数）转换为低精度网络（如 2 位和 8 位）来降低神经网络的计算复杂度。同时，知识蒸馏旨在训练一个小型模型，使其性能可与复杂模型相媲美。一些知识蒸馏方法已被提出，它们在师生框架中利用量化过程（波利诺等人，2018 年；米什拉和马尔，2018 年；魏等人，2018 年；申等人，2019 年；金等人，2019 年 a）。量化蒸馏方法的框架如图 15 所示。

具体而言，波利诺等人（2018 年）提出了一种量化蒸馏方法，将知识转移到权重量化的学生网络中。在（米什拉和马尔，2018 年）中，所提出的量化知识蒸馏被称为“学徒”方法。高精度的教师网络将知识转移到小型低精度的学生网络中。为确保小型学生网络能准确模仿大型教师网络，先对全精度教师网络的特征图进行量化，然后将知识从量化后的教师网络转移到量化后的学生网络（魏等人，2018 年）。金等人（2019 年 a）提出了量化感知知识蒸馏方法，该方法基于量化学生网络的自主学习以及师生网络通过知识转移的共同学习。此外，申等人（2019 年）对深度神经网络同时进行了蒸馏和量化的实证分析，考虑了知识蒸馏的超参数，如教师网络的规模和蒸馏温度。最近，与上述量化蒸馏方法不同，一种自蒸馏训练方案被设计出来以提高量化深度模型的性能，在该方案中教师与学生共享模型参数（布等人，2021 年）。

## 5.8 终身蒸馏
终身学习，包括持续学习、连续学习和元学习，旨在以类似于人类的方式进行学习。它积累先前学到的知识，并将所学知识转移到未来的学习中（陈和刘，2018 年）。知识蒸馏提供了一种有效的方式来保留和转移所学知识，而不会出现灾难性遗忘。最近，越来越多基于终身学习的知识蒸馏变体被开发出来（张等人，2019 年；弗莱内哈格等人，2019 年；彭等人，2019 年 b；刘等人，2019 年 e；李等人，2019 年 b；翟等人，2019 年；周等人，2020 年；什梅利科夫等人，2017 年；李和霍伊姆，2017 年；卡西亚等人，2020 年）。（张等人，2019 年；彭等人，2019 年 b；刘等人，2019 年 e；弗莱内哈格等人，2019 年）中提出的方法采用了元学习。张等人（2019 年）设计了元转移网络，该网络可以确定在师生架构中转移什么知识以及向何处转移。弗莱内哈格等人（2019 年）提出了一个名为 Leap 的轻量级框架，用于通过将知识从一个学习过程转移到另一个学习过程，在任务流形上进行元学习。彭等人（2019 年 b）为少样本图像识别设计了一种新的知识转移网络架构。该架构同时整合了来自图像的视觉信息和先验知识。刘等人（2019 年 e）提出了用于图像检索的语义感知知识保留方法。从图像模态和语义信息中获得的教师知识得到了保留和转移。

此外，为了解决终身学习中的灾难性遗忘问题，全局蒸馏（李等人，2019 年 b）、基于知识蒸馏的终身生成对抗网络（翟等人，2019 年）、多模型蒸馏（周等人，2020 年）以及其他基于知识蒸馏的方法（李和霍伊姆，2017 年；什梅利科夫等人，2017 年）被开发出来，用于提取所学知识并在新任务上教导学生网络。

## 5.9 基于神经架构搜索的蒸馏
神经架构搜索（NAS）是最流行的自动机器学习（或 AutoML）技术之一，旨在自动识别深度神经模型并自适应地学习合适的深度神经结构。在知识蒸馏中，知识转移的成功不仅取决于教师的知识，还取决于学生的架构。然而，大型教师模型和小型学生模型之间可能存在容量差距，这使得学生难以从教师那里学好知识。为了解决这个问题，神经架构搜索已被用于在基于神谕的（康等人，2020 年）和架构感知的知识蒸馏（刘等人，2019 年 i）中找到合适的学生架构。此外，知识蒸馏被用于提高神经架构搜索的效率，例如 AdaNAS（马科等人，2019 年）、具有蒸馏架构知识的 NAS（李等人，2020 年 a）、教师引导的架构搜索（TGSA，巴希万等人，2019 年）以及一次性 NAS（彭等人，2020 年）。在 TGSA 中，每个架构搜索步骤都被引导去模仿教师网络的中间特征表示。教师有效地搜索学生可能的结构，并对特征转移进行有效的监督。

# 6 性能比较
![](https://borninfreedom.github.io/images/2025/03/kd/t5.png)
![](https://borninfreedom.github.io/images/2025/03/kd/t6.png)

知识蒸馏是一种出色的模型压缩技术。通过捕捉教师知识并采用师生学习的蒸馏策略，它为轻量级学生模型提供了有效的性能。最近，许多知识蒸馏方法专注于提高性能，尤其是在图像分类任务中。在本节中，为了清楚地展示知识蒸馏的有效性，我们总结了一些典型知识蒸馏方法在两个流行的图像分类数据集上的分类性能。这两个数据集是 CIFAR10 和 CIFAR100（克里热夫斯基和辛顿，2009 年），它们分别由取自 10 个和 100 个类别的 32×32 RGB 图像组成。两个数据集都有 50000 张训练图像和 10000 张测试图像，并且每个类别的训练图像和测试图像数量相同。为了进行公平比较，知识蒸馏方法的实验分类准确率结果（%）直接来自相应的原始论文，CIFAR10 的结果见表 5，CIFAR100 的结果见表 6。我们报告了在使用不同类型的知识、蒸馏方案以及教师/学生模型结构时不同方法的性能。具体来说，括号内的准确率是教师模型和学生模型单独训练的分类结果。需要注意的是，DML（张等人，2018 年 b）、DCM（姚和孙，2020 年）和 KDCL（郭等人，2020 年）的准确率对是在线蒸馏后教师和学生的性能。

从表 5 和表 6 的性能比较中，可以总结出以下几点：
- 知识蒸馏可以在不同的深度模型上实现。
- 知识蒸馏可以实现不同深度模型的模型压缩。
- 通过协作学习进行的在线知识蒸馏（张等人，2018 年 b；姚和孙，2020 年）可以显著提高深度模型的性能。
- 自知识蒸馏（杨等人，2019 年 b；袁等人，2020 年；徐和刘，2019 年；云等人，2020 年）可以很好地提高深度模型的性能。
- 离线和在线蒸馏方法通常分别转移基于特征的知识和基于响应的知识。
- 高容量教师模型的知识转移可以提高轻量级深度模型（学生模型）的性能。
通过对不同知识蒸馏方法的性能比较，可以很容易地得出结论：知识蒸馏是一种有效且高效的压缩深度模型的技术。 

# 7 应用
作为一种对深度神经网络进行压缩和加速的有效技术，知识蒸馏已广泛应用于人工智能的不同领域，包括视觉识别、语音识别、自然语言处理（NLP）以及推荐系统。此外，知识蒸馏还可用于其他目的，例如数据隐私保护以及抵御对抗性攻击。本节简要回顾知识蒸馏的应用情况。
## 7.1 知识蒸馏在视觉识别中的应用
在过去几年里，各种各样的知识蒸馏方法已广泛应用于不同视觉识别应用中的模型压缩。具体而言，大多数知识蒸馏方法最初是为图像分类而开发的（李和霍伊姆，2017 年；彭等人，2019 年 b；巴盖里内扎德等人，2018 年；陈等人，2018 年 a；王等人，2019 年 b；慕克吉等人，2019 年；朱等人，2019 年），然后被扩展到其他视觉识别应用中，包括人脸识别（罗等人，2016 年；孔等人，2019 年；严等人，2019 年；葛等人，2018 年；王等人，2018 年 b，2019 年 c；杜昂等人，2019 年；吴等人，2020 年；王等人，2017 年；张等人，2020 年 b；王等人，2020 年 b）、图像 / 视频分割（何等人，2019 年；穆拉普迪等人，2019 年；窦等人，2020 年；刘等人，2019 年 h；西亚姆等人，2019 年；侯等人，2020 年；伯格曼等人，2020 年）、动作识别（罗等人，2018 年；郝和张，2019 年；托克尔和加尔，2019 年；加西亚等人，2018 年；王等人，2019 年 e；吴等人，2019 年 b；张等人，2020 年 c；崔等人，2020 年）、目标检测（李等人，2017 年；什梅利科夫等人，2017 年；村和潘，2020 年；王等人，2019 年 d；黄等人，2020 年；魏等人，2018 年；洪和余，2019 年；乔拉等人，2021 年）、车道检测（侯等人，2019 年）、行人重识别（吴等人，2019 年 a）、行人检测（沈等人，2016 年）、面部关键点检测（董和杨，2019 年）、姿态估计（聂等人，2019 年；张等人，2019 年 a；赵等人，2018 年）、视频字幕生成（潘等人，2020 年；张等人，2020 年 f）、行人搜索（蒙贾尔等人，2019 年；张等人，2021 年 c）、图像检索（刘等人，2019 年 e）、阴影检测（陈等人，2020 年 c）、显著性估计（李等人，2019 年）、深度估计（皮尔泽等人，2019 年；叶等人，2019 年）、视觉里程计（萨普特拉等人，2019 年）、文本到图像合成（袁和彭，2020 年；谭等人，2021 年）、视频分类（张和彭，2018 年；巴尔达瓦伊等人，2019 年）、视觉问答（蒙等人，2018 年；阿迪蒂亚等人，2019 年）以及异常检测（伯格曼等人，2020 年）。由于分类任务中的知识蒸馏是其他任务的基础，我们简要回顾在具有挑战性的图像分类场景（如人脸识别和动作识别）中的知识蒸馏情况。

现有的基于知识蒸馏的人脸识别方法不仅注重高效部署，还注重具有竞争力的识别准确率（罗等人，2016 年；孔等人，2019 年；严等人，2019 年；葛等人，2018 年；王等人，2018 年 b，2019 年 c；杜昂等人，2019 年；王等人，2017 年，2020 年 b；张等人，2020 年 b）。具体来说，在（罗等人，2016 年）中，教师网络顶层提示层中选定的信息神经元的知识被转移到学生网络中。为了进行知识转移，设计了一种教师加权策略，该策略利用提示层的特征表示损失，以避免教师的错误监督（王等人，2018 年 b）。通过使用前一个学生网络初始化下一个学生网络，设计了一种递归知识蒸馏方法（严等人，2019 年）。由于大多数人脸识别方法执行开放集识别，即测试集中的类别 / 身份对于训练集来说是未知的，因此人脸识别标准通常是正负样本特征表示之间的距离度量，例如（杜昂等人，2019 年）中的角度损失和（吴等人，2020 年）中的相关嵌入损失。

为了提高低分辨率人脸识别的准确率，开发了知识蒸馏框架，该框架使用高分辨率人脸教师模型和低分辨率人脸学生模型之间的架构，以实现模型加速和提高分类性能（葛等人，2018 年；王等人，2019 年 c；孔等人，2019 年；葛等人，2020 年）。具体而言，葛等人（2018 年）提出了一种选择性知识蒸馏方法，在该方法中，用于高分辨率人脸识别的教师网络通过稀疏图优化，有选择地将其有用的面部特征转移到用于低分辨率人脸识别的学生网络中。在（孔等人，2019 年）中，通过设计一个统一了人脸虚拟和异构识别子网络的分辨率不变模型，实现了跨分辨率人脸识别。为了获得高效有效的低分辨率人脸识别模型，采用了学生网络和教师网络之间的多核最大均值差异作为特征损失（王等人，2019 年 c）。此外，通过改变知识蒸馏中的损失，基于知识蒸馏的人脸识别可以扩展到人脸对齐和验证（王等人，2017 年）。

最近，知识蒸馏已成功用于解决复杂的图像分类问题（朱等人，2019 年；巴盖里内扎德等人，2018 年；彭等人，2019 年 b；李和霍伊姆，2017 年；陈等人，2018 年 a；王等人，2019 年 b；慕克吉等人，2019 年）。对于不完整、模糊和冗余的图像标签，提出了通过自蒸馏和标签递进的标签精炼模型，用于为复杂图像分类学习软的、有信息的、集体的和动态的标签（巴盖里内扎德等人，2018 年）。为了解决在各种图像分类任务中卷积神经网络（CNN）的灾难性遗忘问题，提出了一种针对 CNN 的无遗忘学习方法，该方法包括知识蒸馏和终身学习，用于识别新的图像任务并保留原始任务（李和霍伊姆，2017 年）。为了提高图像分类准确率，陈等人（2018 年 a）提出了基于生成对抗网络（GAN）的基于特征图的知识蒸馏方法。它将知识从特征图转移到学生模型中。利用知识蒸馏，为图像分类器设计了一个统一了解释用师生模型和诊断用深度生成模型的视觉解释和诊断框架（王等人，2019 年 b）。与基于知识蒸馏的低分辨率人脸识别类似，朱等人（2019 年）针对低分辨率图像分类提出了深度特征蒸馏，其中学生模型的输出特征与教师模型的输出特征相匹配。

如 5.3 节所述，具有师生结构的知识蒸馏可以转移和保留跨模态知识。在跨模态任务场景下，高效有效的动作识别可以成功实现（托克尔和加尔，2019 年；罗等人，2018 年；加西亚等人，2018 年；郝和张，2019 年；吴等人，2019 年 b；张等人，2020 年 c）。这些方法是用于动作识别的具有不同知识转移的时空模态蒸馏的例子。例如，相互师生网络（托克尔和加尔，2019 年）、多流网络（加西亚等人，2018 年）、时空蒸馏密集连接网络（郝和张，2019 年）、图蒸馏（罗等人，2018 年）以及多教师对多学生网络（吴等人，2019 年 b；张等人，2020 年 c）。在这些方法中，轻量级学生模型可以提取并共享存储在教师模型中的来自多种模态的知识信息。

我们总结了基于蒸馏的视觉识别应用的两个主要发现，如下所示。

知识蒸馏为各种不同的视觉识别任务提供了高效有效的师生学习方式，因为轻量级学生网络可以在高容量教师网络的指导下轻松训练。

由于灵活的师生架构和知识转移，知识蒸馏可以充分利用复杂数据源中的不同类型知识，例如跨模态数据、多域数据、多任务数据和低分辨率数据。

## 7.2 知识蒸馏在自然语言处理中的应用
像 BERT 这样的传统语言模型结构复杂繁琐，非常耗时且耗费资源。为了获得轻量级、高效且有效的语言模型，知识蒸馏在自然语言处理（NLP）领域得到了广泛研究。越来越多的知识蒸馏方法被提出用于解决众多自然语言处理任务（刘等人，2019 年 b；戈登和杜，2019 年；海达尔和雷扎霍利扎德，2019 年；杨等人，2020 年 d；唐等人，2019 年；胡等人，2018 年；孙等人，2019 年；纳卡肖勒和弗劳格，2017 年；焦等人，2020 年；王等人，2018 年 d；周等人，2019 年 a；桑等人，2019 年；图尔克等人，2019 年；阿罗拉等人，2019 年；克拉克等人，2019 年；金和拉什，2016 年；牟等人，2016 年；刘等人，2019 年 f；哈恩和崔，2019 年；谭等人，2019 年；昆科罗等人，2016 年；崔等人，2017 年；魏等人，2019 年；弗赖塔格等人，2017 年；沙克里等人，2019 年；阿吉拉尔等人，2020 年；傅等人，2021 年；杨等人，2020 年 d；张等人，2021 年 b；陈等人，2020 年 b；王和杜，2021 年）。现有的使用知识蒸馏的自然语言处理任务包括神经机器翻译（NMT）（哈恩和崔，2019 年；周等人，2019 年 a；李等人，2021 年；金和拉什，2016 年；戈登和杜，2019 年；谭等人，2019 年；魏等人，2019 年；弗赖塔格等人，2017 年；张等人，2021 年 b）、文本生成（陈等人，2020 年 b；海达尔和雷扎霍利扎德，2019 年）、问答系统（胡等人，2018 年；王等人，2018 年 d；阿罗拉等人，2019 年；杨等人，2020 年 d）、事件检测（刘等人，2019 年 b）、文档检索（沙克里等人，2019 年）、文本识别（王和杜，2021 年）等等。在这些基于知识蒸馏的自然语言处理方法中，大多数属于自然语言理解（NLU），并且许多用于自然语言理解的知识蒸馏方法被设计为特定任务蒸馏（唐等人，2019 年；图尔克等人，2019 年；牟等人，2016 年）和多任务蒸馏（刘等人，2019 年 f；杨等人，2020 年 d；桑等人，2019 年；克拉克等人，2019 年）。接下来，我们先描述用于神经机器翻译的知识蒸馏研究工作，然后是在自然语言理解中对名为双向变换器表征（BERT）（德夫林等人，2019 年）的典型多语言表示模型的扩展研究。

在自然语言处理中，神经机器翻译是最热门的应用。然而，现有的具有竞争力的神经机器翻译模型规模非常大。为了获得轻量级的神经机器翻译模型，有许多针对神经机器翻译的扩展知识蒸馏方法（哈恩和崔，2019 年；周等人，2019 年 a；金和拉什，2016 年；戈登和杜，2019 年；魏等人，2019 年；弗赖塔格等人，2017 年；谭等人，2019 年）。最近，周等人（2019 年 a）通过实验证明，基于知识蒸馏的非自回归机器翻译（NAT）模型的更好性能在很大程度上依赖于其容量以及通过知识转移得到的蒸馏数据。戈登和杜（2019 年）从数据增强和正则化的角度解释了序列级知识蒸馏的良好性能。在（金和拉什，2016 年）中，有效的词级知识蒸馏在神经机器翻译的序列生成场景中被扩展到序列级。序列生成学生模型模仿教师模型的序列分布。为了克服多语言的多样性，谭等人（2019 年）提出了多教师蒸馏，其中多个处理双语对的单个模型作为教师模型，一个多语言模型作为学生模型。为了提高翻译质量，弗赖塔格等人（2017 年）使用数据过滤方法，让多个神经机器翻译模型的集合作为教师模型来监督学生模型。为了提高机器翻译和机器阅读任务的性能，（魏等人，2019 年）提出了一种新颖的在线知识蒸馏方法，该方法解决了训练过程的不稳定性以及每个验证集上性能下降的问题。在这种在线知识蒸馏中，训练过程中评估最好的模型被选作教师模型，并由随后任何更好的模型进行更新。如果下一个模型性能较差，当前的教师模型将对其进行指导。

作为一种多语言表示模型，BERT（双向编码器表征变换器）在自然语言理解领域引起了广泛关注（德夫林等人，2019 年），但它也是一个结构复杂、部署难度较大的深度模型。

为了解决这个问题，研究人员提出了几种利用知识蒸馏技术的 BERT 轻量化变体（即 BERT 模型压缩）（孙等人，2019 年；焦等人，2020 年；唐等人，2019 年；桑等人，2019 年；王等人，2020 年 a；刘等人，2020 年 b；傅等人，2021 年）。孙等人（2019 年）提出了用于 BERT 模型压缩的耐心知识蒸馏方法（BERT-PKD），该方法可应用于情感分类、释义相似度匹配、自然语言推理以及机器阅读理解等任务。在耐心知识蒸馏方法中，教师模型提示层中 [CLS] 标记的特征表示会被转移到学生模型中。

为了加速语言推理过程，焦等人（2020 年）提出了 TinyBERT，这是一种两阶段的变换器知识蒸馏模型，包含通用领域知识蒸馏和特定任务知识蒸馏。对于句子分类和匹配任务，唐等人（2019 年）提出了从 BERT 教师模型到双向长短期记忆网络（BiLSTM）的特定任务知识蒸馏方法。桑等人（2019 年）设计了一个名为 DistilBERT 的轻量级学生模型，它与 BERT 具有相同的通用结构，并在多种自然语言处理任务上进行了学习。阿吉拉尔等人（2020 年）通过内部蒸馏的方式，利用大型教师 BERT 模型的内部表示，提出了一个简化的学生 BERT 模型。

此外，以下介绍了一些从不同角度出发的自然语言处理领域的典型知识蒸馏方法。在问答任务方面，为了提高机器阅读理解的效率和鲁棒性，胡等人（2018 年）提出了一种注意力引导的答案蒸馏方法，该方法融合了通用蒸馏和答案蒸馏，以避免产生混淆性的答案。对于特定任务蒸馏（图尔克等人，2019 年），研究了预训练、蒸馏和微调之间的相互作用对紧凑学生模型知识蒸馏性能的影响。所提出的预训练蒸馏方法在情感分类、自然语言推理和文本蕴含等任务中表现出色。

在自然语言理解的多任务蒸馏场景中，克拉克等人（2019 年）提出了基于重生神经网络（富拉内洛等人，2018 年）的单任务 - 多任务重生蒸馏方法。单任务教师模型对多任务学生模型进行训练。在多语言表示方面，知识蒸馏可以在多语言词嵌入之间转移知识，用于双语词典的生成（纳卡肖勒和弗劳格，2017 年）。对于资源稀缺的语言，知识转移在多语言模型集合中是有效的（崔等人，2017 年）。

关于自然语言处理中的知识蒸馏，总结出以下几点观察结果：
- 知识蒸馏提供了高效且有效的轻量级语言深度模型。大容量的教师模型能够从大量不同类型的语言数据中转移丰富的知识，用于训练小型学生模型，使学生模型能够快速且高效地完成许多语言任务。
- 考虑到多语言模型的知识可以相互转移和共享，师生之间的知识转移能够轻松且有效地解决许多多语言任务。
- 在深度语言模型中，序列知识能够有效地从大型网络转移到小型网络中。 

## 7.3 知识蒸馏在语音识别中的应用
在语音识别领域，深度神经声学模型因其强大的性能而备受关注。然而，越来越多的实时语音识别系统被部署在计算资源有限且要求快速响应的嵌入式平台上。最先进的深度复杂模型无法满足此类语音识别场景的需求。为了满足这些要求，知识蒸馏在许多语音识别任务中得到了广泛的研究和应用。有许多知识蒸馏系统可用于设计用于语音识别的轻量级深度声学模型（切博塔和沃特斯，2016 年；王和盖尔斯，2016 年；陈等人，2015 年；普赖斯等人，2016 年；福田等人，2017 年；白等人，2019 年；吴等人，2018 年；阿尔巴尼等人，2018 年；卢等人，2017 年；施等人，2019 年 a；罗赫达等人，2018 年；施等人，2019 年 b；高等人，2019 年；戈尔巴尼等人，2018 年；高岛等人，2018 年；渡边等人，2017 年；施等人，2019 年 c；浅见等人，2017 年；黄等人，2018 年；沈等人，2018 年；佩雷斯等人，2020 年；沈等人，2019 年 c；奥尔德等人，2018 年；权等人，2020 年；沈等人，2020 年）。特别是，这些基于知识蒸馏的语音识别应用涵盖了语言识别（沈等人，2018 年、2019 年 c、2020 年）、音频分类（高等人，2019 年；佩雷斯等人，2020 年）、文本无关的说话人识别（吴等人，2018 年）、语音增强（渡边等人，2017 年）、声学事件检测（普赖斯等人，2016 年；施等人，2019 年 a、b）、语音合成（奥尔德等人，2018 年）等等。

现有的大多数语音识别知识蒸馏方法都采用师生架构来提高声学模型的效率和识别准确率（陈等人，2015 年；渡边等人，2017 年；切博塔和沃特斯，2016 年；沈等人，2019 年 c；卢等人，2017 年；沈等人，2018 年、2020 年；高等人，2019 年；施等人，2019 年 c、a；佩雷斯等人，2020 年）。通过使用循环神经网络（RNN）来保存语音序列中的时间信息，教师 RNN 声学模型的知识被转移到一个小型的学生深度神经网络（DNN）模型中（陈等人，2015 年）。通过结合多种声学模式可以获得更好的语音识别准确率。设计了具有不同个体训练标准的不同 RNN 的集合，通过知识转移来训练学生模型（切博塔和沃特斯，2016 年）。经过学习的学生模型在 5 种语言的 2000 小时大词汇量连续语音识别（LVCSR）任务中表现出色。为了增强短话语语言识别（LID）模型的泛化能力，基于长话语的教师网络的特征表示知识被转移到基于短话语的学生网络中，该学生网络能够区分短话语，并且在基于短时长话语的语言识别任务中表现良好（沈等人，2018 年）。为了进一步提高基于短话语的语言识别性能，提出了一种交互式师生在线蒸馏学习方法，以增强短话语特征表示的性能（沈等人，2019 年 c）。通过将教师在长话语上的内部表示知识蒸馏到学生在短话语上的表示知识中，也提高了短话语的语言识别性能（沈等人，2020 年）。

同时，对于音频分类，开发了一种多层次特征蒸馏方法，并采用对抗学习策略来优化知识转移（高等人，2019 年）。为了提高语音识别的抗噪能力，知识蒸馏被用作语音增强的工具（渡边等人，2017 年）。在（佩雷斯等人，2020 年）中，提出了一种视听多模态知识蒸馏方法。知识从基于视觉和声学数据的教师模型转移到基于音频数据的学生模型中。本质上，这种蒸馏在师生之间共享了跨模态知识（佩雷斯等人，2020 年；阿尔巴尼等人，2018 年；罗赫达等人，2018 年）。为了实现高效的声学事件检测，提出了一种结合知识蒸馏和量化的量化蒸馏方法（施等人，2019 年 a）。量化蒸馏将具有更高检测准确率的大型卷积神经网络（CNN）教师模型的知识转移到量化的循环神经网络（RNN）学生模型中。

与大多数现有的传统帧级知识蒸馏方法不同，序列级知识蒸馏在一些用于语音识别的序列模型中表现更好，例如联结主义时间分类（CTC）（王和盖尔斯，2016 年；高岛等人，2018 年；黄等人，2018 年）。在（黄等人，2018 年）中，将序列级知识蒸馏引入联结主义时间分类中，以便匹配教师模型训练中使用的输出标签序列和蒸馏中使用的输入语音帧。在（王和盖尔斯，2016 年）中，研究了帧级和序列级师生训练对语音识别性能的影响，并提出了一种新的序列级师生训练方法。教师集合是通过序列级组合而非帧级组合构建的。为了提高基于单向 RNN 的 CTC 用于实时语音识别的性能，通过帧级知识蒸馏和序列级知识蒸馏，将基于双向长短期记忆网络（LSTM）的 CTC 教师模型的知识转移到基于单向 LSTM 的 CTC 学生模型中（高岛等人，2018 年）。

此外，知识蒸馏还可用于解决语音识别中的一些特殊问题（白等人，2019 年；浅见等人，2017 年；戈尔巴尼等人，2018 年）。为了克服在数据稀缺时 DNN 声学模型的过拟合问题，知识蒸馏被用作一种正则化方法，在源模型的监督下训练适配模型（浅见等人，2017 年）。最终的适配模型在三个真实声学领域中取得了更好的性能。为了克服非母语语音识别性能下降的问题，通过从多个特定口音的 RNN-CTC 模型中蒸馏知识，训练了一个先进的多口音学生模型（戈尔巴尼等人，2018 年）。本质上，（浅见等人，2017 年；戈尔巴尼等人，2018 年）中的知识蒸馏实现了跨域知识转移。为了解决将外部语言模型（LM）融合到用于语音识别的序列到序列模型（Seq2seq）中的复杂性问题，知识蒸馏被用作一种有效的工具，将语言模型（教师）集成到 Seq2seq 模型（学生）中（白等人，2019 年）。经过训练的 Seq2seq 模型可以降低序列到序列语音识别中的字符错误率。

总之，关于基于知识蒸馏的语音识别，可以得出以下几点观察结果：
- 轻量级学生模型能够满足语音识别的实际需求，例如实时响应、有限资源的利用以及高识别准确率。
- 由于语音序列的时间特性，许多师生架构是基于 RNN 模型构建的。一般来说，RNN 模型被选作教师模型，它能够很好地保存并将来自真实声学数据的时间知识转移到学生模型中。
- 序列级知识蒸馏能够很好地应用于性能良好的序列模型。实际上，帧级知识蒸馏通常使用基于响应的知识，而序列级知识蒸馏通常从教师模型的提示层转移基于特征的知识。
- 利用师生知识转移的知识蒸馏能够轻松解决跨域或跨模态的语音识别问题，例如在多口音和多语言语音识别等应用中。 
## 7.4 知识蒸馏在其他应用中的情况
充分且正确地利用外部知识，例如用户评论或图像中的知识，对于深度推荐模型的有效性起着非常重要的作用。降低深度推荐模型的复杂度并提高其效率也十分必要。最近，知识蒸馏已成功应用于推荐系统中，用于深度模型的压缩和加速（陈等人，2018 年 b；唐和王，2018 年；潘等人，2019 年）。在（唐和王，2018 年）中，知识蒸馏首次被引入推荐系统，并被称为排序蒸馏，因为推荐被表示为一个排序问题。陈等人（2018 年 b）提出了一种用于高效推荐的对抗知识蒸馏方法。作为正确评论预测网络的教师模型监督着作为用户 - 物品预测网络（生成器）的学生模型。学生的学习通过教师和学生网络之间的对抗性自适应进行调整。与（陈等人，2018 年 b；唐和王，2018 年）中的蒸馏不同，潘等人（2019 年）通过知识蒸馏为推荐系统设计了一个增强的协作去噪自编码器（ECAE）模型，以从用户反馈中捕获有用知识并减少噪声。统一的 ECAE 框架包含一个生成网络、一个再训练网络和一个蒸馏层，该蒸馏层从生成网络中转移知识并减少噪声。

知识蒸馏还被用作一种有效的策略，利用其师生架构的自然特性，来解决深度模型的对抗攻击或扰动问题（帕佩诺等人，2016 年；罗斯和多希 - 韦莱兹，2018 年；戈德布卢姆等人，2020 年；吉尔等人，2019 年），以及因隐私、保密性和安全性问题导致的数据不可用问题（洛佩斯等人，2017 年；帕佩诺等人，2017 年；王等人，2019 年 a；白等人，2020 年；翁库尔比萨尔等人，2019 年）。具体来说，通过蒸馏，教师网络的鲁棒输出可以抵御对抗样本的扰动（罗斯和多希 - 韦莱兹，2018 年；帕佩诺等人，2016 年）。为了避免暴露隐私数据，多个教师访问敏感或未标记数据的子集，并监督学生（帕佩诺等人，2017 年；翁库尔比萨尔等人，2019 年）。为了解决隐私和安全问题，通过无数据蒸馏，利用教师网络的层激活或层频谱激活来生成用于训练学生网络的数据（洛佩斯等人，2017 年）。为了保护数据隐私并防止知识产权盗版，王等人（2019 年 a）提出了一种通过知识蒸馏的私有模型压缩框架。学生模型应用于公共数据，而教师模型应用于敏感数据和公共数据。这种私有知识蒸馏采用隐私损失和批量损失来进一步提高隐私性。为了考虑隐私和性能之间的平衡，白等人（2020 年）通过一种新颖的每层知识蒸馏方法，开发了一种少样本网络压缩方法，每个类别仅有少量样本。

当然，知识蒸馏还有其他一些特殊有趣的应用，例如神经架构搜索（马科等人，2019 年；巴希万等人，2019 年）、深度神经网络的可解释性（刘等人，2018 年 b）以及联邦学习（比斯特里茨等人，2020 年；林等人，2020 年；徐等人，2020 年；何等人，2020 年 a）。

# 8 结论与讨论
知识蒸馏及其应用在最近几年引起了广泛的关注。在本文中，我们从知识、蒸馏方案、师生架构、蒸馏算法、性能比较和应用等角度，对知识蒸馏进行了全面的综述。

## 8.1 挑战
对于知识蒸馏而言，关键在于：1）从教师模型中提取丰富的知识；2）将教师模型的知识进行转移，以指导学生模型的训练。因此，我们从以下几个方面来讨论知识蒸馏所面临的挑战：知识的质量、蒸馏的类型、师生架构的设计以及知识蒸馏背后的理论。

大多数知识蒸馏方法综合利用了不同类型的知识，包括基于响应的知识、基于特征的知识以及基于关系的知识。因此，了解每种知识类型的影响，以及不同类型的知识如何以互补的方式相互协作是很重要的。例如，基于响应的知识与标签平滑和模型正则化有着相似的目的（金和金，2017 年；米勒等人，2019 年；丁等人，2019 年）；基于特征的知识通常用于模仿教师模型的中间过程，而基于关系的知识则用于捕捉不同样本之间的关系。为此，在一个统一且互补的框架中对不同类型的知识进行建模仍然具有挑战性。例如，来自不同提示层的知识可能对学生模型的训练产生不同的影响：1）基于响应的知识来自最后一层；2）来自较深提示/引导层的基于特征的知识可能会受到过度正则化的影响（罗梅罗等人，2015 年）。

如何将教师模型的丰富知识转移到学生模型是知识蒸馏中的关键步骤。一般来说，现有的蒸馏方法可以分为离线蒸馏、在线蒸馏和自蒸馏。离线蒸馏通常用于从复杂的教师模型中转移知识，而在在线蒸馏和自蒸馏的设置中，教师模型和学生模型具有可比性。为了提高知识转移的效果，模型复杂度与现有蒸馏方案或其他新颖的蒸馏方案（孙等人，2021 年）之间的关系应进一步加以考虑。 

目前，大多数知识蒸馏（KD）方法都侧重于新型知识或蒸馏损失函数，而对师生架构的设计研究不足（Nowak和Corso，2018；Crowley等人，2018；Kang等人，2020；Liu等人，2019i；Ashok等人，2018；Liu等人，2019a）。事实上，除了知识和蒸馏算法外，师生结构之间的关系也会显著影响知识蒸馏的性能。例如，一方面，一些近期的研究发现，由于教师模型和学生模型之间存在模型容量差距，学生模型可能从某些教师模型中学到的东西很少（Zhang等人，2019b；Kang等人，2020）；另一方面，从一些早期对神经网络容量的理论分析来看，浅层网络能够学习到与深度神经网络相同的表示（Ba和Caruana，2014）。因此，设计有效的学生模型或构建合适的教师模型在知识蒸馏中仍然是具有挑战性的问题。

尽管有大量的知识蒸馏方法和应用，但对知识蒸馏的理解，包括理论解释和实证评估，仍然不足（Lopez - Paz等人，2016；Phuong和Lampert，2019a；Cho和Hariharan，2019）。例如，蒸馏可以被视为一种有特权信息的学习形式（Lopez - Paz等人，2016）。线性师生模型的假设使得通过蒸馏对学生学习特征的理论解释研究成为可能（Phuong和Lampert，2019a）。此外，Cho和Hariharan（2019）对知识蒸馏的有效性进行了一些实证评估和分析。然而，要深入理解知识蒸馏的泛化能力，尤其是如何衡量知识的质量或师生架构的质量，仍然非常困难。

## 8.2未来方向
为了提高知识蒸馏的性能，最重要的因素包括采用何种师生网络架构、从教师网络中学习何种知识以及将知识蒸馏到学生网络的何处。

深度神经网络的模型压缩和加速方法通常可分为四类，即参数剪枝和共享、低秩分解、迁移紧凑卷积滤波器和知识蒸馏（Cheng等人，2018）。在现有的知识蒸馏方法中，只有少数相关工作讨论了知识蒸馏与其他压缩方法的结合。例如，量化知识蒸馏可以看作是一种参数剪枝方法，它将网络量化集成到师生架构中（Polino等人，2018；Mishra和Marr，2018；Wei等人，2018）。因此，为了学习高效且有效的轻量级深度模型以部署在便携式平台上，通过知识蒸馏和其他压缩技术相结合的混合压缩方法是必要的，因为大多数压缩技术都需要重新训练/微调过程。此外，如何确定应用不同压缩方法的合适顺序将是未来研究的一个有趣课题。

除了用于深度神经网络加速的模型压缩外，由于师生架构上知识转移的天然特性，知识蒸馏还可以应用于其他问题。最近，知识蒸馏已应用于数据隐私和安全（Wang等人，2019a）、深度模型的对抗攻击（Papernot等人，2016）、跨模态（Gupta等人，2016）、多领域（Asami等人，2017）、灾难性遗忘（Lee等人，2019b）、加速深度模型学习（Chen等人，2016）、神经架构搜索的效率（Bashivan等人，2019）、自监督（Noroozi等人，2018）和数据增强（Lee等人，2019a；Gordon和Duh，2019）。另一个有趣的例子是，从小型教师网络到大型学生网络的知识转移可以加速学生的学习（Chen等人，2016）。这与传统的知识蒸馏非常不同。大型模型从未标记数据中学习到的特征表示也可以通过蒸馏来监督目标模型（Noroozi等人，2018）。为此，将知识蒸馏扩展到其他目的和应用可能是一个有意义的未来方向。

知识蒸馏的学习类似于人类的学习。将知识转移推广到经典和传统的机器学习方法是可行的（Zhou等人，2019b；Gong等人，2018；You等人，2018；Gong等人，2017）。例如，基于知识蒸馏的思想，传统的两阶段分类可以恰当地转化为单教师单学生问题（Zhou等人，2019b）。此外，知识蒸馏可以灵活地应用于各种优秀的学习方案中，如对抗学习（Liu等人，2018）、自动机器学习（Macko等人，2019；Fakoor等人，2020）、标签噪声过滤学习（Xia等人，2018）、终身学习（Zhai等人，2019）和强化学习（Ashok等人，2018；Xu等人，2020c；Zhao和Hospedales，2020）。因此，在未来，将知识蒸馏与其他学习方案相结合以应对实际挑战将是有益的。
