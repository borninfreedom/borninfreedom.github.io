---
title: "重读Attention Is All You Need"
date: 2025-02-24
permalink: /posts/2025/02/blog-post-9/
tags:
-  attention
---

论文链接：[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)


# 摘要
主流的序列转换模型基于复杂的循环神经网络或卷积神经网络，这些模型包含一个编码器和一个解码器。表现最佳的模型还通过注意力机制将编码器和解码器连接起来。我们提出了一种全新的简单网络架构——Transformer，它完全基于注意力机制，完全摒弃了循环和卷积。在两项机器翻译任务上的实验表明，这些模型在质量上更优，同时具有更高的并行化能力，训练所需时间也显著减少。在WMT 2014年英德翻译任务中，我们的模型达到了28.4的BLEU值，比包括集成模型在内的现有最佳结果提高了2个以上的BLEU值。在WMT 2014年英法翻译任务中，我们的模型在8个GPU上训练3.5天后，创造了41.8的单模型最新最优BLEU分数，这仅为文献中最佳模型训练成本的一小部分。我们通过将Transformer成功应用于使用大量及有限训练数据的英语成分句法分析任务，表明它在其他任务上也具有良好的通用性。 

# 1. 引言
循环神经网络，尤其是长短期记忆网络（LSTM）[13]和门控循环单元（GRU）[7]，已成为序列建模和转换问题（如语言建模和机器翻译[35, 2, 5]）中的主流先进方法。此后，众多研究不断拓展循环语言模型和编码器 - 解码器架构的边界[38, 24, 15]。

循环模型通常根据输入和输出序列的符号位置来分解计算。将位置与计算时间步长对应，它们会根据前一个隐藏状态$h_{t - 1}$和位置$t$的输入生成隐藏状态序列$h_t$。这种内在的顺序特性使得在训练样本内无法并行计算，当序列长度增加时，这一问题变得尤为关键，因为内存限制会影响跨样本的批量处理。近期的研究通过因式分解技巧[21]和条件计算[32]显著提高了计算效率，同时后者还提升了模型性能。然而，顺序计算的根本限制依然存在。

注意力机制已成为各种任务中出色的序列建模和转换模型的重要组成部分，它使得模型能够对依赖关系进行建模，而无需考虑其在输入或输出序列中的距离[2, 19]。不过，除了少数情况[27]，这种注意力机制通常与循环网络结合使用。

在这项工作中，我们提出了Transformer，这是一种摒弃循环结构，完全依赖注意力机制来捕捉输入与输出之间全局依赖关系的模型架构。Transformer允许更高程度的并行化，并且在8个P100 GPU上仅训练12小时，就能在翻译质量上达到新的领先水平。

# 2. 背景
减少顺序计算的目标也是扩展神经GPU（Extended Neural GPU）[16]、ByteNet [18]和ConvS2S [9]的基础，这些模型都使用卷积神经网络作为基本构建模块，对所有输入和输出位置并行计算隐藏表示。在这些模型中，关联两个任意输入或输出位置信号所需的操作数量会随着位置间距离的增加而增长，ConvS2S中呈线性增长，ByteNet中呈对数增长。这使得学习远距离位置之间的依赖关系变得更加困难[12]。在Transformer中，这一操作数量被减少到常数，尽管由于对注意力加权位置进行平均会导致有效分辨率降低，不过我们在3.2节中介绍的多头注意力机制可以抵消这种影响。

自注意力机制，有时也称为内部注意力，是一种关注单个序列不同位置之间关系，以计算序列表示的注意力机制。自注意力机制已成功应用于多种任务，包括阅读理解、抽象摘要、文本蕴含以及学习与任务无关的句子表示[4, 27, 28, 22]。

端到端记忆网络基于循环注意力机制，而非与序列对齐的循环结构，并且已被证明在简单语言问答和语言建模任务中表现良好[34]。

据我们所知，Transformer是首个完全依赖自注意力机制来计算输入和输出表示，而不使用与序列对齐的循环神经网络（RNN）或卷积的转换模型。在接下来的章节中，我们将描述Transformer，阐述自注意力机制的动机，并讨论它相对于[17, 18]和[9]等模型的优势。

# 3. 模型架构
大多数具有竞争力的神经序列转换模型都具有编码器 - 解码器结构[5, 2, 35]。在此结构中，编码器将符号表示的输入序列$(x_1, \ldots, x_n)$映射为连续表示的序列$z = (z_1, \ldots, z_n)$。给定$z$，解码器随后一次生成一个符号的输出序列$(y_1, \ldots, y_m)$。在每一步，模型都是自回归的[10]，在生成下一个符号时，将先前生成的符号作为额外输入。 


![](https://borninfreedom.github.io/images/2025/02/attn/1.png)

Transformer遵循这种整体架构，编码器和解码器均使用堆叠的自注意力层和逐点全连接层，分别如图1左半部分和右半部分所示。

## 3.1 编码器和解码器堆叠
 - **编码器**：编码器由N = 6个相同的层堆叠而成。每层有两个子层。第一个是多头自注意力机制，第二个是一个简单的逐点全连接前馈网络。我们在这两个子层的每一个周围都采用残差连接 [11]，然后进行层归一化 [1]。也就是说，每个子层的输出是LayerNorm(x + Sublayer(x))，其中Sublayer(x) 是子层自身实现的函数。为了便于这些残差连接，模型中的所有子层以及嵌入层，都产生维度为dmodel = 512的输出。
 - **解码器**：解码器同样由N = 6个相同的层堆叠而成。除了每个编码器层中的两个子层外，解码器插入了第三个子层，该子层对编码器堆叠的输出执行多头注意力操作。与编码器类似，我们在每个子层周围采用残差连接，然后进行层归一化。我们还修改了解码器堆叠中的自注意力子层，以防止位置关注后续位置。这种掩码操作，结合输出嵌入偏移一个位置的事实，确保位置i的预测仅能依赖于位置小于i的已知输出。

## 3.2 注意力机制
注意力函数可以描述为将一个查询和一组键值对映射到一个输出，其中查询、键、值和输出都是向量。输出计算为值的加权和，其中分配给每个值的权重由查询与相应键的兼容性函数计算得出。

### 3.2.1 缩放点积注意力

![](https://borninfreedom.github.io/images/2025/02/attn/2.png)

我们将我们特定的注意力称为“缩放点积注意力”（图2）。输入由维度为dk的查询和键，以及维度为dv的值组成。我们计算查询与所有键的点积，每个点积除以√dk ，并应用softmax函数以获得值上的权重。

在实践中，我们同时对一组查询计算注意力函数，将它们打包成一个矩阵Q。键和值也被打包成矩阵K和V。我们按如下方式计算输出矩阵：
Attention(Q, K, V) = softmax( QKT / √dk )V  （公式1）

两种最常用的注意力函数是加法注意力 [2] 和点积（乘法）注意力。点积注意力与我们的算法相同，只是没有1 / √dk这个缩放因子。加法注意力使用具有单个隐藏层的前馈网络来计算兼容性函数。虽然两者在理论复杂度上相似，但点积注意力在实践中速度更快且空间效率更高，因为它可以使用高度优化的矩阵乘法代码来实现。

当dk值较小时，这两种机制的表现类似，但对于较大的dk值，未缩放的点积注意力不如加法注意力 [3]。我们怀疑对于较大的dk值，点积的数值会变得很大，将softmax函数推向梯度极小的区域。为了抵消这种影响，我们将点积除以1 / √dk 。

### 3.2.2 多头注意力
我们发现，与其使用dmodel维的键、值和查询执行单个注意力函数，不如使用不同的、经过学习的线性投影，将查询、键和值分别线性投影h次，投影到dk、dk和dv维度，这样做很有益处。然后，我们在这些投影后的查询、键和值的每个版本上并行执行注意力函数，产生dv维的输出值。如图2所示，将这些值连接起来并再次投影，得到最终的值。

多头注意力使模型能够在不同位置联合关注来自不同表示子空间的信息。使用单个注意力头时，求平均操作会抑制这种能力。

![](https://borninfreedom.github.io/images/2025/02/attn/tmp1.png)

在这项工作中，我们采用h = 8个并行注意力层，即头。对于每个头，我们使用dk = dv = dmodel / h = 64。由于每个头的维度降低，总的计算成本与全维度的单头注意力类似。 


### 3.2.3 注意力在我们模型中的应用
Transformer以三种不同方式使用多头注意力：
 - 在“编码器 - 解码器注意力”层中，查询来自前一个解码器层，而记忆键和值来自编码器的输出。这使得解码器中的每个位置都能关注输入序列中的所有位置。这模仿了诸如[38, 2, 9]等序列到序列模型中典型的编码器 - 解码器注意力机制。
 - 编码器包含自注意力层。在自注意力层中，所有的键、值和查询都来自同一处，在此情况下，即编码器前一层的输出。编码器中的每个位置都能关注编码器前一层中的所有位置。
 - 类似地，解码器中的自注意力层允许解码器中的每个位置关注解码器中直至该位置（包括该位置）的所有位置。我们需要防止解码器中信息向左流动，以保持自回归特性。我们在缩放点积注意力内部通过屏蔽（设置为负无穷）softmax输入中所有对应非法连接的值来实现这一点。见图2。

## 3.3 逐点前馈网络
除了注意力子层，我们编码器和解码器中的每一层都包含一个全连接前馈网络，该网络分别且相同地应用于每个位置。它由两个线性变换组成，中间有一个ReLU激活函数。

FFN(x) = max(0, xW₁ + b₁)W₂ + b₂ （公式2）

虽然线性变换在不同位置上是相同的，但不同层使用不同的参数。另一种描述方式是将其看作两个内核大小为1的卷积。

输入和输出的维度为dmodel = 512，内层维度为dff = 2048。

## 3.4 嵌入与Softmax
与其他序列转换模型类似，我们使用可学习的嵌入将输入标记和输出标记转换为维度为dmodel的向量。我们还使用常见的可学习线性变换和softmax函数将解码器输出转换为预测的下一个标记概率。在我们的模型中，我们在两个嵌入层和softmax前的线性变换之间共享相同的权重矩阵，类似于[30]。在嵌入层中，我们将这些权重乘以√dmodel。

## 3.5 位置编码
由于我们的模型既不包含循环结构也不包含卷积，为了使模型能够利用序列的顺序，我们必须注入一些关于序列中标记相对或绝对位置的信息。为此，我们在编码器和解码器堆叠底部的输入嵌入中添加“位置编码”。位置编码与嵌入具有相同的维度dmodel，以便两者可以相加。位置编码有多种选择，包括可学习的和固定的[9]。

在这项工作中，我们使用不同频率的正弦和余弦函数：
![](https://borninfreedom.github.io/images/2025/02/attn/m1.png)


其中pos是位置，i是维度。也就是说，位置编码的每个维度对应一个正弦曲线。波长从2π到10000·2π形成几何级数。我们选择这个函数是因为我们假设它能让模型轻松学习通过相对位置进行关注，因为对于任何固定偏移k，PE(pos + k) 都可以表示为PE(pos) 的线性函数。

我们也尝试过使用可学习的位置嵌入[9]，发现这两种版本产生的结果几乎相同（见表3的(E)行）。我们选择正弦版本是因为它可能使模型能够外推到比训练期间遇到的更长的序列长度。

# 4 为什么选择自注意力机制


![](https://borninfreedom.github.io/images/2025/02/attn/t1.png)
表1：不同层类型的最大路径长度、每层复杂度和最少顺序操作数。其中，n为序列长度，d为表示维度，k为卷积核大小，r为受限自注意力中邻域的大小。 

在本节中，我们将自注意力层的各个方面与常用于将一个变长符号表示序列 (x₁,..., xₙ) 映射到等长的另一个序列 (z₁,..., zₙ) 的循环层和卷积层进行比较，其中xᵢ, zᵢ ∈ Rᵈ，例如典型序列转换编码器或解码器中的隐藏层。为了说明我们使用自注意力机制的动机，我们考虑三个需求。

一是每层的总计算复杂度。另一个是可以并行化的计算量，通过所需的最少顺序操作数来衡量。

第三个是网络中长距离依赖之间的路径长度。在许多序列转换任务中，学习长距离依赖是一个关键挑战。影响学习此类依赖能力的一个关键因素是信号在网络中向前和向后传播所必须经过的路径长度。输入和输出序列中任意位置组合之间的这些路径越短，就越容易学习长距离依赖[12]。因此，我们还比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。

如表1所示，自注意力层通过固定数量的顺序执行操作连接所有位置，而循环层需要O(n) 次顺序操作。在计算复杂度方面，当序列长度n小于表示维度d时，自注意力层比循环层更快，在机器翻译中最先进的模型所使用的句子表示（如词块[38]和字节对[31]表示）中，这种情况经常出现。为了提高涉及非常长序列的任务的计算性能，可以将自注意力限制为仅考虑以相应输出位置为中心、大小为r的输入序列邻域。这将使最大路径长度增加到O(n / r)。我们计划在未来的工作中进一步研究这种方法。

单个内核宽度k < n的卷积层不会连接所有输入和输出位置对。在连续内核的情况下，需要堆叠O(n / k) 个卷积层，在扩张卷积的情况下需要O(logₖ(n)) 个卷积层[18]，这会增加网络中任意两个位置之间最长路径的长度。卷积层通常比循环层计算成本更高，高出k倍。然而，可分离卷积[6]大大降低了复杂度，降至O(k·n·d + n·d²)。然而，即使k = n，可分离卷积的复杂度也与自注意力层和逐点前馈层的组合相同，这就是我们在模型中采用的方法。

作为一个附带好处，自注意力机制可以产生更具可解释性的模型。我们检查了模型中的注意力分布，并在附录中展示和讨论了示例。不仅各个注意力头明显学习执行不同的任务，许多注意力头似乎还表现出与句子的句法和语义结构相关的行为。 
![](https://borninfreedom.github.io/images/2025/02/attn/t2.png)

# 5. 训练
本节描述我们模型的训练机制。
## 5.1 训练数据与批量处理
我们在标准的WMT 2014英德数据集上进行训练，该数据集包含约450万个句子对。句子使用字节对编码（byte - pair encoding）[3]进行编码，源语言和目标语言共享一个约37000词元的词汇表。对于英法翻译任务，我们使用了规模大得多的WMT 2014英法数据集，该数据集包含3600万个句子，并将词元拆分为一个32000词块的词汇表[38]。句子对根据大致的序列长度进行批量组合。每个训练批次包含一组句子对，其中大约包含25000个源语言词元和25000个目标语言词元。

表3：Transformer架构的变体。未列出的值与基础模型的值相同。所有指标均基于英德翻译开发集newstest2013。列出的困惑度是基于我们字节对编码的每个词块的困惑度，不应与每个单词的困惑度进行比较。 
![](https://borninfreedom.github.io/images/2025/02/attn/t3.png)

## 5.2 硬件与训练计划
我们在一台配备8个NVIDIA P100 GPU的机器上训练模型。对于使用本文所述超参数的基础模型，每个训练步骤大约需要0.4秒。我们总共训练基础模型100,000步，即12小时。对于我们的大型模型（见表3最后一行描述），每个步骤时间为1.0秒。大型模型训练300,000步（3.5天）。
5.3 优化器
我们使用Adam优化器[20]，其中β1 = 0.9，β2 = 0.98，ϵ = 10⁻⁹。在训练过程中，我们根据以下公式调整学习率：

![](https://borninfreedom.github.io/images/2025/02/attn/tmp3.png)

这意味着在前warmup_steps个训练步骤中，学习率线性增加，之后则与步骤数的平方根成反比下降。我们使用warmup_steps = 4000。
## 5.4 正则化
在训练过程中，我们采用三种类型的正则化：
 - **残差随机失活**：我们在每个子层的输出添加到子层输入并进行归一化之前，对其应用随机失活（dropout）[33]。此外，我们还对编码器和解码器堆叠中嵌入与位置编码的和应用随机失活。对于基础模型，我们使用的随机失活率Pdrop = 0.1。
 - **标签平滑**：在训练过程中，我们采用值为ϵls = 0.1的标签平滑[36]。这会损害困惑度，因为模型会学习得更加不确定，但能提高准确率和BLEU分数。
# 6. 结果
## 6.1 机器翻译
在WMT 2014英德翻译任务中，大型Transformer模型（表2中的Transformer (big)）比之前报道的最佳模型（包括集成模型）高出超过2.0个BLEU值，创造了28.4的新的最先进BLEU分数。该模型的配置列于表3的最后一行。在8个P100 GPU上训练耗时3.5天。即使是我们的基础模型也超越了所有先前发布的模型和集成模型，且训练成本仅为任何竞争模型的一小部分。
在WMT 2014英法翻译任务中，我们的大型模型达到了41.0的BLEU分数，超越了所有先前发布的单模型，训练成本不到之前最先进模型的四分之一。用于英法翻译训练的Transformer (big) 模型使用的随机失活率Pdrop = 0.1，而非0.3。
对于基础模型，我们使用通过平均最后5个检查点得到的单个模型，这些检查点每隔10分钟保存一次。对于大型模型，我们平均最后20个检查点。我们使用束搜索（beam search），束宽为4，长度惩罚α = 0.6 [38]。这些超参数是在开发集上进行实验后选定的。在推理过程中，我们将最大输出长度设置为输入长度 + 50，但在可能的情况下提前终止[38]。

表2总结了我们的结果，并将我们的翻译质量和训练成本与文献中的其他模型架构进行了比较。我们通过将训练时间、使用的GPU数量以及每个GPU的持续单精度浮点运算能力估计值相乘，来估计训练一个模型所使用的浮点运算次数。
## 6.2 模型变体
为了评估Transformer不同组件的重要性，我们以不同方式改变基础模型，在开发集newstest2013上的英德翻译任务中衡量性能变化。我们使用上一节所述的束搜索，但不进行检查点平均。这些结果列于表3。
在表3的（A）行中，我们改变注意力头的数量以及注意力键和值的维度，同时保持计算量不变，如3.2.2节所述。虽然单头注意力比最佳设置差0.9个BLEU值，但注意力头过多也会导致质量下降。
在表3的（B）行中，我们观察到减小注意力键的大小dk会损害模型质量。这表明确定兼容性并非易事，并且可能需要比点积更复杂的兼容性函数。在（C）和（D）行中，我们进一步观察到，正如预期的那样，更大的模型性能更好，并且随机失活对于避免过拟合非常有帮助。在（E）行中，我们用可学习的位置嵌入[9]替换正弦位置编码，观察到与基础模型几乎相同的结果。
## 6.3 英语成分句法分析
为了评估Transformer是否能推广到其他任务，我们进行了英语成分句法分析实验。这项任务带来了特定的挑战：输出受到很强的结构约束，并且比输入长得多。此外，RNN序列到序列模型在小数据场景下无法取得最先进的结果[37]。
我们在宾州树库（Penn Treebank）的《华尔街日报》（WSJ）部分（约40000个训练句子）上训练了一个4层的Transformer，dmodel = 1024。我们还在半监督设置下对其进行训练，使用了更大的高置信度语料库和伯克利解析器（BerkleyParser）语料库，约有1700万个句子[37]。仅在WSJ设置下，我们使用16000词元的词汇表，在半监督设置下使用32000词元的词汇表。
我们仅在第22节开发集上进行了少量实验，以选择随机失活（包括注意力和残差，见5.4节）、学习率和束宽，所有其他参数与英德基础翻译模型保持不变。在推理过程中，我们将最大输出长度增加到输入长度 + 300。对于仅WSJ设置和半监督设置，我们都使用束宽为21，α = 0.3。

![](https://borninfreedom.github.io/images/2025/02/attn/t4.png)

表4中的结果表明，尽管缺乏针对特定任务的调整，我们的模型表现出奇地好，除了递归神经网络语法（Recurrent Neural Network Grammar）[8]之外，优于所有先前报道的模型。
与RNN序列到序列模型[37]相比，即使仅在40000个句子的WSJ训练集上进行训练，Transformer也优于伯克利解析器（Berkeley - Parser）[29]。
# 7. 结论
在这项工作中，我们提出了Transformer，这是首个完全基于注意力机制的序列转换模型，用多头自注意力取代了编码器 - 解码器架构中最常用的循环层。

对于翻译任务，Transformer的训练速度明显快于基于循环层或卷积层的架构。在WMT 2014英德和WMT 2014英法翻译任务中，我们都取得了新的最先进成果。在前一个任务中，我们的最佳模型甚至超越了所有先前报道的集成模型。
我们对基于注意力机制的模型的未来感到兴奋，并计划将其应用于其他任务。我们计划将Transformer扩展到涉及文本以外的输入和输出模态的问题，并研究局部受限的注意力机制，以有效地处理图像、音频和视频等大型输入和输出。使生成过程减少顺序性是我们的另一个研究目标。

我们用于训练和评估模型的代码可在https://github.com/tensorflow/tensor2tensor获取。


![](https://borninfreedom.github.io/images/2025/02/attn/3.png)
![](https://borninfreedom.github.io/images/2025/02/attn/4.png)
![](https://borninfreedom.github.io/images/2025/02/attn/5.png)
