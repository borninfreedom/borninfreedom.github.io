---
title: "ViT蒸馏CNN论文ScaleKD: Strong Vision Transformers Could Be Excellent Teachers论文解读"
date: 2025-03-26
permalink: /posts/2025/03/blog-post-6/
tags:
-  蒸馏
---


#### 核心贡献
本文提出了 **ScaleKD**，一种跨架构知识蒸馏方法，旨在将预训练的大型视觉变换器（ViT）的可扩展性高效迁移到不同架构的学生模型（如CNN、MLP和其他ViT变体）中。通过解决以下三个关键差异，ScaleKD显著提升了学生模型的性能：

1. **特征计算范式差异**：ViT依赖自注意力机制处理图像块序列，而CNN和MLP分别基于卷积和全连接操作。  
2. **模型规模差异**：ViT通常参数量更大，且在不同尺度下表现出更强的泛化能力。  
3. **知识密度差异**：预训练ViT从海量数据中学习的高密度知识难以直接迁移到小规模学生模型。

#### 方法设计
ScaleKD通过三个紧密耦合的组件实现目标：

1. **交叉注意力投影器（CAP）**  
   - 将学生模型的特征转换为ViT风格的令牌，利用位置编码和交叉注意力对齐语义与空间分辨率差异。  
2. **双重视角特征模仿（DFM）**  
   - **原始空间路径**：直接模仿教师模型的全局特征。  
   - **频率空间路径**：通过离散余弦变换（DCT）去除主导的零频分量，强调高频细节特征。  
3. **教师参数感知（TPP）**  
   - 构建代理路径，将学生的早期层与教师的深层连接，通过参数空间对齐传递预训练知识。

#### 实验结果
- **ImageNet-1K**：以Swin-L为教师，ScaleKD显著提升各类学生模型性能，例如ResNet-50（+3.39%）、ViT-S/16（+4.03%）、Mixer-S/16（+4.61%），且部分学生模型性能超过更大规模的基准模型（如ResNet-50超越ResNet-152）。  
- **下游任务迁移**：在MS-COCO目标检测、实例分割及ADE20K语义分割任务中，ScaleKD训练的模型均显著优于基线，验证了知识的有效迁移。  
- **高效替代预训练**：仅需1/195的训练数据量，ScaleKD即可达到甚至超越传统预训练方法（如CLIP、BEiT）的性能。

#### 创新与意义
- **跨架构对齐**：首次系统解决ViT与CNN/MLP间的架构差异，为异构模型蒸馏提供通用框架。  
- **可扩展性传递**：通过利用教师模型的规模与预训练数据优势，显著降低学生模型对大规模预训练的依赖。  
- **工程潜力**：在资源受限场景下，ScaleKD可作为高效训练范式，替代耗时耗力的预训练流程。

#### 局限性
- 未在超大规模教师模型（如ViT-22B）或学生模型（如ViT-L）上验证。  
- 随着教师模型规模增大，训练成本有所增加，需进一步优化效率。

#### 结论
ScaleKD通过创新性的特征对齐与知识传递机制，为跨架构知识蒸馏开辟了新方向，并在性能、效率与泛化性上展现了显著优势，具有重要的学术价值与应用潜力。

![](https://borninfreedom.github.io/images/2025/03/scaleKD/1.png)

本篇paper的主要贡献在于上图的蒸馏架构。我们下面结合架构和代码来说明。

# 交叉注意力投影器 CAP 图1（a）
上图的（a）图介绍的是交叉注意力投影器（CAP，见图1(a)），主要目的是为了弥合ViT与其他异构架构之间的特征计算范式差异，主要思想是利用交叉注意力机制对齐不同的模态，包括之前的一些类似工作，也是使用交叉注意力来对齐不同的模态。对于语义单元的差异，CAP利用位置嵌入和一个patchify stem 将CNN 和MLP 的语义单元转换为类似transformer 的标记（Token）。为了进一步弥合核心操作的差异，CAP 采用了交叉注意力操作和可训练的查询，这些查询与教师的特征共享相同的属性，用于在学生的特征上建模全局相互依赖关系。通过这种方式，CAP 可以在形式上对齐ViT 教师与异构学生之间的计算范式差异，CAP在蒸馏计算时没有单独使用，而是作为一个基础组件。

在提供的开源代码中，几个配置文件都是ViT来蒸馏CNN的，我们从配置文件中的蒸馏loss中可以找到最终使用的loss是FreqMaskingDistillLossv2。我们看他的实现，一开始就是对student和teacher的输出做一个AttentionProjector。AttentionProjector就是论文中图1的（a）图介绍的CAP。

```python
@MODELS.register_module()
class FreqMaskingDistillLossv2(nn.Module):

    def __init__(self,
                 name,
                 use_this,
                 alpha,
                 student_dims,
                 teacher_dims,
                 query_hw,
                 pos_hw,
                 pos_dims,
                 window_shapes=(1,1),
                 self_query=True,
                 softmax_scale=1.,
                 dis_freq='high',
                 num_heads=8
                 ):
        super(FreqMaskingDistillLossv2, self).__init__()
        self.alpha = alpha
        self.dis_freq = dis_freq
        self.self_query = self_query

        self.projector_0 = AttentionProjector(student_dims, teacher_dims, query_hw, pos_dims, window_shapes=window_shapes, self_query=self_query, softmax_scale=softmax_scale[0], num_heads=num_heads)
        self.projector_1 = AttentionProjector(student_dims, teacher_dims, query_hw, pos_dims, window_shapes=window_shapes, self_query=self_query, softmax_scale=softmax_scale[1], num_heads=num_heads)

```

我们看一下AttentionProjector的具体实现。

```python
class AttentionProjector(nn.Module):
    def __init__(self,
                 student_dims,
                 teacher_dims,
                 hw_dims,
                 pos_dims,
                 window_shapes=(1,1),
                 self_query=True,
                 softmax_scale=1.,
                 num_heads=8,
                 ):
        super(AttentionProjector, self).__init__()

        self.hw_dims = hw_dims
        self.student_dims = student_dims
        self.teacher_dims = teacher_dims

        self.proj_pos = nn.Sequential(nn.Conv2d(teacher_dims, teacher_dims, 1),
                                      nn.ReLU(),
                                      )

        self.proj_student = nn.Sequential(nn.Conv2d(student_dims, student_dims, 3, stride=1, padding=1),
                                      nn.BatchNorm2d(student_dims),
                                      nn.ReLU())

        # 位置编码嵌入，对CNN或者MLP的特征进行类似Transformer的位置编码嵌入
        self.pos_embed = nn.Parameter(torch.zeros(1, student_dims, hw_dims[0], hw_dims[1]), requires_grad=True)
        self.pos_attention = WindowMultiheadPosAttention(teacher_dims, num_heads=num_heads, input_dims=student_dims, pos_dims=pos_dims, window_shapes=window_shapes, softmax_scale=softmax_scale)
        self.ffn = FFN(embed_dims=teacher_dims, feedforward_channels=teacher_dims * 4)

        self.norm = nn.LayerNorm([teacher_dims])

        if self_query:
            self.query = nn.Embedding(hw_dims[0] * hw_dims[1], teacher_dims)
        else:
            self.query = None

        if self.pos_embed is not None:
            trunc_normal_(self.pos_embed, std=0.02)


    def forward(self, x, query=None):
        H, W = self.hw_dims
        N = x.shape[0]

        if query is not None:
            pos_emb = query.permute(0,2,1).reshape(N, -1, H, W).contiguous()
        elif self.query is not None:
            pos_emb = self.query.weight.view(1,H,W,self.teacher_dims).permute(0,3,1,2).repeat(N,1,1,1)
        else:
            raise NotImplementedError("There is no query!")

        preds_S = self.proj_student(x) + self.pos_embed.to(x.device)
        pos_emb = self.proj_pos(pos_emb)
        pos_emb = torch.flatten(pos_emb.permute(0, 2, 3, 1), 1, 2)

        fea_S = self.pos_attention(torch.flatten(preds_S.permute(0, 2, 3, 1), 1, 2), pos_emb)
        fea_S = self.ffn(self.norm(fea_S))

        return fea_S
```

一开始也是对student和teacher的输出进行映射，但是这里对teacher选择的是1x1的卷积，保证输入输出shape不变。而对student使用的是3x3,stride=1,pad=1的卷积，同样也可以保持输入输出shape一致。

这里采用不同映射操作的原因猜测是因为教师模型的特征已经经过充分训练，具有较强的语义信息。只需要轻量级的线性变换（1x1 卷积）来适配后续模块。学生模型的特征较弱，需要更复杂的操作（3x3 卷积 + BatchNorm + ReLU）来增强特征表达能力。通过这种方式，学生特征可以更接近教师特征，从而实现更好的知识蒸馏效果。

这种设计体现了教师和学生模型在知识蒸馏中的角色差异：

教师模型提供高质量的特征，映射操作尽量简单以保留其特性。
学生模型需要通过更复杂的映射操作来弥补其特征表达能力的不足，从而更好地学习和对齐教师的知识。


```python
       self.proj_pos = nn.Sequential(nn.Conv2d(teacher_dims, teacher_dims, 1),
                                      nn.ReLU(),
                                      )

        self.proj_student = nn.Sequential(nn.Conv2d(student_dims, student_dims, 3, stride=1, padding=1),
                                      nn.BatchNorm2d(student_dims),
                                      nn.ReLU())
```

然后就是对student的输出添加位置编码，正如作者在图1（a）中对CAP的解释所示，CAP利用位置嵌入和一个patchify stem 将CNN 和MLP 的语义单元转换为类似transformer 的标记（Token）。

```python
        self.pos_embed = nn.Parameter(torch.zeros(1, student_dims, hw_dims[0], hw_dims[1]), requires_grad=True)
```

接下来是Attention的计算，这里是通过WindowMultiheadPosAttention来计算的。
```python
self.pos_attention = WindowMultiheadPosAttention(teacher_dims, num_heads=num_heads, input_dims=student_dims, pos_dims=pos_dims, window_shapes=window_shapes, softmax_scale=softmax_scale)
```

WindowMultiheadPosAttention的实现为：

```python

class WindowMultiheadPosAttention(BaseModule):
    """Multi-head Attention Module.

    This module implements multi-head attention that supports different input
    dims and embed dims. And it also supports a shortcut from ``value``, which
    is useful if input dims is not the same with embed dims.

    Args:
        embed_dims (int): The embedding dimension.
        num_heads (int): Parallel attention heads.
        input_dims (int, optional): The input dimension, and if None,
            use ``embed_dims``. Defaults to None.
        attn_drop (float): Dropout rate of the dropout layer after the
            attention calculation of query and key. Defaults to 0.
        proj_drop (float): Dropout rate of the dropout layer after the
            output projection. Defaults to 0.
        dropout_layer (dict): The dropout config before adding the shortcut.
            Defaults to ``dict(type='Dropout', drop_prob=0.)``.
        qkv_bias (bool): If True, add a learnable bias to q, k, v.
            Defaults to True.
        qk_scale (float, optional): Override default qk scale of
            ``head_dim ** -0.5`` if set. Defaults to None.
        proj_bias (bool) If True, add a learnable bias to output projection.
            Defaults to True.
        v_shortcut (bool): Add a shortcut from value to output. It's usually
            used if ``input_dims`` is different from ``embed_dims``.
            Defaults to False.
        init_cfg (dict, optional): The Config for initialization.
            Defaults to None.
    """

    def __init__(self,
                 embed_dims,
                 num_heads,
                 window_shapes=(1,1),
                 input_dims=None,
                 attn_drop=0.,
                 proj_drop=0.,
                 softmax_scale=5.,
                 dropout_layer=dict(type='Dropout', drop_prob=0.),
                 qkv_bias=True,
                 qk_scale=None,
                 proj_bias=True,
                 v_shortcut=False,
                 use_layer_scale=False,
                 init_cfg=None,
                 pos_dims=None
                 ):
        super(WindowMultiheadPosAttention, self).__init__(init_cfg=init_cfg)

        self.input_dims = input_dims or embed_dims
        self.pos_dim = pos_dims
        self.embed_dims = embed_dims
        self.num_heads = num_heads
        self.v_shortcut = v_shortcut

        self.head_dims = embed_dims // num_heads
        self.scale = qk_scale or self.head_dims**-0.5
        self.softmax_scale = softmax_scale

        self.q = nn.Linear(self.pos_dim, embed_dims, bias=qkv_bias)
        self.k = nn.Linear(self.input_dims, embed_dims, bias=qkv_bias)
        self.v = nn.Linear(self.input_dims, embed_dims, bias=qkv_bias)

        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(embed_dims, embed_dims, bias=proj_bias)
        self.proj_drop = nn.Dropout(proj_drop)

        self.out_drop = build_dropout(dropout_layer)

        self.window_shapes = window_shapes

        if use_layer_scale:
            self.gamma1 = LayerScale(embed_dims)
        else:
            self.gamma1 = nn.Identity()

    def forward(self, x, pos_emb):
        B, N, _ = x.shape
        N_out = pos_emb.shape[1]
        N_windows = self.window_shapes[0] * self.window_shapes[1]

        q = self.q(pos_emb).reshape(B, N_out, self.num_heads, self.head_dims).permute(0, 2, 1, 3)
        k = self.k(x).reshape(B, N, self.num_heads, self.head_dims).permute(0, 2, 1, 3)
        v = self.v(x).reshape(B, N, self.num_heads, self.head_dims).permute(0, 2, 1, 3)

        #######
        #  [BS, n_heads, n_q, token_dims]
        #  [BS, n_heads, n_kv, token_dims]

        #  [BS, n_heads*n_windows, n_q/n_window, token_dims]
        #  [BS, n_heads*n_windows, n_kv/n_window, token_dims]

        #  [BS, n_heads*n_windows, n_q/n_window, n_kv/n_window]
        #  [BS, n_heads*n_windows, n_kv/n_window, token_dims]
        #######
        if N_windows > 1:
            q = self.separate_tokens(q, self.window_shapes)
            k = self.separate_tokens(k, self.window_shapes)
            v = self.separate_tokens(v, self.window_shapes)

        attn = (q @ k.transpose(-2, -1)) * self.scale * self.softmax_scale
        
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).view(B, self.num_heads, N_windows, N_out//N_windows, self.head_dims)
        x = x.view(B, self.num_heads, N_out, self.head_dims).transpose(1, 2).reshape(B, N_out, self.embed_dims)

        x = self.proj(x)
        x = self.out_drop(self.gamma1(self.proj_drop(x)))

        if self.v_shortcut:
            x = v.squeeze(1) + x
        return x
    

    def separate_tokens(self, x, window_shapes=(2,2)):
        BS, num_heads, num_tokens, head_dims = x.shape
        H = W = int(math.sqrt(num_tokens))
        num_win_h, num_win_w = window_shapes

        x = x.view(BS, num_heads, num_win_h, H//num_win_h, num_win_w, W//num_win_w, head_dims).permute(0,1,2,4,3,5,6)
        x = x.contiguous().view(BS, num_heads*num_win_h*num_win_w, -1, head_dims)

        return x
```




```python
        self.q = nn.Linear(self.pos_dim, embed_dims, bias=qkv_bias)
        self.k = nn.Linear(self.input_dims, embed_dims, bias=qkv_bias)
        self.v = nn.Linear(self.input_dims, embed_dims, bias=qkv_bias)
```

可以看到Query (q)来源于位置嵌入 (pos_emb)。通过 nn.Linear 将位置编码嵌入维度 (pos_dim) 映射到目标嵌入维度 (embed_dims)。这表明 q 是基于位置编码生成的，代表查询的空间位置信息。

Key (k) 和 Value (v)来源于输入特征 x，即学生模型的特征。通过 nn.Linear 将输入特征维度 (input_dims) 映射到目标嵌入维度 (embed_dims)。这表明 k 和 v 是基于学生特征生成的，代表特征的内容信息。

这是Cross-Attention，而不是自注意力机制 (Self-Attention)。Cross-Attention用于对齐不同模态特征，在自注意力中，q、k 和 v 都来源于同一个输入特征，用于捕获输入特征内部的全局依赖关系。


# 双视图特征模拟（dual-view feature mimicking） 图1（b）

这个是对图1（b）的示意图展开说明。

作者把ViT的输出特征图的每个通道做离散余弦变换（DCT）后观察到，预训练 ViTs 的特征频率分布极其不平衡，其中直接分量（零频率）响应在所有频率中占主导地位。这表明，在如此不平衡的分布下进行特征蒸馏可能会忽视所有其他替代分量的特征。

![](https://borninfreedom.github.io/images/2025/03/scaleKD/2.png)


所以作者又设计了双视图特征模拟（dual-view feature mimicking）（DFM）方法，其关键见解是补充KD过程中被忽视的替代特征，DFM采用CAP作为特征投影器，并结合了两个特征仿真路径。在第一条路径中，DFM在原始空间中进行特征仿真，以学习教师的全局特征。在第二条路径中，通过去除频率空间中的主导大部分都是0的主导直接分量，方法就是使用采用离散余弦变换（DCT），它将特征从空间域映射到频率域，然后定义一个算子 ϕ来从特征中去除直接分量响应，公式如下所示。

![](https://borninfreedom.github.io/images/2025/03/scaleKD/m1.png)

DFM强调了特征种的数值较大的替代分量，从而避免忽视这些特征。因此，这两条路径是互补的，共同促进特征空间的对齐。

我们来看其代码：
```python
@MODELS.register_module()
class FreqMaskingDistillLossv2(nn.Module):

    def __init__(self,
                 name,
                 use_this,
                 alpha,
                 student_dims,
                 teacher_dims,
                 query_hw,
                 pos_hw,
                 pos_dims,
                 window_shapes=(1,1),
                 self_query=True,
                 softmax_scale=1.,
                 dis_freq='high',
                 num_heads=8
                 ):
        super(FreqMaskingDistillLossv2, self).__init__()
        self.alpha = alpha
        self.dis_freq = dis_freq
        self.self_query = self_query

        self.projector_0 = AttentionProjector(student_dims, teacher_dims, query_hw, pos_dims, window_shapes=window_shapes, self_query=self_query, softmax_scale=softmax_scale[0], num_heads=num_heads)
        self.projector_1 = AttentionProjector(student_dims, teacher_dims, query_hw, pos_dims, window_shapes=window_shapes, self_query=self_query, softmax_scale=softmax_scale[1], num_heads=num_heads)

        

    def forward(self,
                preds_S,
                preds_T,
                query_s=None,
                query_f=None,
                ):
        """Forward function.
        Args:
            preds_S(Tensor): Bs*C*H*W, student's feature map
            preds_T(Tensor): Bs*C*H*W, teacher's feature map
        """

        preds_S_spat =  self.project_feat_spat(preds_S, query=query_s)
        preds_S_freq =  self.project_feat_freq(preds_S, query=query_f)

        spat_loss = self.get_spat_loss(preds_S_spat, preds_T)
        freq_loss = self.get_freq_loss(preds_S_freq, preds_T)

        return spat_loss, freq_loss

    def project_feat_spat(self, preds_S, query=None):
        preds_S = self.projector_0(preds_S, query=query)

        return preds_S

    def project_feat_freq(self, preds_S, query=None):
        preds_S = self.projector_1(preds_S, query=query)

        return preds_S


    def get_spat_loss(self, preds_S, preds_T):
        loss_mse = nn.MSELoss(reduction='sum')

        N = preds_S.shape[0]
        N, C, H, W = preds_T.shape
        device = preds_S.device

        dct = DCT(resolution=H, device=device)


        preds_S = preds_S.permute(0,2,1).contiguous().view(*preds_T.shape)

        preds_S = F.normalize(preds_S, dim=1)
        preds_T = F.normalize(preds_T, dim=1)

        dis_loss_arch_st = loss_mse(preds_S, preds_T)/N 
        dis_loss_arch_st = dis_loss_arch_st * self.alpha[0]

        return dis_loss_arch_st


    def get_freq_loss(self, preds_S, preds_T):
        loss_mse = nn.MSELoss(reduction='sum')
        N, C, H, W = preds_T.shape
        device = preds_S.device

        dct = DCT(resolution=H, device=device)

        preds_S = preds_S.permute(0,2,1).contiguous().view(*preds_T.shape)

        preds_S_freq = dct.forward(preds_S)
        preds_T_freq = dct.forward(preds_T)

        preds_S_freq[:,:,0,0]=0
        preds_T_freq[:,:,0,0]=0

        preds_S = dct.inverse(preds_S_freq)
        preds_T = dct.inverse(preds_T_freq)

        preds_S = F.normalize(preds_S, dim=1, p=2)
        preds_T = F.normalize(preds_T, dim=1, p=2)

        dis_loss = loss_mse(preds_S, preds_T)/N 

        dis_loss = dis_loss * self.alpha[1]

        return dis_loss
```

在这个loss计算过程中，首先做了入图1（b）所示的两路CAP特征映射。
```python
        self.projector_0 = AttentionProjector(student_dims, teacher_dims, query_hw, pos_dims, window_shapes=window_shapes, self_query=self_query, softmax_scale=softmax_scale[0], num_heads=num_heads)
        self.projector_1 = AttentionProjector(student_dims, teacher_dims, query_hw, pos_dims, window_shapes=window_shapes, self_query=self_query, softmax_scale=softmax_scale[1], num_heads=num_heads)
```

然后分别计算上下两路的loss：上路的loss是直接基于特征的MSELoss，对应代码中的函数是get_spat_loss，下路loss是把特征做了DCT转到频率域，在频率域做的MSELoss计算，对应的代码函数是get_freq_loss。

```python
    def get_spat_loss(self, preds_S, preds_T):
        loss_mse = nn.MSELoss(reduction='sum')

        N = preds_S.shape[0]
        N, C, H, W = preds_T.shape
        device = preds_S.device

        dct = DCT(resolution=H, device=device)


        preds_S = preds_S.permute(0,2,1).contiguous().view(*preds_T.shape)

        preds_S = F.normalize(preds_S, dim=1)
        preds_T = F.normalize(preds_T, dim=1)

        dis_loss_arch_st = loss_mse(preds_S, preds_T)/N 
        dis_loss_arch_st = dis_loss_arch_st * self.alpha[0]

        return dis_loss_arch_st
```

```python
    def get_freq_loss(self, preds_S, preds_T):
        loss_mse = nn.MSELoss(reduction='sum')
        N, C, H, W = preds_T.shape
        device = preds_S.device

        dct = DCT(resolution=H, device=device)

        preds_S = preds_S.permute(0,2,1).contiguous().view(*preds_T.shape)

        preds_S_freq = dct.forward(preds_S)
        preds_T_freq = dct.forward(preds_T)

        preds_S_freq[:,:,0,0]=0
        preds_T_freq[:,:,0,0]=0

        preds_S = dct.inverse(preds_S_freq)
        preds_T = dct.inverse(preds_T_freq)

        preds_S = F.normalize(preds_S, dim=1, p=2)
        preds_T = F.normalize(preds_T, dim=1, p=2)

        dis_loss = loss_mse(preds_S, preds_T)/N 

        dis_loss = dis_loss * self.alpha[1]

        return dis_loss
```

我们看一下DCT变换的代码：

```python
class DCT():
    def __init__(self, resolution, device, norm=None, bias=False):
        self.resolution = resolution
        self.norm = norm
        self.device = device

        #创建一个单位矩阵 I，大小为 (resolution, resolution)，用于初始化 DCT 和 IDCT 的权重矩阵。
        I = torch.eye(self.resolution, device=self.device)
        self.forward_transform = nn.Linear(resolution, resolution, bias=bias).to(self.device)
        #计算单位矩阵的 DCT，生成 DCT 的权重矩阵。.t(): 转置矩阵以适配线性层的权重格式。
        self.forward_transform.weight.data = self._dct(I, norm=self.norm).data.t()
        self.forward_transform.weight.requires_grad = False

        self.inverse_transform = nn.Linear(resolution, resolution, bias=bias).to(self.device)
        self.inverse_transform.weight.data = self._idct(I, norm=self.norm).data.t()
        self.inverse_transform.weight.requires_grad = False

    def _dct(self, x, norm=None):
        """
        Discrete Cosine Transform, Type II (a.k.a. the DCT)
        For the meaning of the parameter `norm`, see:
        https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html
        :param x: the input signal
        :param norm: the normalization, None or 'ortho'
        :return: the DCT-II of the signal over the last dimension
        """
        x_shape = x.shape
        N = x_shape[-1]
        x = x.contiguous().view(-1, N)
        
        #输入信号进行重新排列。x[:, ::2]: 提取偶数索引的元素。x[:, 1::2].flip([1]): 提取奇数索引的元素并反转顺序。torch.cat: 将偶数和奇数索引的元素拼接在一起。
        v = torch.cat([x[:, ::2], x[:, 1::2].flip([1])], dim=1)
        
        #计算重新排列信号的快速傅里叶变换（FFT）。torch.fft.fft: 对信号 v 计算 FFT。torch.view_as_real: 将复数结果转换为实部和虚部的形式。
        Vc = torch.view_as_real(torch.fft.fft(v, dim=1))

        #计算 DCT 的余弦和正弦权重。k: 生成频率因子。W_r: 余弦权重。W_i: 正弦权重。
        k = - torch.arange(N, dtype=x.dtype, device=x.device)[None, :] * np.pi / (2 * N)
        W_r = torch.cos(k)
        W_i = torch.sin(k)

        #根据 DCT-II 的公式，计算变换后的信号。Vc[:, :, 0]: FFT 的实部。Vc[:, :, 1]: FFT 的虚部
        V = Vc[:, :, 0] * W_r - Vc[:, :, 1] * W_i

        #如果启用了正交归一化（'ortho'），对结果进行归一化处理。
        if norm == 'ortho':
            V[:, 0] /= np.sqrt(N) * 2
            V[:, 1:] /= np.sqrt(N / 2) * 2

        V = 2 * V.view(*x_shape)
        return V

    def _idct(self, X, norm=None):
        """
        The inverse to DCT-II, which is a scaled Discrete Cosine Transform, Type III
        Our definition of idct is that idct(dct(x)) == x
        For the meaning of the parameter `norm`, see:
        https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html
        :param X: the input signal
        :param norm: the normalization, None or 'ortho'
        :return: the inverse DCT-II of the signal over the last dimension
        """

        x_shape = X.shape
        N = x_shape[-1]

        X_v = X.contiguous().view(-1, x_shape[-1]) / 2

        if norm == 'ortho':
            X_v[:, 0] *= np.sqrt(N) * 2
            X_v[:, 1:] *= np.sqrt(N / 2) * 2

        k = torch.arange(x_shape[-1], dtype=X.dtype, device=X.device)[None, :] * np.pi / (2 * N)
        W_r = torch.cos(k)
        W_i = torch.sin(k)

        V_t_r = X_v
        V_t_i = torch.cat([X_v[:, :1] * 0, -X_v.flip([1])[:, :-1]], dim=1)

        V_r = V_t_r * W_r - V_t_i * W_i
        V_i = V_t_r * W_i + V_t_i * W_r

        V = torch.cat([V_r.unsqueeze(2), V_i.unsqueeze(2)], dim=2)

        v = torch.fft.irfft(torch.view_as_complex(V), n=V.shape[1], dim=1)
        x = v.new_zeros(v.shape)
        x[:, ::2] += v[:, :N - (N // 2)]
        x[:, 1::2] += v.flip([1])[:, :N // 2]
        return x.view(*x_shape)

    def forward(self, x):
        X1 = self.forward_transform(x)
        X2 = self.forward_transform(X1.transpose(-1, -2))
        return X2.transpose(-1, -2)

    def inverse(self, x):
        X1 = self.inverse_transform(x)
        X2 = self.inverse_transform(X1.transpose(-1, -2))
        return X2.transpose(-1, -2)
```


# 教师参数感知（teacher parameter perception）TPP，图1（c）

TPP的主要区别就在于分阶段对特征进行处理。TPP 通过一个CAP 将学生的早期阶段与教师
的后期阶段连接起来（有点类似自蒸馏的方法，将模型深层的信息转移到浅层），从而建立了一个代理特征处理路径。在我们的实现中，代理路径由
学生的前n−1 阶段和教师的最后阶段。

```python
@MODELS.register_module()
class ClassificationDistiller(BaseModel, metaclass=ABCMeta):
    """Base distiller for dis_classifiers.

    It typically consists of teacher_model and student_model.
    """
    def __init__(self,
                 teacher_cfg,
                 student_cfg,
                 is_vit = False,
                 use_logit = False,
                 sd = False,
                 distill_cfg = None,
                 teacher_pretrained = None,
                 train_cfg: Optional[dict] = None,
                 data_preprocessor: Optional[dict] = None,
                 init_cfg: Optional[dict] = None):

        if data_preprocessor is None:
            data_preprocessor = {}
        # The build process is in MMEngine, so we need to add scope here.
        data_preprocessor.setdefault('type', 'mmcls.ClsDataPreprocessor')

        if train_cfg is not None and 'augments' in train_cfg:
            # Set batch augmentations by `train_cfg`
            data_preprocessor['batch_augments'] = train_cfg

        super(ClassificationDistiller, self).__init__(
            init_cfg=init_cfg,
            data_preprocessor=data_preprocessor)

        self.teacher = build_classifier((Config.fromfile(teacher_cfg)).model)
        self.teacher_pretrained = teacher_pretrained
        self.teacher.eval()
        for param in self.teacher.parameters():
            param.requires_grad = False

        self.student = build_classifier((Config.fromfile(student_cfg)).model)

        self.distill_cfg = distill_cfg   
        self.distill_losses = nn.ModuleDict()
        if self.distill_cfg is not None:  
            for item_loc in distill_cfg:
                for item_loss in item_loc.methods:
                    loss_name = item_loss.name
                    use_this = item_loss.use_this
                    if use_this:
                        self.distill_losses[loss_name] = MODELS.build(item_loss)

        self.is_vit = is_vit
        self.sd = sd
        self.use_logit = use_logit

        if 'loss_tcd' in self.distill_losses.keys():
            self.distill_losses['loss_tcd'].set_head(self.teacher.head)

    def init_weights(self):
        if self.teacher_pretrained is not None:
            load_checkpoint(self.teacher, self.teacher_pretrained, map_location='cpu')
        self.student.init_weights()

    def forward(self,
                inputs: torch.Tensor,
                data_samples: Optional[List[BaseDataElement]] = None,
                mode: str = 'tensor'):
        if mode == 'tensor':
            feats = self.student.extract_feat(inputs)
            return self.student.head(feats) if self.student.with_head else feats
        elif mode == 'loss':
            return self.loss(inputs, data_samples)
        elif mode == 'predict':
            return self.student.predict(inputs, data_samples)
        else:
            raise RuntimeError(f'Invalid mode "{mode}".')

    def loss(self, inputs: torch.Tensor,
             data_samples: List[ClsDataSample]) -> dict:
        """Calculate losses from a batch of inputs and data samples.

        Args:
            inputs (torch.Tensor): The input tensor with shape
                (N, C, ...) in general.
            data_samples (List[ClsDataSample]): The annotation data of
                every samples.

        Returns:
            dict[str, Tensor]: a dictionary of loss components
        """
        if 'score' in data_samples[0].gt_label:
            # Batch augmentation may convert labels to one-hot format scores.
            gt_label = torch.stack([i.gt_label.score for i in data_samples])
        else:
            gt_label = torch.cat([i.gt_label.label for i in data_samples])

        fea_s = self.student.extract_feat(inputs, stage='backbone')    

        x = fea_s
        if self.student.with_neck:
            x = self.student.neck(x)
        if self.student.with_head and hasattr(self.student.head, 'pre_logits'):
            x = self.student.head.pre_logits(x)

        if self.is_vit:
            logit_s = self.student.head.layers.head(x)
        else:
            logit_s = self.student.head.fc(x)
        loss = self.student.head._get_loss(logit_s, data_samples)

        s_loss = dict()
        for key in loss.keys():
            s_loss['ori_'+key] = loss[key]

        with torch.no_grad():
            fea_t = self.teacher.extract_feat(inputs, stage='backbone')
            if self.use_logit:
                logit_t = self.teacher.head.layers.head(self.teacher.head.pre_logits(fea_t))
        

        all_keys = self.distill_losses.keys()


        if 'loss_tfd' in all_keys:
            loss_name = 'loss_tfd'
            preds_S = fea_s[-1]
            preds_T = fea_t[-1]
            # print(preds_S.shape)
            # print(preds_T.shape)
            # assert 0 == 1
            # pos_emb_1 = fea_t[0]   # feature before the first stage
            pos_emb = fea_t[-2] # feature before the last stage
            # s_loss[loss_name] = self.distill_losses[loss_name](preds_S, preds_T, pos_emb)
            s_loss[loss_name] = self.distill_losses[loss_name](preds_S, preds_T, pos_emb)

        if 'loss_mfd' in all_keys:
            loss_name = 'loss_mfd'
            fea_t = fea_t[1:]
            s_loss['loss_mfd_s0'], s_loss['loss_mfd_s1'], s_loss['loss_mfd_s2'], s_loss['loss_mfd_s3'] \
                = self.distill_losses[loss_name](fea_s, fea_t)

        if 'loss_fd' in all_keys:
            loss_name = 'loss_fd'
            preds_S = fea_s[-1]
            preds_T = fea_t[-1]
            s_loss[loss_name] = self.distill_losses[loss_name](preds_S, preds_T)



        if ('loss_kd' in all_keys) and self.use_logit:
            loss_name = 'loss_kd'
            ori_alpha, s_loss[loss_name] = self.distill_losses[loss_name](logit_s, logit_t)
            s_loss['ori_loss'] = ori_alpha * s_loss['ori_loss']


        return s_loss
```

从蒸馏的class实现来看，先pass

# ScaleKD总体框架


