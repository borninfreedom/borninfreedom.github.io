---
title: "cvpr2025知识蒸馏论文Attention Distillation: A Unified Approach to Visual Characteristics Transfer解读"
date: 2025-03-10
permalink: /posts/2025/03/blog-post-2/
tags:
-  蒸馏
---

论文链接: [Attention Distillation: A Unified Approach to Visual Characteristics Transfer](https://arxiv.org/pdf/2502.20235)


总结来说，本文提出了一种名为**注意力蒸馏（Attention Distillation, AD）**的统一方法，用于解决视觉特征迁移任务中的关键挑战，包括风格迁移、外观迁移、特定风格的文本到图像生成以及纹理合成。以下是文章的核心内容总结：

---

### **核心贡献**
1. **注意力蒸馏损失（AD损失）**  
   - 提出一种基于预训练扩散模型自注意力特征的新型损失函数。通过计算**理想风格化结果**（参考图像键值对与目标查询的注意力输出）与**当前风格化结果**（目标图像自注意力输出）之间的L1差异，反向优化潜在编码，逐步缩小目标查询与参考键值对之间的差距，从而提升特征聚合的准确性。
   - 该损失可结合内容损失（通过对齐目标与参考图像的查询特征），实现内容保留的视觉特征迁移。

2. **注意力蒸馏引导采样**  
   - 将AD损失整合到扩散模型的采样过程中，提出一种改进的分类器引导方法，显著加快生成速度，并支持与ControlNet等条件生成技术的结合，扩展应用场景。

3. **改进的变分自编码器（VAE）解码**  
   - 通过微调VAE解码器，减少高频细节损失，提升合成图像的视觉质量。

---

### **方法亮点**
- **解决即插即用方法的局限性**：针对领域差距、误差累积和架构限制，AD损失通过优化潜在编码，增强目标查询与参考键值对的语义对齐。
- **多任务适用性**：支持风格迁移、外观迁移、可控纹理合成、超高分辨率纹理扩展等任务，如图1、图8、图10所示。
- **高效优化与生成**：结合反向传播优化（200次迭代约30秒）和引导采样（50步采样），平衡生成速度与质量。

---

### **实验结果**
1. **定性对比**  
   - 在风格迁移中，AD损失有效捕捉连贯的抽象风格（如艺术笔触），同时保留内容结构（图6）；在外观迁移中，避免颜色过饱和问题（对比Cross-Image Attention）。
   - 特定风格的文本到图像生成（图7）中，AD方法在风格一致性与文本语义对齐上优于InstantStyle等方法。

2. **用户偏好研究**  
   - 在风格迁移、外观迁移和文本到图像生成任务中，用户对AD方法的偏好得分显著高于对比方法（图13），尤其在风格一致性上表现突出。

3. **消融实验**  
   - 内容损失权重（λ）的调整可实现风格抽象程度与内容保留的灵活平衡（图11）。
   - 使用Adam优化器管理梯度更新强度，显著提升生成质量（图12）。

---

### **局限性与未来方向**
1. **颜色过饱和与分辨率限制**：生成超高分辨率图像时，可能出现颜色失真，需进一步约束数据分布。
2. **语义匹配依赖模型能力**：当内容差异较大时，模型的语义理解局限可能导致错误匹配。
3. **计算效率**：引导采样需多次迭代优化，未来可探索轻量化策略。

---

### **总结**
本文通过注意力蒸馏损失与改进的采样策略，实现了视觉特征迁移的统一框架，在多个任务中超越现有方法。其技术核心在于利用扩散模型的自注意力特征，通过优化潜在空间实现全局与局部的一致性，为图像合成提供了新的思路。代码已开源，适用于学术研究与实际应用。
![](https://borninfreedom.github.io/images/2025/03/ad/1.png)
图1. 给定一张参考图像，我们的方法能够在合成过程中忠实地再现其视觉特征，为广泛的基于示例的图像合成应用提供了一个统一的框架，例如艺术风格迁移、外观迁移、特定风格的文本到图像生成，以及各种纹理合成任务。 



# 摘要
生成式扩散模型的最新进展显示出其对图像风格和语义有着显著的内在理解。在本文中，我们利用预训练扩散网络中的自注意力特征，将参考图像的视觉特征迁移到生成的图像上。与之前将这些特征用作即插即用属性的工作不同，我们提出了一种新颖的注意力蒸馏损失，该损失通过计算理想的风格化结果和当前风格化结果之间的差异得到。基于此，我们在隐空间中通过反向传播来优化合成图像。接下来，我们提出了一种改进的分类器引导方法，将注意力蒸馏损失整合到去噪采样过程中，进一步加快了合成速度，并适用于广泛的图像生成应用。大量实验表明，我们的方法在将示例的风格、外观和纹理迁移到合成的新图像上表现出色。代码可在https://github.com/xugao97/AttentionDistillation获取。

# 1. 引言
合成具有示例图像的视觉元素（如风格或纹理）的新图像，是计算机图形学和计算机视觉领域中一个长期存在且具有挑战性的问题。关键挑战在于如何恰当地表示图像的纹理或风格特征。传统方法[6, 11, 15, 34, 37, 38, 42, 67]通常将纹理定义为重复的局部模式，并通过从源图像中复制局部块来合成新的纹理。当涉及到风格时，作为一种比纹理更广泛且更抽象的视觉特征，则需要新的表示方法。

得益于深度学习的变革，视觉特征的神经表示应运而生。一类方法通过匹配参考图像和输出图像之间深度特征的全局分布来进行特定纹理或风格的合成。例如，开创性的工作“格拉姆损失”[21, 22]将特征图的统计量视为纹理/风格的表示。其他一些工作通过最小化瓦瑟斯坦距离[24]或对抗判别损失[51, 52, 68]来优化深度特征。然而，匹配全局分布缺乏局部感知，通常会导致明显的细节瑕疵。另一类方法重新采用局部块策略，通过最近邻匹配[40, 43, 69]来优化输出的深度特征。由于局部块没有考虑全局结构，这些方法总是需要额外的结构引导。

除了上述进展之外，大规模扩散模型的最新突破为视觉特征带来了新的表示方法，在合成过程中实现了全局和局部一致性之间的良好平衡[2, 7, 10, 25, 29, 70]。这些技术的一个关键共识是，预训练扩散模型中自注意力模块的键（K）和值（V）能够表征示例图像的外观。在新图像生成过程中，通过自注意力机制根据目标查询（Q）重新聚合这些特征，可以再现参考图像的视觉特征。尽管由于扩散模型的强大性能取得了令人印象深刻的结果，但这些方法仍然常常存在风格化不足等问题。我们认为其局限性有以下三个原因：

1）领域差距。当两张图像差异很大时，目标查询Q（合成图像的查询）与源键值对KV（示例图像的键和值）之间的相似度会变低且不可靠，从而导致错误的聚合结果。像自适应实例归一化（AdaIN）[27]和注意力缩放等技术可以部分缓解这个问题[2, 25]。

2）误差累积。虽然扩散模型中的迭代采样过程可以改善目标查询Q与源键值对KV之间的较大差异，但误差也可能会累积。如[56]所示，扩散模型不同层的特征关注不同的信息，如语义和几何信息。不正确的匹配会沿着马尔可夫链将误差传播到后续层，并降低最终图像的质量。

3）架构限制。自注意力机制是在去噪网络的残差分支中实现的。注入来自源的自注意力特征可能对目标隐编码的影响有限，这可能会降低它们在合成中的效果。

在这项工作中，我们仍然遵循去噪网络中的自注意力特征能够捕捉图像视觉外观的假设。为了解决上述局限性，我们引入了一种新颖的注意力蒸馏（AD）损失，基于此我们通过反向传播直接更新合成图像。具体来说，我们将计算目标查询Q和源键值对KV之间的注意力得到的输出视为理想的风格化结果，而原始的注意力输出表示当前的风格化结果。我们将注意力蒸馏损失定义为这两个输出之间的L1距离，并通过在隐空间中反向传播来优化合成图像。我们同时计算不同层之间的差异，以避免误差累积。这样的优化过程逐渐减小了目标查询Q与源键值对KV之间的差异，提高了相似度计算的准确性，从而改善了最终的风格化效果。

尽管我们的方法与之前将自注意力特征用作即插即用属性的工作有很大不同，但新的注意力蒸馏损失也可以集成到扩散模型的采样过程中，作为一种改进的分类器引导方法。与普通的无分类器引导相结合，它进一步实现了基于文本的可控生成，并且还与其他条件生成技术（如ControlNet）兼容，从而实现了广泛的图像合成应用；例如，见图1。大量实验以及与最先进方法的比较证明了我们方法的优势。

总之，我们的主要贡献如下：
• 我们分析了之前即插即用注意力特征方法的局限性，并提出了一种新颖的注意力蒸馏损失，用于再现参考图像的视觉特征，取得了显著更优的结果。
• 我们开发了注意力蒸馏引导采样方法，这是一种改进的分类器引导方法，将注意力蒸馏损失集成到去噪过程中，显著加快了合成速度，并实现了广泛的视觉特征迁移和合成应用。 

# 2. 相关工作
我们的主要贡献在于提出了一种新颖的损失函数，它能够有效地将一张图像的视觉特征迁移到另一张图像上。这种损失函数在各种图像合成任务中有着广泛的应用，包括纹理合成、风格迁移、外观迁移，以及基于文本到图像模型的定制化图像生成。我们回顾了这些领域的相关工作。

纹理合成专注于生成与原始纹理相似且没有重复和瑕疵的图像。传统方法依赖于参数化纹理模型[23, 32, 53]或对像素/图像块进行采样[14, 15, 37, 38, 62]来创建新图像。这些方法在处理简单纹理时表现出色，但在处理复杂或高分辨率纹理时往往会遇到困难。基于深度学习的方法利用卷积神经网络（CNN）提取能够捕捉不同尺度纹理的多层次特征。加蒂斯（Gatys）等人[21]引入了格拉姆矩阵（Gram matrix），这是一种特征图的二阶统计量，用于表示平稳纹理。直方图损失[47]和切片瓦瑟斯坦损失[16, 24]对纹理分布提供了更好的建模，从而得到更逼真的结果，但无法捕捉大规模结构，尤其是对于非平稳纹理。生成对抗网络（GANs）也广泛应用于纹理合成[51, 52, 68]；例如，周等人[68]使用一种自监督方法，通过学习单个纹理的内部图像块分布来扩展该纹理，在细节和结构扩展方面取得了令人印象深刻的成果。最近，“自校正”（Self-Rectification）[70]利用预训练的扩散网络和自注意力机制，逐步细化一个经过粗略编辑的输入，解决了合成非平稳纹理这一复杂难题。

神经风格迁移是将源图像的艺术风格应用到新的内容图像上。加蒂斯等人[22]开创了这一领域，他们使用格拉姆损失作为风格表示，并提出了一种神经风格迁移算法，在优化过程中结合了内容损失和风格损失。虽然格拉姆损失及其变体[27, 30, 39]是应用最广泛的风格损失，但它们主要评估全局分布差异，无法考虑局部语义对应关系。像“卷积神经网络马尔可夫随机场”（CNNMRF）[40]和“上下文损失”（Contextual Loss）[43]这样的方法通过在高维特征空间中计算语义相似度来解决这个问题，从而实现语义连贯的风格迁移。后来的研究工作引入了带有自注意力机制的变换器（transformers），以捕捉风格和内容之间更强的关系。邓等人[12]提出了一种完全基于变换器的架构“StyTR2”，在当时取得了领先的成果。扩散模型的最新进展使得可解释和可控的内容-风格分离成为可能。“基于逆映射的风格迁移”（InST）[66]提出了一种基于逆映射的方法，从单幅绘画中学习艺术风格。“风格扩散”（StyleDiffusion）[60]引入了一种基于CLIP的风格解耦损失。“风格标识”（StyleID）[10]是一种无需训练的方法，通过在交叉注意力机制中用风格图像的键和值替换内容图像的键和值，来操纵预训练扩散模型的自注意力特征。

外观迁移是一种特殊的语义风格迁移，旨在迁移语义对应区域的外观。早期的研究工作[28, 45, 71]使用成对或不成对的数据集来训练生成对抗网络，以实现特定领域的外观迁移。图马扬（Tumanyan）等人[57]使用预训练的“基于可变形实例的视觉变换器”（DINO-ViT）[8]提取结构和外观特征，并为每对图像训练一个生成器。最近，基于预训练扩散模型的跨图像注意力[2]通过隐式地建立图像间的语义对应关系，实现了零样本外观迁移。

基于文本到图像（T2I）扩散模型的定制化/个性化图像生成最近受到了特别关注，其目标是从一张或多张参考图像中学习风格，以生成新的内容。微调方法[1, 18–20, 31, 36, 50, 54]使模型能够从少量图像中学习新的风格概念。然而，这些方法容易出现过拟合问题，可能导致图像质量下降或内容泄露。为了缓解这个问题，“基于低秩适应的风格分离”（B-LoRA）[19]利用“低秩适应”（LoRA）[26]来隐式地分离单张图像的风格和内容成分。“成对定制化”（Pair Customization）[31]从单对图像中学习风格差异，然后将学到的风格应用到生成过程中。基于编码器的方法[9, 33, 59, 61, 63, 64]利用视觉编码器来捕捉图像信息，并通过对大规模数据集的训练，在图像提示和模型之间建立映射关系。虽然这些技术目前被认为是最先进的方法，但它们受到视觉编码器能力的限制，通常只能提取抽象的风格信息，在处理参考图像的细粒度纹理时会遇到困难。一些研究工作提出了无需训练的风格定制化的即插即用解决方案。例如，“风格对齐”（StyleAligned）[25]和“视觉风格提示”（Visual Style Prompt）[29]通过在后期的自注意力层中保留原始特征的查询，同时与参考特征的键和值进行共享或交换，来保持风格的一致性。“反向扩散调制”（RB-Modulation）[49]通过在终端成本中纳入所需的属性（如风格或内容）来调制反向扩散动力学的漂移场。 

![](https://borninfreedom.github.io/images/2025/03/ad/2.png)
图2. 注意力蒸馏概述。基于扩散模型中的自注意力机制，我们计算理想风格化结果与当前风格化结果之间的差异，从而构建出一种新颖的注意力蒸馏（AD）损失（a）。这种新的损失类似于一种风格损失。当它与内容损失（同样源自自注意力机制）相结合时，我们能够实现高质量的保留内容的图像合成，比如风格迁移或外观迁移（b）。我们的注意力蒸馏损失可以作为一种改进的分类器引导方法融入到常规的扩散采样过程中（c），这使得基于示例的图像生成应用范围得以大幅拓展。 

![](https://borninfreedom.github.io/images/2025/03/ad/3.png)
图3. 键值对注入（KV-injection）与注意力蒸馏之间的差异。我们从相同的潜在噪声开始进行采样和优化，均运行100步，且使用空提示。信息流（红色箭头）仅在恒等连接部分有所不同。然而，我们的注意力蒸馏优化结果（b）明显优于使用键值对注入进行的采样结果（a）。 


# 3. 方法
## 3.1. 预备知识
以“稳定扩散”（Stable Diffusion）[46, 48] 为例的潜在扩散模型（LDM），由于其对复杂数据分布的强大建模能力，在图像生成方面取得了领先的性能。在潜在扩散模型中，首先使用预训练的变分自动编码器VAE将图像\(x\)压缩到一个学习得到的潜在空间中。随后训练一个基于U型网络（UNet）的去噪网络\(\epsilon_{\theta}(\cdot)\)，通过最小化预测噪声与实际添加噪声\(\epsilon\)之间的均方误差，来预测扩散过程中的噪声：
![](https://borninfreedom.github.io/images/2025/03/ad/m1.png)

其中，\(y\)表示条件，\(t\)表示时间步长。去噪U型网络通常由一组卷积块以及自注意力/交叉注意力模块组成，所有这些模块都集成在残差架构的预测分支内。

键值对注入（KV-injection）广泛应用于图像编辑[2, 7, 58]、风格迁移[10, 25, 29]以及纹理合成[70]中。它基于自注意力机制，并将扩散模型中的自注意力特征用作即插即用的属性。自注意力机制的公式为：
![](https://borninfreedom.github.io/images/2025/03/ad/m2.png)

注意力机制的核心在于基于查询（\(Q\)）与键（\(K\)）之间的相似度计算一个权重矩阵，该权重矩阵用于对值（\(V\)）进行加权聚合。键值对注入通过在不同的合成分支之间复制或共享键值对（\(KV\)）特征来扩展这一机制。其关键假设是键值对特征代表了一幅图像的视觉外观。在采样过程中，用示例图像在相应时间步长的键值对特征替换合成分支中的键值对特征，能够实现从源图像到合成目标图像的外观迁移。

## 3.2. 注意力蒸馏损失
尽管键值对注入取得了显著的成果，但由于残差机制的存在，它在保留参考图像的风格或纹理细节方面存在不足；例如，见图3（a）。键值对注入仅在残差部分上进行操作，这意味着信息流（红色箭头）随后会受到恒等连接的影响，从而导致信息的不完全迁移。结果是，采样输出无法完全再现期望的视觉细节。

在这项工作中，我们提出了一种新颖的损失函数，通过在自注意力机制内重新聚合特征来提取视觉元素；因此，我们将其称为注意力蒸馏（AD）损失。我们利用预训练的文本到图像（T2I）扩散模型“稳定扩散”[48] 的U型网络，从自注意力模块中提取图像特征。如图2（a）所示，我们首先根据目标分支的查询\(Q\)，对参考分支的键值对（\(KV\)）特征（\(K_s\)和\(V_s\)）的视觉信息进行重新聚合，这与键值对注入的操作相同。我们将这个注意力输出视为理想的风格化结果。然后，我们计算目标分支的注意力输出，并计算其与理想注意力输出之间的L1损失，该损失定义了注意力蒸馏损失：
![](https://borninfreedom.github.io/images/2025/03/ad/m3.png)

我们可以使用所提出的注意力蒸馏损失，通过梯度下降来优化一个随机的潜在噪声，从而在输出中实现生动的纹理或风格再现；例如，见图3（b）。这可以归因于优化过程中的反向传播，它使得信息不仅能够在（残差）自注意力模块之间流动，还能通过恒等连接流动。随着不断的优化，查询\(Q\)与键\(K_s\)之间的差距逐渐缩小，使得注意力计算越来越准确，最终，特征被正确聚合以产生期望的视觉细节。

根据最近的实验分析[7, 29, 58]，我们通过实验选择了U型网络的最后6个自注意力层来计算注意力蒸馏损失。此外，在优化过程中，我们通过将输入到U型网络的时间步长\(t\)从\(T\)线性减小到\(0\)，来模拟扩散模型的采样过程。我们从不同的随机潜在噪声开始，并对它们进行100步的优化。请注意，在整个优化过程中，U型网络预测的噪声将被完全丢弃，并且我们会不断更新同一个潜在噪声。
![](https://borninfreedom.github.io/images/2025/03/ad/4.png)
图4. 多次运行中对注意力蒸馏损失的优化。在对同一参考图像的多次运行中，纹理和风格上的一致性，以及结构上的变化，展示了我们的注意力蒸馏（AD）损失在风格对齐和空间适应性方面的能力。 

为了更好地理解我们的注意力蒸馏损失，我们展示了多次运行的优化结果，如图4所示。这些结果表明：i）注意力蒸馏损失能够有效地提取高质量的风格和纹理视觉特征；ii）注意力蒸馏损失能够自适应于不同的空间结构，在多次运行中展现出多样性。 


## 3.3. 内容保留优化
借助注意力蒸馏损失提取的纹理和风格，我们可以使用内容损失将合成的内容进一步与另一张参考图像对齐。这样的优化使得合成的图像能够在保留目标内容的同时，变换一张图像的视觉元素，从而实现风格迁移、外观迁移等任务。
如图2（b）所示，我们以类似于注意力蒸馏损失的方式定义内容损失，它同样基于自注意力机制，充分利用了扩散模型对图像的深度理解优势。具体来说，通过计算目标查询\(Q\)与参考查询\(Q_c\)之间的L1损失来定义内容损失：

![](https://borninfreedom.github.io/images/2025/03/ad/m4.png)

在实现过程中，我们同样选择最后6个自注意力层来计算内容损失，这与计算注意力蒸馏损失的层一致。
内容保留优化的目标是：
![](https://borninfreedom.github.io/images/2025/03/ad/m5.png)

优化完成后，使用预训练的变分自动编码器（VAE）将优化后的潜在编码解码到图像空间。我们在补充材料的算法1中总结了内容保留优化的过程。

## 3.4. 注意力蒸馏引导采样
上述的合成方法采用的是反向传播优化。在本节中，我们将介绍如何以一种改进的分类器引导方式，将注意力蒸馏损失融入到扩散模型的采样过程中。

![](https://borninfreedom.github.io/images/2025/03/ad/tmp1.png)

在注意力蒸馏损失的引导下，我们可以在带有时间步长条件的潜在编码上计算损失，而不是像最近的一些研究工作[4, 41, 49]那样，将潜在编码转换到图像空间并计算图像级别的损失。使用Adam优化器也使我们能够建立一个通用的学习率，从而减轻了设置引导强度的难度。注意力蒸馏引导采样的详细过程见补充材料中的算法2。需要注意的是，公式（4）中提出的内容损失也可以添加到采样过程中与注意力蒸馏损失一起使用，以进一步保留内容参考图像的结构。



## 3.5. 改进的变分自动编码器解码

![](https://borninfreedom.github.io/images/2025/03/ad/5.png)
图5. 改进的变分自动编码器（VAE）解码。预训练的变分自动编码器在高频细节方面存在信息损失。通过参考图像对变分自动编码器进行若干步的微调（标记为VAE*），可以提高重建质量，并改善对新图像合成的解码效果。 

![](https://borninfreedom.github.io/images/2025/03/ad/tmp2.png)

# 4. 实验
## 4.1. 应用与比较
接下来，我们将注意力蒸馏损失应用于各种视觉特征迁移任务，并将结果与每个应用领域中最先进的方法进行比较。详细的参数配置、运行时间及更多结果见补充材料。
![](https://borninfreedom.github.io/images/2025/03/ad/6.png)
图6. 风格与外观迁移的比较。我们的比较主要聚焦于近期基于扩散模型的风格与外观迁移方法，包括“基于条件空间生成优化的风格迁移”（CSGO）[63]、“风格快照”（StyleShot）[33]、“风格标识”（StyleID）[10]以及“跨图像注意力”（Cross-Image Attention）[2]。此外，我们还纳入了如“神经风格迁移”（NST）[22]这样的传统方法，以及像“基于变换器的风格迁移2”（StyTR2）[12]和“拼接视觉变换器”（SpliceViT）[57]这类基于变换器的方法用于对比。 


风格与外观迁移。秉承加蒂斯（Gatys）等人[22]的杰出工作精神，我们通过3.3节中描述的优化方法实现了风格与外观迁移。在风格迁移方面，我们将我们的方法与“基于条件空间生成优化的风格迁移”（CSGO）[63]、“风格快照”（StyleShot）[33]、“风格标识”（StyleID）[10]、“基于变换器的风格迁移2”（StyTR2）[12]以及“神经风格迁移”（NST）[22]进行比较；在外观迁移方面，则与“跨图像注意力”（Cross-Image Attention）[2]和“拼接视觉变换器”（SpliceViT）[57]进行比较。图6展示了定性比较结果。在风格迁移中，我们的方法有效地捕捉到了高质量、连贯的风格特征，同时保留了内容图像的语义结构。这在第3行和第4行的草图风格中尤为明显。相比之下，基线方法尽管保留了原始结构，但存在明显的风格差异。在外观迁移中，我们的方法也表现出优越性，避免了“跨图像注意力”方法中出现的颜色过度饱和问题。
![](https://borninfreedom.github.io/images/2025/03/ad/7.png)
图7. 特定风格的文本到图像生成的比较。我们以三个风格参考图像为例，将我们的方法与“即时风格”（InstantStyle）[59]、“视觉风格提示”（Visual Style Prompting）[29]以及“反向扩散调制”（RB-Modulation）[49]进行比较。对于每种风格，我们使用相同的文本提示：“一只鹿”（左侧）、“一枚火箭”（右上角）和“一架钢琴”（右下角）。 

![](https://borninfreedom.github.io/images/2025/03/ad/8.png)
图8. 通过将注意力蒸馏引导采样与“控制网络”（ControlNet）[65]整合到文本到图像的流程中，我们能够生成高质量的结果，这些结果既能使生成图像的结构与条件图像保持一致，同时又能与风格参考图像保持风格上的连贯性。 


特定风格的文本到图像生成。如3.4节所述，我们可以在扩散采样中应用我们的注意力蒸馏损失，从而实现特定风格的文本到图像生成。我们将参考图像设置为期望的风格图像。图7展示了一些生成的结果，并与其他方法进行了比较，包括“视觉风格提示”（Visual Style Prompting）[29]、“即时风格”（InstantStyle）[59]和“反向扩散调制”（RB-Modulation）[49]。图7中的结果表明，我们的方法在文本语义对齐方面与现有方法相当，同时在与参考图像的风格一致性上表现得明显更好。除了上述方法，我们还进一步结合了“控制网络”（ControlNet）[65]，以便在各种模态（如深度图和Canny边缘图）的额外条件下实现特定风格的文本到图像生成。图8展示了一些生成的示例。更多结果见补充材料。

可控纹理合成。如3.2节所示，我们的方法可以应用于纹理优化。受文献[7]的启发，我们在计算注意力蒸馏损失时进一步结合了掩码引导，从而约束查询\(Q\)的值，实现可控的纹理合成。
![](https://borninfreedom.github.io/images/2025/03/ad/tmp3.png)


为了进一步确保与目标掩码对齐，我们用从源图像相应标记区域随机抽取的像素填充目标掩码作为初始化。然后，我们将这个初始化视为内容参考，并根据查询特征添加如3.3节所述的内容损失。图1和图9展示了我们的可控纹理合成结果。与“基于图像块的神经纹理优化”（GCD）[69]这种基于图像块的神经纹理优化方法相比，我们的结果在纹理细节上相当，且物体边缘更平滑。相比之下，GCD存在颜色混叠的瑕疵，见图9的第2行。

最近，“自校正”（Self-Rectification）[70]引入了一种“粗略编辑”控制来生成非平稳纹理。为了实现相同的目标，我们利用“基于随机微分方程的图像编辑”（SDEdit）[44]来保留用户编辑的布局图像的结构。然后，我们将我们提出的注意力蒸馏损失和内容损失融入到采样过程中（如3.4节所述）。如图9所示，“自校正”输出的纹理过渡更平滑，而我们的结果更贴合原始纹理示例。

![](https://borninfreedom.github.io/images/2025/03/ad/10.png)
图10. 纹理扩展的比较。我们以一张512×512的纹理图像作为输入示例，利用自动语义理解对其进行扩展，并合成分辨率为512×1536的结果。我们将我们的方法与基于图像块的神经纹理优化方法（GCD）[69]以及生成式参数化纹理模型（GPDM）[16]进行了比较。 

纹理扩展。由于图像块来源有限，使用传统方法合成超高分辨率的纹理非常困难。在这里，我们将注意力蒸馏引导采样应用于“多重扩散”（MultiDiffusion）[5]模型，实现了对任意分辨率的纹理扩展。尽管“稳定扩散1.5”（SD-1.5）[48]是在尺寸为\(512×512\)的图像上进行训练的，但令人惊讶的是，在结合注意力蒸馏后，它在大尺寸纹理合成方面表现出了强大的能力。图10展示了与“基于图像块的神经纹理优化”（GCD）[69]和“生成式参数化纹理模型”（GPDM）[16]在将纹理扩展到\(512×1536\)尺寸时的比较结果。在这样具有挑战性的任务中，我们的方法显示出了显著的优势。

## 4.2. 消融实验
在本节中，我们展示了对我们方法两个方面的消融实验结果：i）内容保留优化（3.3节）中内容损失权重的影响，以及ii）注意力蒸馏引导采样（3.4节）中用于管理引导强度的优化器的作用。

![](https://borninfreedom.github.io/images/2025/03/ad/11.png)
图11. 内容损失权重的消融实验。在两个示例上改变内容损失权重λ，分别是一个风格迁移的示例（左侧）和一个外观迁移的示例（右侧）。结果表明了内容损失在迁移过程中是如何帮助保留源图像内容的。当权重处于合适的范围内时，结果会呈现出不同程度的抽象效果和外观过渡。 

内容损失权重。如图11所示，改变内容损失权重\(\lambda\)会对迁移结果产生有趣的影响。例如，在风格迁移中，左侧展示的抽象风格示例说明了\(\lambda\)的调整如何导致不同程度的抽象效果，为艺术创作提供了灵活性。在外观迁移中，由于扩散网络对语义的精确理解，右侧展示的面部图像迁移随着\(\lambda\)的变化呈现出平滑的身份过渡。

![](https://borninfreedom.github.io/images/2025/03/ad/12.png)
图12. 用于管理引导强度的优化器的影响。使用和不使用Adam优化器[35]这两种情况，均进行了50步的扩散采样步骤。所有结果均使用“稳定扩散1.5”（Stable Diffusion v1.5）[48]生成，且文本提示为空。Adam优化器的学习率设置为0.02。 

优化器。图12展示了优化器在管理引导强度方面的重要性。我们通过实验测试了一种简单的策略，即手动设置引导强度来控制对潜在编码的梯度更新幅度。然而，手动调整这个强度往往无法得到合理的结果：示例中的纹理或外观特征通常会丢失，如图12的最后三列所示。相比之下，引入Adam优化器来管理潜在编码的优化得到的结果与输入示例的视觉特征非常匹配（图12的第2至4列）。此外，在采样过程中增加每个时间步长内的优化迭代次数通常会提高生成结果的质量，尽管这也会带来额外的计算时间。在实际应用中，我们将迭代次数设置为2，以实现高效的平衡，有效地生成高质量的结果。 

## 4.3. 用户偏好研究

![](https://borninfreedom.github.io/images/2025/03/ad/13.png)
图13. 用户偏好得分。我们给出了在三项迁移任务中，将我们的方法与选定的其他方法进行比较时的整体偏好得分。每次比较都是单独进行的，直接评估用户对我们的结果与每个竞争方法的结果之间的偏好。请注意，CIA代表“跨图像注意力”方法[2]，VSP代表文献[29]中的方法。更多详细信息请参阅补充材料。 


为了验证定性分析的结果，我们针对三项迁移任务设置了30个问题（对6个选定的对比方法各设置5个问题）来开展用户研究。在每个问题中，我们展示两个结果：一个来自我们的方法，另一个来自对比方法。我们要求用户根据给出的说明和标准，选择出更好的那个结果。我们从50位参与者那里收集到了1500份回复，整体的偏好得分总结如图13所示。我们的方法始终以显著优势胜过其他对比方法。更多细节请参考补充材料。


# 5. 结论
我们提出了一种统一的方法，适用于各种视觉特征迁移任务，包括风格/外观迁移、特定风格的图像生成以及纹理合成。所提出方法的关键在于一种新颖的注意力蒸馏损失，它通过计算理想风格化结果与当前风格化结果之间的差异，并逐步调整合成过程。我们的方法克服了先前研究工作的局限性，并且实验已经验证了其优越性。 


# 附录


![](https://borninfreedom.github.io/images/2025/03/ad/t1.png)

# A. 算法
我们的方法是基于预训练的“稳定扩散”（Stable Diffusion）模型构建的。算法1以风格迁移为例，概述了我们结合注意力蒸馏损失的内容保留优化方法。对于注意力蒸馏引导采样，我们以特定风格的文本到图像生成为例，并在算法2中描述了我们的方法。我们将变分自动编码器（VAE）的编码器和解码器分别表示为\(E(·)\)和\(D(·)\)，并使用\(\epsilon_{\theta}(·)\)表示去噪网络。在算法2中，\(Sampling(·)\)指的是从\(z_t\)到\(z_{t - 1}\)的一次扩散采样步骤，而\(AdaIN(·,·)\) [27]指的是对特征的方差和均值进行调制，以增强风格化效果。
# B. 实现细节
我们使用PyTorch框架实现了我们的方法，并应用了混合精度计算以节省时间和内存成本。对于特定风格的文本到图像生成，我们使用“稳定扩散超大模型”（SDXL）[46]；对于其他任务，我们采用“稳定扩散1.5版”（Stable Diffusion v1.5）[48]。遵循最近的研究工作[7, 29, 70]，我们从U型网络（U-Net）的最后六个自注意力层中提取注意力特征，以计算注意力蒸馏损失。为了进行比较，我们使用了所有基线方法的公开可用实现，并遵循它们建议的配置。所有实验都在单个英伟达（NVIDIA）RTX 6000 Ada GPU上进行。除了特定风格的文本到图像生成（学习率为0.015）之外，我们为Adam优化器设置了固定的学习率（0.05）。接下来，我们详细说明每个任务的具体配置。

风格/外观迁移。我们使用内容/结构图像对目标潜在编码进行初始化。内容损失是根据最后6个自注意力层的\(Q\)特征计算的。对于风格迁移，内容损失权重\(\lambda\)设置为0.25；对于外观迁移，该权重设置为0.2。默认情况下，我们对目标潜在编码进行200次迭代优化。所有实验生成的图像分辨率均为512x512。通过我们在潜在空间中的优化，合成一张图像的时间大约为30秒。

特定风格的文本到图像生成。我们使用SDXL生成分辨率为1024x1024的图像。采样过程使用去噪扩散隐式模型（DDIM）采样进行50步，无分类器引导的比例设置为7。在每个采样步骤中，我们利用注意力蒸馏损失对潜在编码进行2次迭代优化。整个过程耗时不超过30秒。默认情况下，Adam优化器的学习率设置为0.015。

![](https://borninfreedom.github.io/images/2025/03/ad/14.png)

![](https://borninfreedom.github.io/images/2025/03/ad/15.png)


可控纹理合成。对于掩码控制的纹理合成，我们将图像调整为512×512的分辨率，并以优化的方式进行合成。默认情况下，优化过程进行200次迭代。我们采用了与“基于图像块的神经纹理优化”（GCD）[69]相同的初始化策略，即我们用从源纹理语义对应区域中随机抽取的像素填充目标分割图。然而，如图15所示，U型网络特征的低空间分辨率使得掩码注意力蒸馏（Masked AD）损失不足以进行精确的空间控制。为了解决这个问题，我们使用初始化图像的\(Q\)特征来计算内容损失，内容权重\(\lambda\)为0.15。引入内容损失可在不影响纹理质量的前提下实现精确的空间对齐。对于像“自校正”（Self-Rectification）[70]那样的布局控制任务，我们直接使用颜色布局作为内容图像来计算内容损失。

纹理扩展。在这个任务中，示例纹理被调整为512×512的大小。为了提高效率，我们使用注意力蒸馏引导采样来生成结果。我们使用“多重扩散”（MultiDiffusion）[5]来合成超高分辨率的纹理，并取得了显著的成果；见图26中一个大小为4096×4096的示例。采样过程使用DDIM采样进行50步，且不使用无分类器引导。在每个采样步骤中，我们利用注意力蒸馏损失对潜在编码进行3次迭代优化。


# C. 额外实验
时间效率。对于纹理合成，可以采用优化或采样的方法。我们记录了不同方法所消耗的时间（不包括模型加载、编译以及图像编码/解码的时间）。具体来说，采样方法使用了50步的DDIM采样器，且不使用无分类器引导。对于这两种方法，Adam优化器都设置了固定的学习率0.05。通常情况下，非平稳纹理需要更多的迭代次数才能生成合理的空间结构。详细结果见表1和图14。 

![](https://borninfreedom.github.io/images/2025/03/ad/algo.png)

![](https://borninfreedom.github.io/images/2025/03/ad/16.png)
图16. 深入探究我们基于优化的方法和基于采样的方法之间的差异。上图：使用不同固定时间步长进行的优化（在优化过程中固定）。下图：使用干净潜在编码进行的优化（案例1）、使用含噪潜在编码进行的优化（案例2）以及使用含噪潜在编码进行的采样（案例3）。为了进行公平比较，我们为基于优化的方法（案例1和案例2）在每个时间步长上都添加了迭代次数。详情见正文内容。 


基于注意力蒸馏的优化方法与采样方法的深度比较。我们基于注意力蒸馏的优化方法和采样方法之间的主要区别在于所提取特征的本质。如算法1和算法2所示，基于采样的方法从随机反向的图像中提取特征，在这些图像中添加了与时间步长相关的预定噪声，这使得潜在编码不会在数据分布之外被优化。相比之下，我们基于优化的方法从干净的图像（即由VAE编码器编码的\(z_0\)）中提取特征。实验结果表明，通过调整时间步长\(t\)，可以提取从粗到细不同粒度级别的特征。如图16顶部所示，从不同时间步长提取的特征被用于计算注意力蒸馏（AD）损失，以优化相同的高斯噪声。使用学习率为0.05的Adam优化器并进行200次迭代，结果表明，对应较大时间步长的特征关注的是粗糙的结构，而来自较小时间步长的特征则关注精细的细节，这证明了我们基于优化的方法中如论文正文3.2节所述线性减小时间步长的必要性。

为了进一步研究这两种方法之间的差异，我们针对纹理合成设计了三个实验案例。案例1涉及使用从干净图像潜在编码中提取的特征来计算用于优化的AD损失。案例2使用从含噪潜在编码中提取的特征来实现相同的目的。案例3同样使用来自含噪潜在编码的特征，但在每个时间步长用U型网络去噪后对潜在编码进行优化，也就是我们的AD引导采样方法。在这些实验中，相同的高斯噪声被用作初始潜在编码，使用了学习率为0.05的Adam优化器，并且步数设置为50。如图16底部所示，案例1和案例2的比较表明，案例2产生的结果噪声更大，并且收敛速度慢得多。更重要的是，案例2和案例3的比较表明，我们的引导采样方法（或者说，等效地，用AD损失优化去噪U型网络采样的结果）显著提高了纹理合成的质量和速度。

![](https://borninfreedom.github.io/images/2025/03/ad/17.png)

超参数对特定风格的文本到图像生成的影响。我们研究了两个超参数，即采样中的优化迭代次数和学习率，对特定风格的文本到图像生成的影响。如图17所示，较低的学习率或较少的优化迭代次数会导致风格化不足，而增加学习率或优化迭代次数可能会导致从文本提示中得出的语义结构丢失。根据这项研究，我们将优化迭代次数设置为2，学习率设置为0.015作为默认值，以平衡图像质量、文本对齐和时间。

# D. 用户研究详情

![](https://borninfreedom.github.io/images/2025/03/ad/18.png)

我们针对三项迁移任务进行了用户研究，每项任务选择了两个对比方法。具体来说，对于风格迁移，我们比较了“风格标识”（StyleID）[10]和“基于变换器的风格迁移2”（StyTR2）[12]；对于外观迁移，比较了“跨图像注意力”（Cross-image Attention）[2]和“拼接视觉变换器”（SpliceViT）[57]；对于特定风格的文本到图像生成，比较了“即时风格”（InstantStyle）[59]和“视觉风格提示”（Visual Style Prompting）[29]。对于每项任务，如图18所示的用户界面会从我们的结果集合中随机呈现一组结果，在屏幕中央并排显示我们方法生成的结果以及一个对比方法的结果。参考图像或提示在左侧提供，屏幕顶部有评估标准的摘要。用户被要求选择更好的那个结果。每项任务的标准总结如下：

风格迁移：i）与内容图像的结构相似性，以及ii）与风格图像的风格相似性。

外观迁移：i）与结构图像的结构相似性，以及ii）与外观图像的外观相似性。

特定风格的文本到图像生成：i）与文本提示的语义对齐，以及ii）与风格参考图像的风格相似性。

# E. 局限性与讨论


虽然我们已经证明了注意力蒸馏损失在广泛的视觉特征迁移任务中的有效性，比如艺术风格和外观迁移、特定风格的文本到图像生成以及纹理合成，但仍有一些局限性需要注意。首先，我们观察到纹理扩展的结果偶尔会出现颜色过度饱和的情况。这个问题的出现是因为AD损失没有明确约束数据分布的一致性。相反，它依赖于模型对参考图像的理解来重新组合视觉元素。当生成图像的分辨率超出模型的训练范围时，聚合过程可能会产生不理想的结果。其次，在风格和外观迁移任务中，AD损失依赖于模型基于对图像的理解来建立语义对应关系的能力。当两张图像的内容差异很大时，模型的局限性可能会导致错误的语义匹配，从而对最终输出产生负面影响。两个示例如图19所示。

![](https://borninfreedom.github.io/images/2025/03/ad/19.png)


# F. 更多结果
最后，在下面的图中，我们提供了更多结果：

（1）在图20和图21中，我们展示了带有特定风格引导的创意性文本引导生成的更多结果。

（2）在图22中，我们展示了在不同内容和风格示例上更多的风格迁移结果。

（3）在图23中，我们展示了在无条件纹理合成方面的比较，以展示我们的注意力蒸馏损失对纹理的理解能力。我们应用了基于我们方法的优化方法和采样方法，并将它们与最先进的方法进行比较，包括“自校正”（Self-Rectification）[70]、“基于图像块的神经纹理优化”（GCD）[69]、“生成式参数化纹理模型”（GPDM）[16]和“切片瓦瑟斯坦距离”（SWD）[24]。

（4）在图24和图25中，我们展示了通过我们的引导采样方法实现的平稳和非平稳纹理合成与扩展的更多结果。

（5）最后，在图26中，我们通过使用512×512的示例生成尺寸为4096×4096的高分辨率图像，展示了极端的纹理扩展效果。 


![](https://borninfreedom.github.io/images/2025/03/ad/20.png)
![](https://borninfreedom.github.io/images/2025/03/ad/21.png)

![](https://borninfreedom.github.io/images/2025/03/ad/22.png)
![](https://borninfreedom.github.io/images/2025/03/ad/23.png)
图23. 无条件纹理合成的比较。请注意，“自校正”方法需要一个粗略的布局，但在这里，我们仅将一个随机初始化的结果作为目标提供给它。在我们展示的第4行和第6行的结果中，使用了经过微调的变分自动编码器解码器。 


![](https://borninfreedom.github.io/images/2025/03/ad/24.png)

![](https://borninfreedom.github.io/images/2025/03/ad/25.png)

![](https://borninfreedom.github.io/images/2025/03/ad/26.png)

