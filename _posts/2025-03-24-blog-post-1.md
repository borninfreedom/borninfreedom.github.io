---
title: "jetson orin nano super AI模型部署之路（二）保姆级最好用AI环境配置"
date: 2025-03-24
permalink: /posts/2025/03/blog-post-5/
tags:
-  jetson
---

对于产品发布来说，最合适的环境配置方式是使用docker，产品和环境可以一起发布，用户会得到最好的使用体验。jetson orin nano super 的AI docker环境配置有两个难点：

（1）CPU是arm架构
（2）super要使用jetpack 6.2[L4T 36.4.3]版本，因为版本太新，这个版本很多开源项目还没有发布对应的docker image，大部分的docker image都是基于jetpack 5.x构建的。

下面介绍一下我自己使用的包含pytorch和tensorrt（C++、python）等常用组件的docker image。

## 系统设置

check一下在jetosn orin nano super上安装的是JetPack 6。可以通过`jtop`工具来查看自己安装的jetpack版本。

![](https://borninfreedom.github.io/images/2025/03/jetson/1.png)


## 下载jetson-containers工具

jetson-containers可以通过模块化的方式来自动构建image，但是jetson-containers也有构建好的包含所有我们使用组件的image，我们用的就是他们构建好的image。

```bash
git clone https://github.com/dusty-nv/jetson-containers
bash jetson-containers/install.sh
```
安装脚本会提示您输入sudo密码，并会安装一些Python依赖项，并通过在`/usr/local/bin`下建立链接的方式将诸如autotag之类的工具添加到`$PATH`中（如果您移动了jetson-containers存储库，请再次运行此步骤）。


## 修改Docker默认运行时为nvidia

这一步建议做，不然每次启动container时，都要加上--runtime=nvidia，例如下面的启动指令，就要加上--runtime。

```bash
sudo docker run --runtime nvidia --gpus all --net host --ipc host -it --name pytorch_ngc_v2 -v /home:/home nvcr.io/nvidia/pytorch:25.01-py3-igpu
```

修改`/etc/docker/daemon.json`文件，将`"default-runtime": "nvidia"`添加到`/etc/docker/daemon.json`配置文件中：
``` json
{
    "runtimes": {
        "nvidia": {
            "path": "nvidia-container-runtime",
            "runtimeArgs": []
        }
    },

    "default-runtime": "nvidia"
}
```

然后重启Docker服务：
```bash
$ sudo systemctl restart docker
```
可以通过查看`docker info`来确认更改：

```bash
$ sudo docker info | grep 'Default Runtime'
Default Runtime: nvidia
```

## 重新定位Docker数据根目录

这一步如果jetson设备已经额外安装了硬盘，就一般不需要做了。或者自己的docker安装位置分区足够，也不需要做。


容器可能会占用大量磁盘空间。如果有可用的外部存储，建议将Docker容器缓存重新定位到更大的驱动器上（如果可能的话，NVME是首选）。如果尚未格式化，请将驱动器格式化为ext4格式，并使其在启动时挂载（即应在`/etc/fstab`中）。如果在Docker守护进程启动之前，驱动器在启动时未自动挂载，那么Docker将无法使用该目录。

将现有的Docker缓存从`/var/lib/docker`复制到您选择的驱动器上的目录（在本例中为`/mnt/docker`）：
```bash
$ sudo cp -r /var/lib/docker /mnt/docker
```
然后在`/etc/docker/daemon.json`中添加您的目录作为`"data-root"`：
``` json
{
    "runtimes": {
        "nvidia": {
            "path": "nvidia-container-runtime",
            "runtimeArgs": []
        }
    },

    "default-runtime": "nvidia",
    "data-root": "/mnt/docker"
}
```
然后重启Docker服务：

```bash
$ sudo systemctl restart docker
```

可以通过查看`docker info`来确认更改：

```bash
$ sudo docker info | grep 'Docker Root Dir'
Docker Root Dir: /mnt/docker
...
Default Runtime: nvidia
```

## docker pull设置代理

```python
sudo vi /etc/systemd/system/docker.service.d/http-proxy.conf
```

在文件中添加：
```bash
[Service]
Environment="HTTP_PROXY=http://192.168.1.10:7890"
Environment="HTTPS_PROXY=http://192.168.1.10:7890"
```

然后
```bash
sudo systemctl daemon-reload
sudo systemctl restart docker
```

## 增大swap分区

这一步建议做。因为jetson orin nano super只有8G的显存，对于跑更大的模型，如果swap分区足够大，也是可以跑得开的，只是慢一点罢了。

如果您要构建容器或处理大型模型，建议挂载交换分区（通常与开发板上的内存量相关）。运行以下命令来禁用ZRAM并创建一个交换文件：
``` bash
sudo systemctl disable nvzramconfig
sudo fallocate -l 16G /mnt/16GB.swap
sudo mkswap /mnt/16GB.swap
sudo swapon /mnt/16GB.swap
```
> 如果有可用的NVME存储，最好在NVME上分配交换文件。

然后在`/etc/fstab`的末尾添加以下行，以使更改永久生效：
``` bash
/mnt/16GB.swap  none  swap  sw 0  0
```

## 禁用桌面图形用户界面

如果内存不足，您可能需要尝试禁用Ubuntu桌面图形用户界面（GUI）。这将释放窗口管理器和桌面所占用的额外内存（对于Unity/GNOME约为800MB，对于LXDE约为250MB）。

在我的机器上，图形用户界面占用了450M左右的memory。将它关掉还是能省很多的memory的。一般我都是不用图形化的时候就先关掉，用的时候再打开。
![](https://borninfreedom.github.io/images/2025/03/jetson/2.png)



可以临时禁用桌面，在控制台中运行命令，然后在需要时重新启动桌面：
``` bash
$ sudo init 3     # 停止桌面
# 使用Ctrl+Alt+F1、F2等组合键让用户重新登录到控制台
$ sudo init 5     # 重新启动桌面
```
如果希望在重启后该设置仍然生效，可以使用以下命令来更改启动行为：
``` bash
$ sudo systemctl set-default multi-user.target     # 启动时禁用桌面
$ sudo systemctl set-default graphical.target      # 启动时启用桌面
```

## 将用户添加到Docker组

由于Ubuntu用户默认不在`docker`组中，他们需要使用`sudo`来运行docker命令（构建工具在需要时会自动执行此操作）。因此，在构建过程中可能会定期要求您输入sudo密码。

相反，您可以按如下方式将用户添加到docker组：
```bash
sudo usermod -aG docker $USER
```
然后关闭/重新启动终端（或注销），您应该能够在无需使用sudo的情况下运行docker命令（例如`docker info`）。

## 设置电源模式

根据Jetson设备可用的电源来源（即墙上电源或电池），您可能希望将Jetson设置为最大功率模式（MAX-N），以获得Jetson设备的最高性能。您可以使用[`nvpmodel`](https://docs.nvidia.com/jetson/archives/r36.2/DeveloperGuide/SD/PlatformPowerAndPerformance/JetsonOrinNanoSeriesJetsonOrinNxSeriesAndJetsonAgxOrinSeries.html#power-mode-controls)命令行工具来实现，或者通过Ubuntu桌面使用[nvpmodel图形用户界面小部件](https://docs.nvidia.com/jetson/archives/r36.2/DeveloperGuide/SD/PlatformPowerAndPerformance/JetsonOrinNanoSeriesJetsonOrinNxSeriesAndJetsonAgxOrinSeries.html#nvpmodel-gui)（或者使用jetson-stats中的[`jtop`](https://github.com/rbonghi/jetson_stats)）。
```bash
# 检查当前电源模式
$ sudo nvpmodel -q
NV Power Mode: MODE_30W
2

# 将其设置为模式0（通常是最高模式）
$ sudo nvpmodel -m 0

# 如有必要，重启并确认更改
$ sudo nvpmodel -q
NV Power Mode: MAXN
0
```
![](https://borninfreedom.github.io/images/2025/03/jetson/3.png)
我的当前电源模式是功率最高的设置，从jtop的右下角可以看到实时的功率信息。

有关不同Jetson设备可用的电源模式表，以及[`nvpmodel`](https://docs.nvidia.com/jetson/archives/r36.2/DeveloperGuide/SD/PlatformPowerAndPerformance/JetsonOrinNanoSeriesJetsonOrinNxSeriesAndJetsonAgxOrinSeries.html#power-mode-controls)工具的文档，请参阅[此处](https://docs.nvidia.com/jetson/archives/r36.2/DeveloperGuide/SD/PlatformPowerAndPerformance/JetsonOrinNanoSeriesJetsonOrinNxSeriesAndJetsonAgxOrinSeries.html#supported-modes-and-power-efficiency)。 

## 拉取docker镜像

我们使用jetson-containers工具来自动匹配我们的机器，这个命令会查看当前jetson的jetpack版本以及当前host的其他组件的版本，来自动选择合适的docker image。

```bash
# automatically pull or build a compatible container image
jetson-containers run $(autotag nanoowl)
```
这个命令在我的机器上，其实是直接拉取的`dustynv/nanoowl:r36.4.0`这个镜像。


拉取完成后，镜像会自动运行，我们可以直接ctrl+D退出，使用我们的自定义命令重新打开。

```bash
sudo docker run --runtime nvidia --gpus all --net host --ipc host -it --name ai_all_in_one  -v /home:/home dustynv/nanoow:r36.4.0
```

这样就可以开始美滋滋的AI模型训练、部署之路了。
