---
title: "pytorch hook"
date: 2023-09-05
permalink: /posts/2023/09/blog-post-1/
tags:
  - pytorch
  - hook
---

pytorch的hook机制允许我们在不修改模型class的情况下，去debug backward、查看forward的activations和修改梯度。hook是一个在forward和backward计算时可以被执行的函数。在pytorch中，可以对`Tensor`和`nn.Module`添加hook。hook有两种类型，`forward hook`和`backward hook`。

## 1. 对Tensors添加hook

对于Tensors来说，只有backward hook，没有forward hook。对于backward hook来说，其函数输入输出形式是 `hook(grad) -> Tensor or None`。其中，grad是pytorch执行backward之后，一个tensor的grad属性值。

例如：
```python
import torch 
a = torch.ones(5)
a.requires_grad = True

b = 2*a
c = b.mean()
c.backward()

print(f'a.grad = {a.grad}, b.grad = {b.grad}')
```

输出：
```
a.grad = tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000]), b.grad = None
```
由于b不是叶子节点，因此在计算完梯度后，b的grad会被释放。因此，b.grad=None。这里，我们要显式的指定不释放掉非叶子节点的grad。代码改为下面这样：

```python
import torch 
a = torch.ones(5)
a.requires_grad = True

b = 2*a

b.retain_grad()   # 让非叶子节点b的梯度保持
c = b.mean()
c.backward()

print(f'a.grad = {a.grad}, b.grad = {b.grad}')
```

输出：

```
a.grad = tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000]), b.grad = tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])
```

我们可以通过加print的方式来查看一个tensor的梯度值，也可以通过加hook的方式来实现这点。

```python
import torch

a = torch.ones(5)

a.requires_grad = True

b = 2*a

a.register_hook(lambda x:print(f'a.grad = {x}'))
b.register_hook(lambda x: print(f'b.grad = {x}'))  

c = b.mean()

c.backward() 
```

输出：

```
b.grad = tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])
a.grad = tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000])
```

使用hook的一个好处是：代码中的`b.retain_grad()   # 让非叶子节点b的梯度保持` 这句可以删除掉，同样可以记录到非叶子节点的值。对于不方便修改源码的程序，可以通过对tensors添加hook查看梯度。同时，`.retain_grad()`操作会增加显存的使用。

另外一点对Tensors使用hook的好处是，可以对backward时的梯度进行修改。来看一个更加实际具体的例子：



## 2. 对nn.Module添加hook

对nn.Module添加hook的函数输入输出形式为：

backward hook：`hook(module, grad_input, grad_output) -> Tensor or None`

forward hook：`hook(module, input, output) -> None`










```python
import torch.nn as nn
import torch
class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(2,2)
        self.s1 = nn.Sigmoid()
        self.fc2 = nn.Linear(2,2)
        self.s2 = nn.Sigmoid()
        self.fc1.weight = torch.nn.Parameter(torch.Tensor([[0.15,0.2],[0.250,0.30]]))
        self.fc1.bias = torch.nn.Parameter(torch.Tensor([0.35]))
        self.fc2.weight = torch.nn.Parameter(torch.Tensor([[0.4,0.45],[0.5,0.55]]))
        self.fc2.bias = torch.nn.Parameter(torch.Tensor([0.6]))
        
    def forward(self, x):
        x= self.fc1(x)
        x = self.s1(x)
        x= self.fc2(x)
        x = self.s2(x)
        return x

net = Net()
print(net)

out = net(data)
target = torch.Tensor([0.01,0.99])  # a dummy target, for example
criterion = nn.MSELoss()
loss = criterion(out, target)

class Hook():
    def __init__(self, module, backward=False):
        if backward==False:
            self.hook = module.register_forward_hook(self.hook_fn)
        else:
            self.hook = module.register_backward_hook(self.hook_fn)
    def hook_fn(self, module, input, output):
        self.input = input
        self.output = output
    def close(self):
        self.hook.remove()

# register hooks on each layer
hookF = [Hook(layer[1]) for layer in list(net._modules.items())]
hookB = [Hook(layer[1],backward=True) for layer in list(net._modules.items())]

out=net(data)

out.backward(torch.tensor([1,1],dtype=torch.float),retain_graph=True)

print('***'*3+'  Forward Hooks Inputs & Outputs  '+'***'*3)
for hook in hookF:
    print(hook.input)
    print(hook.output)
    print('---'*17)

print('***'*3+'  Backward Hooks Inputs & Outputs  '+'***'*3)
for hook in hookB:             
    print(hook.input)          
    print(hook.output)         
    print('---'*17)

    

```