---
title: "hdrnet,Deep Bilateral Learning for Real-Time Image Enhancement解读"
date: 2024-10-28
permalink: /posts/2024/10/blog-post-12/
tags:
  - hdrnet
  - 拍照算法
---

论文、代码和ppt地址：[Deep Bilateral Learning
for Real-Time Image Enhancement](https://groups.csail.mit.edu/graphics/hdrnet/)

论文使用的数据集：

1. **HDR+**: 这是一个复杂的摄影管道，包括色彩校正、自动曝光、去雾和色调映射等操作。

2. **MIT “FiveK” 数据集**: 这个数据集由 Bychkovsky 等人 提供，包含了500张用于验证和测试的图像，其余4500张用于训练。数据集中的图像被用来应用各种效果，并且通过随机裁剪、翻转和旋转进行数据增强。

3. **面部明亮化数据集**: 这个数据集包含了标记的面部图像，用于面部明亮化任务。

4. **n-Styles Transfer 数据集**: 这个数据集是通过将 MIT “FiveK” 数据集中的每个图像映射到100个不同的图像（风格目标）来生成的。

这些多样化的数据集使得模型能够在不同的图像操作和主观人类调整上学习，从而在各种图像增强任务上达到高质量的结果。



论文开头放了一张效果图

![](https://borninfreedom.github.io/images/2024/10/hdrnet/1.png)

这张图显示了hdrnet算法，在保证了实时处理性能的基础上，和HDR+算法取得了基本差不多的效果。这里解释下tone mapping。

"tone-mapped for visualization" 指的是将高动态范围（HDR）图像通过色调映射（tone mapping）技术转换成低动态范围（LDR）图像，以便在常规显示设备上进行可视化查看。由于HDR图像能够记录比常规显示设备更广泛的亮度范围，直接显示HDR图像可能会导致细节丢失，尤其是在非常亮或非常暗的区域。色调映射技术通过非线性变换将HDR图像的动态范围压缩，使其适应显示设备的动态范围，从而在显示设备上提供更自然和愉悦的图像再现。

色调映射过程通常包括以下几个关键步骤：
1. **全局色调映射**（Global Tone Mapping）：通过一个全局的色调映射算子（如Reinhard等人提出的算法），将整个HDR图像的亮度范围压缩到显示设备可以处理的范围内。
2. **局部色调映射**（Local Tone Mapping）：考虑图像中不同区域的局部亮度和细节，通过更复杂的算法（如 bilateral filtering 或者 multi-scale processing）来保留更多的细节和对比度。
3. **细节增强**（Detail Enhancement）：在色调映射过程中，可能会丢失一些细节，因此需要通过一些技术手段（如高频提升）来增强图像的细节。
4. **颜色保护**（Color Preservation）：在色调映射过程中保持颜色的真实性和自然性，避免颜色饱和度的过度损失。

最终的目的是生成在视觉上令人愉悦、细节丰富的LDR图像，这些图像能够在标准的显示设备上忠实地再现HDR图像的场景。这种技术特别适用于摄影、电影、动画渲染等领域，以及任何需要在有限的显示设备上查看高动态范围内容的场合。


性能是移动图像处理中的一个关键挑战。不管是使用一个标准的图像处理流程，抑或者是人工精修的图像对，我们的目标是再现这些图像的增强效果，并实现实时运行。为此，我们引入了一种受双边网格处理和局部仿射颜色变换启发的新型神经网络架构。通过使用输入/输出图像对，我们训练一个卷积神经网络来预测双边空间中的局部仿射模型的系数。我们的架构学会做出局部的、全局的和内容依赖的决策，以近似所需的图像转换。在运行时，神经网络输入图像的低分辨率版本，产生一组双边空间中的仿射变换，使用一种新的切片节点以保留边缘的方式上采样这些变换，然后将这些上采样的变换应用到全分辨率图像上。我们的算法能够在智能手机上在毫秒级处理高分辨率图像，提供1080p分辨率的实时取景器，并在大量图像操作类上匹配最先进的近似技术的质量。

# INTRODUCTION

当代相机和移动设备产生的高分辨率图像和视频对图像处理算法提出了巨大的性能压力，这要求技术娴熟的程序员进行复杂的代码优化。虽然在系统优化的贡献上已经寻求促进高性能可执行文件的实现，例如 [Hegarty 等人 2014；Mullapudi 等人 2016；Ragan-Kelley 等人 2012]，但它们需要程序员的专业知识，它们的运行时成本随着图像处理pipeline的复杂性增长，并且仅在图像处理的filter的源代码可用时才适用。此外，由于图像增强是主观的，通常希望能够直接从人为调整中学习增强模型，例如 [Bychkovsky 等人 2011]。为此，我们提出了一种机器学习方法，其中参考filter、图像处理pipeline，甚至主观手动照片调整的效果。通过一个深度网络学习，该网络可以快速评估，并且制作图像处理系统与图像目标质量的复杂性无关。我们专注于不会产生空间扭曲图像或添加新边缘的计算摄影增强，例如 [Aubry 等人 2014；Hasinoff 等人 2016]。

我们与之前的工作一样，作者也在想办法加速“黑盒”深度网络图像处理操作的性能，无论是通过使用远程服务器[Gharbi et al. 2015]，还是通过处理低分辨率图像，然后使用低分辨率输出来近似高分辨率等效图像[Chen et al. 2016]。对于一些操作，这些方法可以实现大幅加速，但它们存在显著的局限性：底层的图像处理操作必须是在某种程度上不随尺度变化的（图9），并且必须在低分辨率下快速运行。此外，这些技术依赖于显式参考实现的可用性，因此不能用于从人类标注的输入/输出对数据集中学习隐式定义的操作。

![](https://borninfreedom.github.io/images/2024/10/hdrnet/9.png)
图9，我们的方法（d）能够学习复制对于不随尺度变化的操作的正确效果（b），例如这里展示的局部拉普拉斯滤波器（a–b）。像只在低分辨率下使用双向引导上采样这样的方法（a–b中的小图）会产生看起来不同的输出（c）。差异在箭头指向的区域最为明显。


许多深度学习架构已被用于图像到图像的转换，例如[Isola et al. 2016; Liu et al. 2016; Long et al. 2015; Xu et al. 2015; Yan et al. 2016]。然而，大多数先前的工作会产生与输入图像大小成线性关系的比较高的计算成本，这通常是因为在全分辨率下必须做大量的叠加卷积和非线性操作。这种一般形式允许学习灵活的模型，但这种表达力是有代价的：这样的架构对于实时预览应用来说慢了几个数量级，在最好的桌面GPU上处理1百万像素图像需要几秒钟——比我们提出的模型慢了1000倍以上（我们的模型在GPU上是2毫秒）。我们的加速是通过特别针对摄影变换实现的，这些变换通常可以用双边空间中的线性操作很好地近似[Chen et al. 2016]，因此我们的模型是在这个空间中学习的。

我们提出了一种新的网络架构，能够学习丰富多样的摄影图像增强，并且可以在高分辨率输入上快速运行。我们通过三个关键策略实现这一点：

1）我们在低分辨率双边网格[Chen et al. 2007]中执行大部分预测，其中每个像素的x，y坐标增加了第三个维度，这个维度是像素颜色的函数。为此，我们引入了一个新的深度学习节点，执行数据依赖的查找。这使得所谓的切片操作成为可能，它通过考虑每个像素的输入颜色以及其x，y位置，在全图像分辨率下从3D双边网格重建输出图像。

2）我们遵循之前的工作，观察到通常预测从输入到输出的转换比直接预测输出要简单，例如[Chen et al. 2016; Gharbi et al. 2015; Shih et al. 2013]。这就是为什么我们的架构被设计为学习一个局部仿射颜色变换作为中间表示，该变换将通过一个新的乘法节点应用于输入。

3）虽然我们的大部分学习和推理是在低分辨率下进行的，但在训练期间使用的损失函数是在全分辨率下评估的，这导致我们学习的低分辨率变换直接针对高分辨率图像的影响进行了优化。

综合这三个策略（切片、仿射颜色变换和全分辨率损失），我们能够在低分辨率下执行大部分处理（从而节省大量的计算成本），同时再现参考操作的高频行为。


我们通过在7个应用的基准测试中展示了我们模型的表现力，包括：模拟已发布的图像滤镜[Aubry et al. 2014; Hasinoff et al. 2016]、逆向工程黑盒Photoshop操作，以及从一组手动校正的照片[Bychkovsky et al. 2011]中学习摄影师的修饰风格。我们的技术产生的输出质量与以往工作相当或更好，同时更广泛地适用，因为它不需要被近似的图像操作的某些参考实现，可以从输入/输出图像对中端到端地学习，并且能够在移动硬件上实时运行。我们的网络的前向传递在Google Pixel手机上处理全屏分辨率1920×1080的图像仅需14毫秒，从而实现50Hz的实时预览效果。


# 2 相关工作

尽管图像增强算法一直是大量研究的重点，但大多数复杂的算法在移动设备上快速评估的成本太高，而绝大多数的数字图像正是在移动设备上捕获和处理的。因此，以前的工作已经确定了特定的关键操作，并开发了新的算法来加速它们。例如，Farbman等人[2011]引入了卷积金字塔来加速线性平移不变滤波器。同样，由于边缘感知图像处理的普遍性[Adams等人2010; Chen等人2007; Paris和Durand2006; Tomasi和Manduchi1998]，许多方法已经被提出来加速双边滤波。

加速一个操作符的一种方法是简单地在低分辨率下应用它，然后上采样结果。一个简单的上采样通常会导致不可接受的模糊输出，但这个问题通常可以通过使用一种更复杂的上采样技术来改善，这种技术保留原始图像的边缘。联合双边上采样[Kopf等人2007]通过在高分辨率引导图上使用双边滤波器来实现这一点，产生分段平滑的边缘感知上采样。双边空间优化[Barron等人2015; Barron和Poole2016]在这一思想的基础上，通过在双边网格内解决一个紧凑的优化问题，产生最大平滑度的上采样结果。

Gharbi等人[2015]专注于学习从输入到输出的转换，而不是输出本身。他们用一组简单的局部模型——一个转换操作——来近似一大类复杂的空间变化操作符，这个操作是为给定的输入/输出对量身定制的。计算操作符和拟合操作的任务被转移到云端，而移动设备只需要应用这个转换操作，从而节省时间和能源。同样，Chen等人[2016]用双边空间中的局部仿射模型网格来近似一个图像操作符，其参数被拟合到输入/输出对，类似于引导滤波器[He等人2013]。通过在低分辨率图像对上执行这种模型拟合，这种技术使得设备上的实时计算成为可能。我们在这个双边空间表示的基础上构建，但我们不是拟合一个模型来近似来自一对图像的单个操作符实例，而是构建一个丰富的CNN类模型，该模型被训练应用于任何未见过的输入。这绕过了在运行时需要原始操作符的需求，并为我们提供了学习非算法转换的机会（即，手动调整的输入/输出图像对）。这也允许我们优化仿射系数以模拟在全分辨率下运行的操作符，这对于随尺度变化的滤波器很重要（图9）。

神经网络在图像处理方面的应用。最近，深度卷积网络在低级视觉和图像处理任务上取得了显著进展，例如深度估计[Eigen等人2014]、光流[Ilg等人2016]、超分辨率[Dong等人2014]、去马赛克和去噪[Gharbi等人2016; Zhang等人2016]、图像抠图[Shen等人2016]、着色[Iizuka等人2016]，以及一般的图像到图像的“翻译”任务[Isola等人2016]。最近的工作甚至探索了在双边网格内学习深度网络[Jampani等人2016]，尽管这项工作没有解决我们在这个空间中学习图像转换的任务，而是专注于分类和语义分割。

一些架构已经被训练来近似一类操作符。Xu等人[2015]在梯度域中开发了一个三层网络，以加速边缘感知平滑滤波器。Liu等人[2016]提出了一个架构来学习去噪、图像平滑、修复和颜色插值的递归滤波器。他们联合训练了一系列递归网络和一个卷积网络，以预测图像依赖的传播权重。尽管这些工作可以在桌面GPU上以交互速率处理低分辨率图像，但对于我们的应用来说，它们仍然太慢：在移动设备上实时处理高分辨率图像。

自动照片编辑。我们的模型可以被训练以自动校正由人工修图师提供的输入/输出图像对。这是Bychkovsky等人[2011]引入的任务，他们估计了5位训练有素的摄影师个人风格的全局亮度/对比度调整。他们在5000张原始图像的数据集上训练了一个回归模型，该模型使用手工制作的特征，这些特征捕获了低级信息和语义内容（例如，面部）。Hwang等人[2012]采用从粗到细的搜索方法来寻找最匹配的场景，对于一张500×333的图像需要超过一分钟的时间。Kaufman等人[2012]从硬编码的特征（面部、蓝天、云彩、曝光不足的区域）学习局部颜色和对比度操作，处理一张VGA图像需要超过2分钟。最近，Yan等人[2016]使用了一个紧凑的逐像素神经网络和手工制作的特征。他们的网络需要1.5秒来处理一张百万像素的图像（除了用于他们特征中的对象检测、密集图像分割和场景识别所需的时间之外）。我们的模型可以学习类似的全局色调调整，并且可以推广到更复杂的效果，包括颜色校正和局部编辑，此外，它的速度要快得多。


# 3 我们的架构

我们提出了一种新的卷积网络架构，可以被训练以执行快速的图像增强（见图2）。我们的模型旨在具有表现力、保留边缘，并在全分辨率下需要有限的计算量。它是完全端到端可训练的，并且可以在现代智能手机上以1080p的分辨率实时运行。

![](https://borninfreedom.github.io/images/2024/10/hdrnet/2.png)
图2. 我们的新网络架构旨在尽可能在低分辨率下执行计算，同时在全图像分辨率下捕捉高频效应。它由两个在不同分辨率下操作的不同流组成。低分辨率流（顶部）处理输入I的下采样版本˜I，通过几个卷积层来估计仿射系数的双边网格A。这个低分辨率流进一步分为两条路径来学习局部特征Li和全局特征Gi，它们在最终预测前被融合（F）。全局和局部路径共享一组共同的低级特征Si。反过来，高分辨率流（底部）执行最小但关键的工作量：它学习一个灰度引导图д，我们的新切片节点用它来将仿射系数网格上采样回全分辨率¯A。然后，这些逐像素的局部仿射变换被应用于全分辨率输入，产生最终输出O。


我们在低分辨率副本˜I上执行大部分推理，这是输入I在低分辨率流（图2，顶部）中的副本，最终预测出类似于双边网格的局部仿射变换[Chen等人2016]。根据我们的经验，图像增强通常不仅依赖于局部图像特征，还依赖于全局图像特征，如直方图、平均强度甚至场景类别。因此，我们的低分辨率流进一步分为本地路径和全局路径。然后，我们的架构将这两个路径融合，以产生代表仿射变换的最终系数。

高分辨率流（图2，底部）以全分辨率工作，执行最小的计算，但关键作用是捕捉高频效应并在需要时保留边缘。为此，我们引入了一个受双边网格处理启发的切片节点[Chen等人2007；Paris和Durand2006]。这个节点根据学习到的引导图在低分辨率网格中执行数据依赖的查找。给定通过全分辨率引导图切片得到的高分辨率仿射系数，我们对每个像素应用局部颜色变换以产生最终输出O。在训练时，我们在全分辨率下最小化我们的损失函数。这意味着只处理大量下采样数据的低分辨率流仍然学习中间特征和仿射系数，这些特征和系数可以再现高频效应。

作为首次近似，人们可以将我们的工作视为减轻Chen等人的双边引导上采样[2016]在运行时对参考滤波器的需求。从某种意义上说，我们试图预测给定图像的低分辨率版本时双边网格中的仿射颜色变换系数。然而，有几个关键元素超出了这一点。首先，进入双边网格的下采样是学习的。其次，引导图像也是学习的，并且不仅限于亮度。最后，我们不是在仿射系数上应用损失函数，而是在全分辨率的最终图像上应用，这使我们能够捕捉高频效应并处理不随尺度变化的操作符（图9）。我们在图3、4、5和7中通过消融研究说明了我们架构中每个组件的作用。


![](https://borninfreedom.github.io/images/2024/10/hdrnet/3.png)
图3. 我们的低级卷积层是完全学习的，并且可以提取语义信息。将这些层替换为标准的双边网格splatting操作会导致网络失去大部分的表现力。在这个面部提亮操作的例子中（a-b），使用硬编码splatting的网络（d）无法正确检测面部，因为网格的分辨率太低。相反，它轻微提亮了所有肤色，如在手上可见。我们通过带有步长的卷积进行渐进式下采样，学习了解决这个任务所需的语义特征（c），仅提亮面部同时像参考图一样使背景变暗。

![](https://borninfreedom.github.io/images/2024/10/hdrnet/4.png)
图4. 我们架构中的全局特征路径允许我们的模型对整幅图像进行推理，例如，在复制可能受到强度分布或场景类型影响的主观人类调整等主观任务中（a）。没有全局路径，模型可能会做出空间上不一致的局部决策（b）。在这里，网络未能识别出左上角的蓝色区域也属于天空，因此应该接受与它下方区域相同的校正。

![](https://borninfreedom.github.io/images/2024/10/hdrnet/5.png)
图5. 我们的新切片节点是我们架构的表现力及其处理高分辨率效果的核心。将这个节点替换为标准的可学习反卷积滤波器组会降低表现力（b），因为预测输出像素时没有使用全分辨率数据。得益于其学习到的全分辨率引导图，我们的切片层能够以更高的保真度近似所需的增强（c），从而保留输入的边缘（a）并捕捉到地面真实输出中可见的高频变换（d）。


![](https://borninfreedom.github.io/images/2024/10/hdrnet/7.png)
图7. 我们的切片节点使用了一个学习到的引导图。使用亮度作为引导在HDR+管道的再现中会导致伪影，特别是在额头和脸颊的高光区域出现海报化伪影（b）。相比之下，我们学习到的引导图（c）正确地再现了地面真实情况（d）。

## 3.1 低分辨率预测双边系数

低分辨率流的输入˜I具有固定的分辨率256×256。它首先通过一系列步长的卷积层（Si）i=1,...,nS进行处理，以提取低级特征并降低空间分辨率。然后，在Iizuka等人[2016]的设计启发下，最后的低级特征通过两条不对称路径进行处理：第一条路径（Li）i=1,...,nL是全卷积的[Long等人2015]，专门学习传播图像数据的同时保留空间信息的局部特征。第二条路径（Gi）i=1,...,nG使用卷积和全连接层来学习一个固定大小的全局特征向量（例如，高级场景类别、室内/室外等），其感受野覆盖整个低分辨率图像˜I。两条路径的输出，GnG和LnL，然后被融合到一个共同的特征集F中。一个逐点线性层从融合的流中输出最终的数组A。我们将这个数组解释为双边网格中的仿射系数（第3.2节）。由于我们以内容依赖的方式从2D图像中产生3D双边网格，因此我们可以将低分辨率流视为实现了一种学习到的splatting形式。

### 3.1.1 低级特征

我们首先使用一系列标准步长卷积层处理低分辨率图像S0 := ˜I，步长s = 2（见图2）：

![](https://borninfreedom.github.io/images/2024/10/hdrnet/m1.png)

其中，i=1,...,n_s是每层的索引，c和c^'是每层通道的索引，w^i是卷积层的权重，b^i是偏置的向量，这个加法操作的范围限制是x^'大于等于-1，y^'小于等于1（卷积层的卷积核是3x3的，zero-padding的）。使用ReLU激活函数fai。

这些低级层逐渐将空间维度减少一个总共的因子2^ns。因此，2^ns有两个效果：1) 它推动低分辨率输入~I和最终仿射系数网格之间的空间下采样,ns越高，最终网格越粗糙；2)ns控制预测的复杂性：更深层的层具有指数级更大的空间支持和更复杂的非线性（通过组合）；因此，它们能够提取输入中更复杂的模式。图3显示了将网络的比较深层的层去掉，并被硬编码的splatting操作[Chen et al. 2007]所取代。没有这些层，网络失去了很多表现力。我们的架构使用了ns=4个低级层。表1总结了每层的维度。


![](https://borninfreedom.github.io/images/2024/10/hdrnet/t1.png)


### 3.1.2 Local features path

最后一层低级特征层S^ns接着被局部路径中的n_L=2个卷积层L^i 处理（图2，黄色）。这些层与公式（1）的形式相同，确定L^0:=S^ns，但这次步长s=1。我们保持局部路径中的空间分辨率和特征数量不变。因为分辨率保持不变，所以滤波器的空间支持仅随n^L线性增长。足够深的卷积层堆叠，大致为n^S+n^L，对于捕获有用的语义特征[Krizhevsky et al. 2012]是至关重要的。如果需要更高空间分辨率的最终系数网格，可以通过减少n^S并相应增加n^L来补偿，以免降低网络的表现力。没有局部路径，预测的系数将失去任何空间位置的概念。

### 3.1.3 Global features path

与局部路径类似，全局特征路径也从S^ns分支出去，即G^0:=S^ns。它包括两个步长的卷积层（公式（1），步长S = 2）后跟三个全连接层，总共有N^G=5个全局层（图2，蓝色）。使用全连接层的一个结果是输入全连阶层的分辨率需要固定，因为这决定了G^2的维度以及作用于其上的网络参数数量。正如我们将在第3.3节看到的，得益于我们的切片操作符，我们仍然可以处理任何分辨率的图像，尽管low-res流的大小是固定的。

全局路径产生一个64维的向量，总结了输入的全局信息，并作为先验来规范局部路径所做的局部决策。没有全局特征来编码输入的这种高级描述，网络可能会做出错误的局部决策，导致像图4中天空中大规模变化那样的伪影。

### 3.1.4 Fusion and linear prediction

我们将局部路径和全局路径的输出通过逐点仿射混合后跟一个ReLU激活函数结合起来：

![](https://borninfreedom.github.io/images/2024/10/hdrnet/m2.png)

这将产生一个16×16×64的特征数组，我们从中做出最终的1×1线性预测，以产生一个具有96个通道的16×16映射图。

![](https://borninfreedom.github.io/images/2024/10/hdrnet/m3.png)

## 3.2 将图像特征视为双边网格

到目前为止，我们已经将我们的模型描述为一个神经网络。现在我们转换视角，从双边网格的角度来看待它。为了便于理解，我们稍微滥用一下符号，偶尔会将最终的特征图A视为一个多通道的双边网格，其第三维度已经展开：
![](https://borninfreedom.github.io/images/2024/10/hdrnet/m4.png)

当d=8时，可以理解为网格的深度。在这种解释下，A可以被看作是一个16×16×8的双边网格，其中每个网格单元包含12个数字，每个数字对应一个3×4仿射颜色变换矩阵的系数。这种重塑让我们可以将方程（1）中的步长卷积解释为在双边域中的作用，其中它们对应于(x, y)维度上的卷积，并在z和c维度上表达全连接。因此，这种操作比简单地在网格中应用3D卷积更具表现力，后者只会在z上引入局部连接 [Jampani et al. 2016]。它也比标准的双边网格splatting更具表现力，后者将图像I离散化为几个强度箱，然后对结果进行盒式滤波 [Chen et al. 2007]；这个操作可以用一个2层网络轻松表达。从某种意义上说，通过在整个过程中保持2D卷积的形式，并且只将最后一层解释为双边网格，我们让网络决定2D到3D转换的最佳时机。

## 3.3 使用可训练的切片层进行上采样

到目前为止，我们已经描述了如何使用我们的网络的低分辨率流~I从低分辨率图像学习预测双边网格系数A。现在，我们需要将这些信息重新传输回原始输入I的高分辨率空间，以产生我们的最终输出图像。为此，我们引入了一个基于双边网格切片操作的层 [Chen et al. 2007]。这一层接收一个单通道引导图g和一个特征图A（被视为双边网格），其空间分辨率远低于g。它在最终特征图A中执行数据依赖的查找。该层对A和g都是可微分的，这允许我们在训练时通过它进行反向传播。

切片操作的结果是一个新特征图A_，其空间分辨率与g相同，通过在g定义的位置处对A的系数进行三线性插值获得：

![](https://borninfreedom.github.io/images/2024/10/hdrnet/m5.png)

其中![](https://borninfreedom.github.io/images/2024/10/hdrnet/m6.png)

是线性插值核，s_x和s_y是网格尺寸与全分辨率图像尺寸的比例。本质上，每个像素被分配了网格深度由g[x,y]的灰度值给出的系数向量，即![](https://borninfreedom.github.io/images/2024/10/hdrnet/m7.png)。Flownet2 [Ilg et al. 2016] 和 Spatial Transformer Networks [Jaderberg et al. 2015] 也使用了类似的插值操作来进行网络内的空间变形。我们固定网格的空间分辨率为 16×16，深度为d=8。

切片操作是无参数的，并且可以高效地在 OpenGL 着色器中实现 [Chen et al. 2007]。它作为一个瓶颈层，将神经网络的表示限制在低维空间中。这既简化了学习问题，也加快了处理时间 [Barron et al. 2015; Barron and Poole 2016]。关键的是，在双边网格内进行推理迫使我们模型的预测遵循g中的边缘，从而将我们的预测规范到边缘感知的解决方案（与基于转置卷积或“反卷积层”的标准网络不同，见图5）。这一设计决策倾向于有利于我们的摄影操作任务，并使我们在速度上比更通用的模型有显著提升，原因是A的低维度（见图10）。

![](https://borninfreedom.github.io/images/2024/10/hdrnet/10.png)
图10显示了我们算法的速度和质量与两种现代网络架构的比较：U-Net（改编自[Isola et al. 2016]）和扩张卷积（dilated convolutions）[Yu and Koltun 2015]。运行时间是在桌面CPU上处理400万像素图像的20次迭代的平均值。PSNR数值是指局部拉普拉斯任务。由于深度不足，U-Net和扩张卷积无法捕捉局部拉普拉斯滤波器的大尺度效应，导致PSNR值较低。竞争架构的运行速度比我们的慢100倍以上，并且使用的内存多几个数量级。图上我们的模型的性能代表了多个不同参数表示的性能。图上红色的版本，是本文中的所有结果使用的模型。有关我们模型的速度/质量权衡的详细信息，请参见图11。

![](https://borninfreedom.github.io/images/2024/10/hdrnet/11.png)
图11显示了局部拉普拉斯任务的PSNR值以及预测双边系数所需的计算时间，这些数据是使用我们模型的几个参数设置得到的。每条曲线代表一个网格深度d。对于每条曲线，网格的空间分辨率在{8, 16, 32}中变化。我们用来产生所有结果的参考模型用方形标记突出显示。不出所料，具有更大网格深度的模型表现更好（绿色）。加倍中间特征的数量也可以提供0.5 dB的改进（红色曲线）。运行时间是在Intel Core i7-5930K上测量的。


这种数据依赖的查找对我们模型的表现力至关重要。正如我们将在第3.4.2节中看到的，它允许我们使用一系列更简单的局部模型来预测全分辨率图像上的复杂操作。

## 3.4 组合全分辨率输出

到目前为止，我们已经描述了如何获得并上采样双边仿射系数网格。其余的处理工作在全分辨率下进行。因此，它应该是简单且易于并行化的，以最小化计算成本。从全分辨率输入I中，我们提取一组n_ϕ个全分辨率特征ϕ，它们有两个作用：

1) 它们被组合来预测切片节点中使用的引导图g，以及
2) 它们被用作局部仿射模型的回归变量。

最具成本效益的方法是使用输入图像的通道作为特征，即ϕ= I（其中n_ϕ= 3），局部仿射模型是颜色变换。我们所有的结果都使用了这种快速的公式。

### 3.4.1 引导图辅助网络

我们定义g为全分辨率特征的简单逐点非线性变换。

![](https://borninfreedom.github.io/images/2024/10/hdrnet/m8.png)

其中，M_c^T表示3×3颜色变换矩阵的行。在图像处理中，这种矩阵通常用于颜色校正，通过乘以原始图像的RGB值来调整颜色，以匹配目标色彩空间或修正颜色偏差。b和b_c^'是bias。ρc表示分段线性传递函数，参数化表示为16个缩放的ReLU（Rectified Linear Unit）函数的和，其中包含阈值t_{c,i}和斜率a_{c,i}。

![](https://borninfreedom.github.io/images/2024/10/hdrnet/m9.png)

参数M、a、t、b和b'与网络的其他参数一起学习。M初始化为单位矩阵，而a、t、b和b'初始化得使得每个ρc在[0, 1]区间内是恒等映射，这是避免学习到退化的g所必需的。图7展示了使用这个学习到的引导的影响，图6展示了对应任务学习到的颜色变换矩阵和色调曲线的例子。


![](https://borninfreedom.github.io/images/2024/10/hdrnet/6.png)
图6. 一个模型实例学习到的用于生成引导图g的颜色变换矩阵（左）和每个通道的色调曲线（右）。


### 3.4.2 组合最终输出

尽管在整幅图像的尺度上观察时图像操作可能很复杂，但最近的工作已经观察到，即使是复杂的图像处理流程通常也可以被准确地建模为一系列简单的局部变换的集合 [Chen et al. 2016; Gharbi et al. 2015; He and Sun 2015]。因此，我们对最终输出的每个通道O_c建模为全分辨率特征的仿射组合，其系数由切片特征图¯A的通道定义：。

![](https://borninfreedom.github.io/images/2024/10/hdrnet/m10.png)

类似于这种插值仿射变换已经成功地用于抠图[Levin et al. 2008]、内在图像分解[Bousseau et al. 2009]和一天中的时间转移[Shih et al. 2013]。对于这类模型，拟合仿射模型的局部区域（patch）的大小决定了效率和质量之间的权衡。在极端情况下，总是可以通过在每个像素处拟合一个独立的模型（即，局部区域大小为1×1）来实现任何操作符的完美重建。对于小局部区域（例如，3×3），仿射模型可以忠实地再现许多图像操作符。随着局部区域的增大，除了一些简单的操作符外，仿射关系不再适用于所有操作符，尽管已有研究显示，这种限制可以通过使用分段线性函数[Yuan and Sun 2011]或非线性和边缘感知组件[Gharbi et al. 2015]来减轻。

## 3.5 训练过程

我们使用给定的操作符的全分辨率输入/输出对数据集D = {(Ii, Oi)}i进行网络训练。我们通过最小化这个训练集上的L2损失来优化权重和偏置：

![](https://borninfreedom.github.io/images/2024/10/hdrnet/m11.png)

我们还通过L2权重衰减10^-8来正则化权重。卷积层和全连接层的权重根据[He et al. 2015]进行初始化，偏置初始化为0。我们在每对中间特征图之间使用批量归一化[Ioffe and Szegedy 2015]，并且使用ADAM优化器[Kingma and Ba 2015]来优化网络参数。我们使用4到16的批量大小（取决于分辨率）和10^-4的学习率进行训练。ADAM中的其余参数保持作者推荐的值。我们的模型在Tensorflow [Abadi et al. 2015]和Halide [Ragan-Kelley et al. 2012]中实现。对于所有实验，模型在NVIDIA Titan X (Maxwell)上训练30个周期，通常需要2-3天。

# 4 结论

我们评估了我们的模型再现算法图像操作符（第4.1节）和人工标注修图（第4.2节）的能力。我们的模型比标准神经网络和最先进的滤波器近似技术都要快，并且可以在移动设备上实时运行（第4.3节）。在图14中可以看到我们在不同任务上的结果。我们的输出通常是准确的，即使与真实情况有所不同，也仍然看起来合理。尽管我们的方法固有地包含了大量的空间和双边下采样，但图像伪影很少见且不易察觉。这是因为双边网格的边缘感知特性以及我们模型学习平滑输出变换的能力。我们的输出通常稍微柔和一些（例如，在图14中的HDR+示例），因为最高频率的变换，如锐化和色差校正，可能会引入输入中不存在的新边缘，我们的模型还无法处理这些。

## 4.1 复现图像操作符

我们评估了模型在多个由程序定义的图像操作任务上的准确性：

- HDR+ [Hasinoff et al. 2016] —— 一个复杂的手工设计的摄影流程，包括颜色校正、自动曝光、去雾和色调映射。
- Local拉普拉斯滤波器 [Paris et al. 2011] —— 一个用于细节增强的保留边缘、多尺度（但非尺度不变）操作符（我们使用了两种不同的强度来实现效果），
- 以及基于本地拉普拉斯滤波器的风格迁移任务 [Aubry et al. 2014]。
- 使用标记人脸数据集的面部提亮任务 [Jain and Learned-Miller 2010]。
- 几种不同的黑盒Adobe Photoshop（PS）滤镜和用户创建的“动作”。

使用我们的模型和基线方法在这些任务上的PSNR数值可以在表2中找到。
![](https://borninfreedom.github.io/images/2024/10/hdrnet/t2.png)
表2. 我们比较了我们模型的准确性与双边引导上采样（BGU）和转换配方（TR）。请注意，BGU和TR在降低或全分辨率下运行用于评估每个图像操作符的代码，因此可以被认为是性能的上限。尽管处于劣势，我们的模型有时表现得比这些基线更好，这得益于其表现力和模拟非尺度不变操作符的能力。



我们使用了两种风格迁移任务的变体。在第一种变体（风格迁移）中，我们学习将任何新的输入转换为一个独特的固定风格。在更具挑战性的第二种变体（n-风格迁移）中，我们调整网络以接受两个输入图像（沿通道轴连接）并预测将一张图像的风格转移到另一张图像的结果（再次使用Aubry等人[2014]的算法）。在这种变体中，网络不是学习预测一个单一的一致输出；而是，它学习从目标图像中提取所需的转换并将其应用于输入图像。

### 4.1.1 数据集

除了HDR+和面部提亮数据集外，所有效果都应用于MIT“FiveK”数据集的raw图集合 [Bychkovsky et al. 2011]。我们保留500张图像用于验证和测试，并在剩余的4500张上进行训练。我们通过随机裁剪、翻转和旋转来增强数据。我们通过将MIT“FiveK”数据集中的每个图像映射到100个不同的图像（风格目标）来生成n-风格迁移的数据集。

### 4.1.2 基线

与我们的目标最接近的先前工作是双边引导上采样（BGU）[Chen et al. 2016]和转换配方（TR）[Gharbi et al. 2015]，我们将我们的输出与这些进行比较。然而，与我们的技术从图像数据集中离线学习摄影操作符不同，BGU和TR不使用先前训练，而是在线方式拟合特别定制的模型到输入/输出对。因此，BGU和TR需要直接访问图像操作符，因为它们需要能够在图像上运行该图像操作符（无论是在设备上降采样还是在服务器上全分辨率）。这使得我们与这些基线的比较对我们的技术有些不利，因为这些基线对可用性做出了更限制性的假设，也不能从数据中学习近似一般实例的图像操作符。尽管如此，我们还是报告了这些技术的指标，作为一种基线。

TR假设移动设备会将高度压缩（因此降低了图像质量）的图像发送到服务器进行处理，并从服务器接收一个方案，用于近似图像转换。由于TR的客户端-服务器设置与本文的范围无关，我们在未压缩的全分辨率图像上运行模型（使用作者推荐的设置），从而提高输出质量，使我们的TR基线尽可能具有竞争力。在方法的预期用例中，图像质量通常降低了3-5 dB。

BGU假设图像操作符在输入的低分辨率版本上运行，然后拟合模型到低分辨率输入/输出对。我们无法在低分辨率下运行HDR+滤波器，因此我们使用了全分辨率输入/输出对，并通过下采样为BGU创建了低分辨率输入。然而，对于我们有实现的本地拉普拉斯和风格迁移任务，我们确实遵循了正确的程序，直接在低分辨率上应用了滤波器。对于这些非尺度不变的任务，我们技术的优越性变得更加明显（见图9）。

## 4.2 从人工注释中学习

我们还使用MIT-Adobe“FiveK”数据集[Bychkovsky et al. 2011]评估了与人工注释相关的准确性，我们的表现与先前工作的比较显示在表3中。这项任务衡量了我们模型学习一个高度主观的图像操作符的能力，这需要大量的学习和语义推理。我们报告了在MIT“FiveK”数据集中5位摄影师（A、B、C、D、E）的修饰的平均L2误差（在L*a*b*空间中，越低越好），尽管先前的工作只呈现了摄影师C的结果[Hwang et al. 2012; Yan et al. 2016]。我们使用了[Hwang et al. 2012]中提出的“random250”和“highvar50”数据集分割，分别在测试集中有250张随机选择和50张用户加权的图像。
![](https://borninfreedom.github.io/images/2024/10/hdrnet/t3.png)
表3展示了在MIT5k数据集中5位摄影师（A、B、C、D、E）的修图工作中，L*a*b*色彩空间下的均方误差（Mean L2 error），数值越低表示越好。我们的算法能够比以往的工作更好地学习摄影师的修图风格，并且运行速度提高了数个数量级。前两组比较是在之前技术偏好的摄影师C的数据集上进行评估的；具体细节请参见正文。在第三组中，我们报告了其余4位摄影师的结果，以保证完整性。从先前工作中获取的指标[Hwang et al. 2012; Yan et al. 2016]用†表示。


这是一个更加困难的任务，摄影师修饰的不一致性之前已经被指出[Yan et al. 2016]。例如，我们发现这个数据集中的修图师B更加自我一致，对我们的网络来说更容易学习。尽管如此，我们的模型在分别针对每位艺术家的修正进行训练后，始终预测出合理的调整，并超越了先前的工作。


## 4.3 性能

我们在运行Android 7.1.1的Google Pixel手机上实现了我们的技术。我们的实现能够实时处理取景器分辨率1920×1080的图像，帧率高达40-50 Hz。我们使用Camera2 API提取8位预览帧，这些帧以YUV420格式呈现。这些图像被下采样到256×256，转换为浮点RGB，然后输入到我们的网络中。网络产生输出（一个双边网格的仿射系数）后，我们将它们作为一组三个3D RGBA纹理传输到GPU，在那里进行切片并应用于全分辨率输入以渲染最终处理过的预览。总体吞吐量低于20毫秒，其中14毫秒用于推理（CPU），与上传系数的1毫秒和GPU上渲染的18毫秒重叠。作为比较，在同一设备上运行优化过的局部拉普拉斯滤波器[Paris et al. 2011]的实现[Ragan-Kelley et al. 2012]需要超过200毫秒。以降低的256×256分辨率运行相同的滤波器，并应用双边引导上采样[Chen et al. 2016]，网格尺寸相同，需要17毫秒（相比之下我们的是14毫秒），但会丢失一些滤波器预期的效果（见图9）。我们的处理时间与输入大小成线性关系，处理一张1200万像素的图像需要61毫秒。尽管通常具有更高的保真度，TR[Gharbi et al. 2015]每张图像需要2.95秒，几乎比实时取景器性能低两个数量级。最值得注意的是，TR和双边引导上采样都不能应用从人类修图中学习的效果，或者像Photoshop滤镜或HDR+这样的“黑盒”操作符。

其他一些可能用于此类学习的基于神经网络的架构也远未达到实时处理。在图10中，我们将我们的技术与从Isola等人[2016]改编的U-Net架构[Ronneberger et al. 2015]和基于扩张卷积的线性网络[Yu and Koltun 2015]进行了比较。我们探索了这些架构中深度（层数，3到11）和宽度（滤波器数量，16到64）的几种设置，涵盖了各种速度和质量水平。对于U-Net，“深度”指的是下采样步骤的数量，“宽度”指的是第一层卷积层的通道数（在每个下采样步骤中翻倍，详见Isola等人[Isola et al. 2016]）。在扩张卷积网络中，“深度”是扩张卷积层的数量，“宽度”是每层的通道数。我们的混合CPU/OpenGL技术在桌面CPU上比这两种架构快两个数量级以上。在GPU上（未显示），网络的前向传递性能差距相同，但数据传输成为我们方法的瓶颈。总体而言，我们的运行时间仍然快一个数量级以上。此外，U-Net和扩张卷积都需要更多的内存，这使它们不适合移动处理。对于这个基准测试，我们使用了一台拥有4个核心的Intel Core i7-5930K @ 3.5GHz和一块Titan X (Maxwell) GPU。

我们探索了我们的架构在局部拉普拉斯任务中的速度/质量权衡，改变了几个参数：将网格深度d从4变为16，网格的空间尺寸从8×8变为32×32，并加倍通道数（与表1中报告的数字相比）。总结可以在图11中找到。

## 4.4 讨论和限制

我们所有的结果都使用了最简单的全分辨率特征ϕ=I；即，引导图g和仿射回归目标都是输入图像的颜色通道（第3.4节）。如果放宽实时渲染的限制，可以通过从高分辨率图像中提取特征来扩展我们的模型。在图13中，我们展示了一个例子，其中ϕ是一个3级的高斯金字塔。然后双边网格包含了3×12=36个仿射参数（每个尺度12个）。相应地，我们将网络中的中间特征数量增加到表1中的数字的三倍。这大致使网络速度降低了3-4倍，但在局部拉普拉斯（强）任务上提高了2 dB的质量。

我们还探索了使用我们的架构来学习超出图像增强的任务，如抠图、着色、去雾和单目深度预测。这些实验的成功有限，因为快速摄影校正所需的强烈建模假设使我们的模型不适合于那些输出不能轻易表示为输入图像的局部逐点变换的不同任务（图12）。


![](https://borninfreedom.github.io/images/2024/10/hdrnet/12.png)
图12. 当图像操作符严重违反我们的建模假设时，我们的算法失败。(a) hazing降低了局部对比度，这限制了我们的引导图的有用性。它还破坏了我们的仿射模型无法恢复的图像细节（例如，在白板上）。(b) 抠图已经成功地被局部仿射模型在3×3邻域上建模[Levin et al. 2008]。然而，这种仿射关系在更大的尺度上（像我们模型中的一个网格单元）崩溃，其中抠图不再遵循色调或颜色变化，而且大多是二进制的。这限制了我们的双边网格的有用性。(c) 对于着色，学习到的引导图最多是对灰度输入的非线性重映射。我们的模型因此只能在由网格分辨率决定的空间分辨率下，学习每个离散强度级别的局部颜色。我们的输出因L2损失而充满了粗糙的颜色变化，这些颜色变化被抑制（见道路线和树/天空边界）。

![](https://borninfreedom.github.io/images/2024/10/hdrnet/13.png)
图13展示了，如果我们愿意在全分辨率下进行额外的计算，我们的模型可以通过使用更丰富的仿射回归特征来扩展。在这里，通过使用3级高斯金字塔作为特征ϕ，我们能更好地捕捉局部拉普拉斯（强）任务中的高频细节。

![](https://borninfreedom.github.io/images/2024/10/hdrnet/14.png)
图14展示了我们的方法能够学习准确且快速地近似各种图像操作符，通过在被该操作符处理的输入/输出对上进行训练。这些操作符可以是复杂的“黑盒”图像处理流程，其中只有二进制文件可用，例如HDR+或Photoshop滤镜/动作。有些操作符，如面部提亮，需要语义理解。我们的模型甚至能够从高度主观的人工标注的输入/输出对中学习，使用MIT-Adobe FiveK数据集。差异被重新缩放到使用完整的[0, 1]范围。


# 5 结论

我们介绍了一种新的神经网络架构，它能够在全分辨率图像上实时进行图像增强，同时捕捉到高频效果。我们的模型通过输入/输出图像对进行训练，使其能够从某些算法的参考实现或人工调整中学习。通过在双边网格内进行大部分计算，并预测局部仿射颜色变换，我们的模型在表现力和速度之间取得了恰当的平衡。为了构建这个模型，我们引入了两个新层：一个数据依赖的查找层，它使我们能够对双边网格进行切片；以及一个用于仿射变换的乘法操作。通过端到端的训练，并在全分辨率下优化损失函数（尽管我们的网络大部分在大幅降低的分辨率下运行），我们的模型能够学习全分辨率和非尺度不变的效果。我们的模型在多种不同的图像操作符、流程和主观人工标注的数据集上展示了准确性。















